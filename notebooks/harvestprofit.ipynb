{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c03cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "import os\n",
    "import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "from intuitlib.client import AuthClient\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse, unquote, urljoin\n",
    "from time import perf_counter\n",
    "import re\n",
    "import yaml\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633bb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job:\n",
    "    def __init__(self):\n",
    "        base_dir = Path(\"c:/Users/ZheRao/OneDrive - Monette Farms/Monette Farms Team Site - Innovation Projects/Production/Database\")\n",
    "        self.base_dir = base_dir\n",
    "        self.today = dt.date.today()\n",
    "        month_format = \"\".join([\"0\",str(self.today.month)]) if self.today.month < 10 else str(self.today.month)\n",
    "        self.us_companies = [\"MFUSA\", \"MFAZ\", \"MSUSA\", \"MPUSA\"]\n",
    "        self.company_names = self.us_companies + [\"MSL\", \"NexGen\", \"MFBC\", \"MPL\", \"MFL\"]\n",
    "        self.raw_path = {\n",
    "            \"QBO\": {\n",
    "                \"Raw\": base_dir/\"Bronze\"/\"QBO\"/\"Raw\"/f\"{self.today.year}_{self.today.month}\",\n",
    "                \"GL\": base_dir/\"Bronze\"/\"QBO\"/\"GeneralLedger\",\n",
    "                \"PL\": base_dir/\"Bronze\"/\"QBO\"/\"ProfitAndLoss\",\n",
    "                \"Time\":base_dir/\"Bronze\"/\"QBOTime\",\n",
    "                \"APAR\": base_dir/\"Bronze\"/\"QBO\"/\"APAR\"\n",
    "            },\n",
    "            \"Delivery\": {\"Traction\":base_dir/\"Bronze\"/\"Traction\", \"HP\":base_dir/\"Bronze\"/\"HarvestProfit\"},\n",
    "            \"Auth\": {\"QBO\":base_dir/\"Bronze\"/\"Authentication\"/\"QBO\", \"QBOTime\": base_dir/\"Bronze\"/\"Authentication\"/\"QBOTime\",\n",
    "                     \"Harvest Profit\": base_dir/\"Bronze\"/\"Authentication\"/\"Harvest Profit\"},\n",
    "            \"Log\": base_dir/\"Load_History\"/f\"{self.today.year}\"/month_format\n",
    "        }\n",
    "        self.silver_path = {\n",
    "            \"QBO\": {\n",
    "                \"Dimension_time\": base_dir/\"Silver\"/\"QBO\"/\"Dimension\"/f\"{self.today.year}_{self.today.month}\",\n",
    "                \"Dimension\": base_dir/\"Silver\"/\"QBO\"/\"Dimension\",\n",
    "                \"Raw\": base_dir/\"Silver\"/\"QBO\"/\"Fact\"/\"Raw\",\n",
    "                \"PL\": base_dir/\"Silver\"/\"QBO\"/\"Fact\"/\"ProfitAndLoss\",\n",
    "                \"GL\": base_dir/\"Silver\"/\"QBO\"/\"Fact\"/\"GeneralLedger\",\n",
    "                \"Time\": base_dir/\"Silver\"/\"QBOTime\",\n",
    "                \"APAR\": base_dir/\"Silver\"/\"QBO\"/\"Fact\"/\"APAR\"\n",
    "            },\n",
    "            \"Delivery\": {\"Traction\":base_dir/\"Silver\"/\"Traction\", \"HP\":base_dir/\"Silver\"/\"HarvestProfit\"}\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def get_fx(self):\n",
    "        key  = os.getenv(\"ALPHAVANTAGE_KEY\")\n",
    "        url  = (\"https://www.alphavantage.co/query?\"\n",
    "                \"function=CURRENCY_EXCHANGE_RATE\"\n",
    "                \"&from_currency=USD&to_currency=CAD\"\n",
    "                f\"&apikey={key}\")\n",
    "        rate = float(requests.get(url, timeout=10).json()\n",
    "                    [\"Realtime Currency Exchange Rate\"][\"5. Exchange Rate\"])\n",
    "        self.fx = rate\n",
    "    \n",
    "    def create_log(self, path: Path) -> None:\n",
    "        self.check_file(path)\n",
    "        day_format = \"\".join([\"0\",str(self.today.day)]) if self.today.day < 10 else str(self.today.day)\n",
    "        self.log = open(path/(day_format+\"_Log.txt\"), \"a\")\n",
    "\n",
    "    \n",
    "    def close_log(self):\n",
    "        self.log.close()\n",
    "\n",
    "    def check_file(self, path: Path) -> None:\n",
    "        if not Path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    def formulate_date(self, df:pd.DataFrame, date_cols:list[str], drop_time:bool=True) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            format the date columns into datetime format\n",
    "        \"\"\"\n",
    "        assert len(set(date_cols) - set(df.columns)) == 0, \"Not all columns passed are inside dataframe passed\"\n",
    "        for col in date_cols:\n",
    "            df[col] = pd.to_datetime(df[col], utc=True)\n",
    "            if drop_time: df[col] = df[col].dt.date\n",
    "        return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e8f15",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba1075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projects(Job):\n",
    "    \"\"\" \n",
    "        for project specific data transformations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, focus_last_FY:bool = False, is_dev:bool=False):\n",
    "        super().__init__()\n",
    "        self.gold_path = {\n",
    "            \"weekly_banking\": self.base_dir / \"Gold\" / \"FinanceProject\" / \"WeeklyBanking\",\n",
    "            \"inventory\": self.base_dir / \"Gold\" / \"InventoryProject\",\n",
    "            \"payroll\": self.base_dir / \"Gold\" / \"HRProject\" /\"PayrollProject\",\n",
    "            \"finance_operational\": self.base_dir / \"Gold\" / \"FinanceOperationalProject\",\n",
    "            \"budget\": self.base_dir / \"Gold\" / \"BudgetProject\",\n",
    "            \"QBOTime\": self.base_dir / \"Gold\" / \"HRProject\" / \"QBOTimeProject\",\n",
    "            \"hr_combined\": self.base_dir / \"Gold\" / \"HRProject\" / \"CombinedSummary\",\n",
    "            \"pillar_dashboard\": self.base_dir / \"Gold\" / \"DirectorDashboards\",\n",
    "            \"APReporting\": self.base_dir / \"Gold\" / \"FinanceProject\" / \"APReporting\"\n",
    "        }\n",
    "        self.silver_acc = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension_time\"]/\"Account.csv\")\n",
    "        self.commodities = {\n",
    "            \"Produce\": [\"Strawberry\", \"Watermelon\", \"Cantaloupe\", \"Market Garden\", \"Broccoli\", \"Pumpkin\", \"Sweet Corn\", \"Cauliflower\", \"Squash\", \"Honeydew Melon\", \"Potato\", \"Carrot\", \"Cabbage\",\n",
    "                        \"Lettuce\", \"Brussel Sprouts\", \"Prairie Pathways\", \"Beet\", \"Corn Maze\", \"CSA\"],\n",
    "            \"Grain\": [\"Blackeye Pea\", \"Winter Wheat\", \"Durum\", \"Cotton\", \"Chickpea\", \"Barley\", \"Green Lentil\", \"Red Lentil\", \"Canola\", \n",
    "                        \"Wheat\",\"Field Pea\", \"Corn\", \"Oat\", \"Soybean\", \"Bean\"],\n",
    "            \"Cattle\": [\"Weaned Calves\", \"Cull Bull\", \"Cull Cow\", \"Bred Heifer\", \"Purebred Yealing Bull\", \"Purebred Heifer\", \n",
    "                        \"Purebred Cow\", \"Purebred Bull\", \"Cow\", \"Bull\", \"Steer\", \"Heifer\", \"Yearling\", \"Calf\"]\n",
    "        }\n",
    "        self.locations = {\n",
    "            \"Produce\": [\"BritishColumbia (produce)\", \"Outlook\", \"Arizona (produce)\", \"Montana (produce)\", \"Seeds USA\"],\n",
    "            \"Cattle\": [\"Airdrie\", \"Eddystone (cattle)\", \"Ashcroft\", \"Home Ranch\", \"Diamond S\", \"Wolf Ranch\", \"Fraser River Ranch\", \"Moon Ranch\", \"Waldeck\", \"Calderbank\"],\n",
    "            \"Grain\": [\"Eddystone (grain)\", \"Arizona (grain)\", \"Colorado\", \"Swift Current\", \"Regina\", \"Raymore\", \"Prince Albert\", \"The Pas\",\n",
    "                      \"Kamsack\", \"Hafford\", \"Yorkton\", \"Fly Creek\", \"Camp 4\", \"Havre\", \"Billings\"],\n",
    "            \"Seed\": [\"NexGen\", \"Seeds\"],\n",
    "            \"Others\": [\"Eddystone (corporate)\", \"Arizona (corporate)\", \"Legacy\", \"BritishColumbia (corporate)\", \"Corporate\"]\n",
    "        }\n",
    "        self.bc_ranches = [\"Ashcroft\", \"Fraser River Ranch\", \"Moon Ranch\", \"Wolf Ranch\", \"Diamond S\",\"Home Ranch\"]\n",
    "        self.pl_exist = False # determines whether _financial_operational has run and gold_pl is stored in self, if not, any subsequent downstream projects will run _financial_operational first\n",
    "        self.currentFY = self.today.year if self.today.month<=10 else self.today.year + 1\n",
    "        if focus_last_FY: self.currentFY -= 1\n",
    "        self.is_dev = is_dev\n",
    "        self.accnum_reroute = {\"MFL405101\": \"MFL405110\", \"MFL405102\":\"MFL405120\", \"MFL405103\":\"MFL405130\", \"MSL585000\":\"MSL562505\", \"MSL402110\": \"MSL402112\", \"MFBC575000\": \"MFBC575020\",\n",
    "                               \"MFBC629000\": \"MFBC629010\"}\n",
    "        self.accid_reroute = {\"MFBC250\": \"MFBC210\", \"MFBC216\":\"MFBC210\", \"MFBC358\":\"MFBC210\", \"MFBC255\":\"MFBC210\", \"MFBC314\":\"MFBC272\", \"MFBC268\":\"MFBC584\", \"MFBC188\":\"MFBC192\", \"MFBC374\":\"MFBC190\",\n",
    "                              \"MFBC61\":\"MFBC190\", \"MFBC26\":\"MFBC585\", \"MFBC412\":\"MFBC242\", \"MFBC66\": \"MFBC592\", \"MFBC65\":\"MFBC220\", \"MFBC19\":\"MFBC210\", \"MFBC60\":\"MFBC187\"}\n",
    "        self.conversion_mt_to_lb = 2204.62262185\n",
    "        self.conversion_bu_to_lb_canola = 55\n",
    "        self.conversion_bu_to_lb_others = 60\n",
    "\n",
    "    def _pillar_classification(self, entry: pd.Series) -> str:\n",
    "        \"\"\" \n",
    "            this function classifies pillar of a transaction based on location\n",
    "        \"\"\"\n",
    "        location = entry[\"Location\"]\n",
    "        if not isinstance(location, str) or (location == \"Missing\"):\n",
    "            return \"Missing\"\n",
    "        if \"produce\" in location:\n",
    "            return \"Produce\"\n",
    "        elif \"grain\" in location:\n",
    "            return \"Grain\"\n",
    "        elif \"cattle\" in location:\n",
    "            return \"Cattle\"\n",
    "        elif \"corporate\" in location:\n",
    "            return \"Unclassified\"\n",
    "        match location.lower():\n",
    "            case \"hafford\"|\"kamsack\"|\"prince albert\"|\"raymore\"|\"regina\"|\"swift current\"|\"the pas\"|\"camp 4\"|\"fly creek\"|\"havre\"|\"yorkton\"|\"colorado\"|\"billings\":\n",
    "                return \"Grain\"\n",
    "            case \"outlook\"|\"seeds usa\":\n",
    "                return \"Produce\"\n",
    "            case \"ashcroft\"|\"diamond s\"|\"fraser river ranch\"|\"home ranch\"|\"moon ranch\"|\"wolf ranch\"|\"waldeck\"|\"calderbank\"|\"airdrie\":\n",
    "                return \"Cattle\"\n",
    "            case \"seeds\"|\"nexgen\":\n",
    "                return \"Seed\"\n",
    "            case _:\n",
    "                return \"Unclassified\"\n",
    "    \n",
    "    def _identify_product(self, entry: pd.Series, for_budget:bool=False) -> str:\n",
    "        \"\"\" \n",
    "            this function identifies commodity from account names, except for seed, \n",
    "                if this function is called from budget project, it combines MG & CSA and take CM into consideration, and name forage differently\n",
    "        \"\"\"\n",
    "        if not for_budget:\n",
    "            if entry[\"Corp\"] in [\"MSL\", \"MSUSA\", \"NexGen\"]:\n",
    "                return \"SeedProduct\"\n",
    "        accname = entry[\"AccName\"].lower() if not for_budget else entry[\"AccFull\"].lower()\n",
    "        if \"float\" in accname:\n",
    "            return \"Others\"\n",
    "        for x in self.commodities[\"Produce\"] + self.commodities[\"Grain\"] + self.commodities[\"Cattle\"]:\n",
    "            if x.lower() in accname:\n",
    "                if for_budget:\n",
    "                    match x:\n",
    "                        case \"Market Garden\"|\"CSA\":\n",
    "                            return \"Market Garden / CSA\"\n",
    "                        case \"Corn Maze\":\n",
    "                            return \"Prairie Pathways\"\n",
    "                    return x \n",
    "                return x\n",
    "        if \"straw\" in accname or \"forage\" in accname or \"hay bale\" in accname:\n",
    "            if for_budget: \n",
    "                return \"Hay/Silage\" \n",
    "            else: \n",
    "                return \"Forage\"\n",
    "        return \"Others\"\n",
    "    \n",
    "    def _weekly_banking(self) -> None:\n",
    "        \"\"\" \n",
    "            weekly banking project: match latest GL bank transactions with raw activities - extract accounts for those activities\n",
    "                assumptions: a raw entry (e.g., invoice) can have multiple lines - multiple associated accounts, only considering the first one \n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Weekly Banking Project Transformation\\n\")\n",
    "        # determine minal date to keep for GL\n",
    "        if self.today.month > 6:\n",
    "            year = self.today.year \n",
    "            month = self.today.month - 6 \n",
    "        else:\n",
    "            year = self.today.year - 1\n",
    "            month = self.today.month + 12 - 6\n",
    "        # load and prepare data\n",
    "        ## account\n",
    "        account = self.silver_acc.copy(deep=True)\n",
    "        ## change some accounts to Transfer category\n",
    "        acc_list = [\"MFL264\", \"MSL250\"]\n",
    "        account.loc[account[\"AccID\"].isin(acc_list), \"ProfitType\"] = \"Asset\"\n",
    "        account.loc[account[\"AccID\"].isin(acc_list), \"Category\"] = \"Transfer\"\n",
    "        account_bank = account[account[\"AccountType\"]==\"Bank\"]\n",
    "        ## LinkedTxn for invoice and bill\n",
    "        invoice_linked = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"LinkedTxn\"/ \"LinkedTxn_Mapping_Invoice.csv\")\n",
    "        bill_linked = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"LinkedTxn\"/ \"LinkedTxn_Mapping_Bill.csv\")\n",
    "        mapping = pd.concat([invoice_linked, bill_linked])\n",
    "        mapping = mapping.drop(columns=[\"Corp\"])\n",
    "        # define customized function for processing other raw table\n",
    "        def _process_facts(df_type:str) -> pd.DataFrame:\n",
    "            \"\"\" \n",
    "                function for processing raw tables for mapping table - TransactionID_partial to AccID\n",
    "            \"\"\"\n",
    "            df = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/(df_type+\".csv\"), usecols = [\"TransactionID\", \"AccID\"])\n",
    "            df[\"TransactionID\"] = df[\"TransactionID\"].apply(lambda x: x.split(\"-\")[1])\n",
    "            df = df.drop_duplicates()\n",
    "            df = df.rename(columns={\"TransactionID\":\"TxnId\"})\n",
    "            return df\n",
    "        ## purchase table for expense transactions\n",
    "        purchase = _process_facts(\"Purchase\")\n",
    "        purchase[\"TxnType\"] = \"Expense\"\n",
    "        mapping = pd.concat([mapping,purchase])\n",
    "        ## journal entries - exclude most entries related to bank\n",
    "        journal = _process_facts(\"JournalEntry\")\n",
    "        journal[\"TxnType\"] = \"Journal Entry\"\n",
    "        # for journal entries, exclude most of entires where the activity account ID is a bank ID\n",
    "        exclude_list = list(account_bank.AccID.unique())\n",
    "        # mylist = [\"MFL51\", \"MFBC470\", \"MFBC471\", \"MFL28\", \"MFL27\", \"MFL1150040024\"]\n",
    "        mylist = [\"MFBC470\", \"MFBC471\"] # should include these accounts\n",
    "        for acc in mylist:\n",
    "            exclude_list.remove(acc)\n",
    "        journal = journal[~journal[\"AccID\"].isin(exclude_list)]\n",
    "        mapping = pd.concat([mapping,journal])\n",
    "        ## deposit\n",
    "        deposit = _process_facts(\"Deposit\")\n",
    "        deposit[\"TxnType\"] = \"Deposit\"\n",
    "        mapping = pd.concat([mapping,deposit])\n",
    "        ## salesreceipts\n",
    "        sales = _process_facts(\"SalesReceipt\")\n",
    "        sales[\"TxnType\"] = \"Sales Receipt\"\n",
    "        mapping = pd.concat([mapping,sales])\n",
    "        # process mapping table - dedup\n",
    "        mapping = mapping.drop_duplicates(subset=[\"TxnId\"],keep=\"first\")\n",
    "        ## load GL transacitons\n",
    "        cols = [\"TransactionType\",\"TransactionID_partial\",\"AccID\",\"AccNum\",\"AccName\", \"TransactionDate\", \"Amount\", \"SplitAcc\", \"SplitAccID\", \"Memo\", \"Corp\", \"Balance\"]\n",
    "        transactions = pd.read_csv(self.silver_path[\"QBO\"][\"GL\"]/\"GeneralLedger.csv\",dtype={\"TransactionID_partial\":str}, usecols=cols)\n",
    "        transactions = transactions[transactions[\"AccID\"].isin(account_bank.AccID.unique())]\n",
    "        transactions[\"TransactionDate\"] = pd.to_datetime(transactions[\"TransactionDate\"])\n",
    "        transactions = transactions[transactions[\"TransactionDate\"]>=dt.datetime(year, month, 1)]\n",
    "        transactions = transactions.rename(columns={\"TransactionType\":\"TxnType\",\"TransactionID_partial\":\"TxnId\",\n",
    "                                                    \"AccID\":\"BankAccID\",\"AccNum\":\"BankAccNum\",\"AccName\":\"BankAccName\",\n",
    "                                                    \"TransactionDate\":\"BankActivityDate\",\"Amount\":\"BankAmount\"})\n",
    "        transactions[\"Sign\"] = transactions[\"BankAmount\"].apply(lambda x: \"Positive\" if x>=0 else \"Negative\")\n",
    "        # merge to get CurrencyID for bank_acc\n",
    "        transactions = pd.merge(transactions, account_bank.loc[:,[\"AccID\",\"CurrencyID\"]], left_on=[\"BankAccID\"], right_on=[\"AccID\"], how=\"left\")\n",
    "        transactions = transactions.drop(columns=[\"AccID\"])\n",
    "        # separating transfers - don't merge with mapping table\n",
    "        transfers = transactions[transactions[\"TxnType\"] == \"Transfer\"].copy(deep=True)\n",
    "        transactions = transactions[transactions[\"TxnType\"]!=\"Transfer\"]\n",
    "        transactions = transactions.drop(columns=[\"SplitAcc\", \"SplitAccID\"])\n",
    "        transactions[\"BankActivityDate\"] = pd.to_datetime(transactions[\"BankActivityDate\"])\n",
    "        transactions[\"TxnType\"] = transactions[\"TxnType\"].replace({\"Cheque Expense\":\"Expense\", \"Check\": \"Expense\"})\n",
    "        # merge with mapping table\n",
    "        transactions_mapped = pd.merge(transactions,mapping,on=[\"TxnId\",\"TxnType\"],how=\"left\")\n",
    "        non_match = transactions_mapped[transactions_mapped[\"AccID\"].isna()]\n",
    "        print(\"None Match Transaction Types\")\n",
    "        print(non_match.TxnType.value_counts())\n",
    "        print(f\"Non matches - {len(non_match)}\")\n",
    "        # function to determine transfer type\n",
    "        def _determine_transfer_type(entry:str) -> str:\n",
    "            \"\"\" \n",
    "                determine whether the transfer is for visa, bank, or other transfer\n",
    "            \"\"\"\n",
    "            if \"visa\" in entry.lower():\n",
    "                return \"Visa Payment\"\n",
    "            elif \"due\" in entry.lower():\n",
    "                return \"Bank Transfer\"\n",
    "            else:\n",
    "                return \"Other Transfer\"\n",
    "        # allocate transfer type \n",
    "        transfers[\"TransferType\"] = transfers[\"SplitAcc\"].apply(lambda x: _determine_transfer_type(x))\n",
    "        transfers = transfers.rename(columns={\"SplitAccID\":\"AccID\"})\n",
    "        transfers = transfers.drop(columns=[\"SplitAcc\"])\n",
    "        transactions_mapped = pd.concat([transactions_mapped,transfers], ignore_index=True)\n",
    "        # clean up the dataframe\n",
    "        transactions_mapped = transactions_mapped.rename(columns={\"CurrencyID\":\"BankCurrencyID\"})\n",
    "        transactions_mapped = pd.merge(transactions_mapped, account.loc[:,[\"AccID\",\"AccName\",\"AccNum\",\"Category\",\"ProfitType\",\"CurrencyID\"]], on=\"AccID\", how=\"left\")\n",
    "        transactions_mapped.loc[transactions_mapped[\"TransferType\"]==\"Bank Transfer\",\"Category\"] = \"Bank Transfer\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"BankAccNum\"].str.startswith(\"MSL\"))&(transactions_mapped[\"AccNum\"]==\"MSL120001\")), \"Category\"] = \"Seed Processing Revenue\"\n",
    "        transactions_mapped = transactions_mapped.rename(columns={\"AccNum\":\"ActivityAccNum\", \"AccName\":\"ActivityAccName\"})\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Positive\")), \"ProfitType\"] = \"Other Operating Revenue\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Positive\")), \"Category\"] = \"Miscellaneous income\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Negative\")), \"ProfitType\"] = \"Operating Overheads\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Negative\")), \"Category\"] = \"Office and miscellaneous\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")), \"ActivityAccNum\"] = \"Manual Adjustment\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")), \"ActivityAccName\"] = \"Manual Adjustment\"\n",
    "        # csv from sharepoint is unstable, and produced unpredictable readings from Power BI\n",
    "        self.check_file(self.gold_path[\"weekly_banking\"])\n",
    "        transactions_mapped.to_excel(self.gold_path[\"weekly_banking\"]/\"BankingActivity.xlsx\", sheet_name=\"transactions\", index=False)\n",
    "\n",
    "    def _extract_accnum_accid(self) -> None:\n",
    "        \"\"\" \n",
    "            this function creates a accnum to accID mapping table to avoid repeated merges\n",
    "        \"\"\"\n",
    "        self.acc_map = self.operation_acc.set_index([\"AccNum\"])[\"AccID\"]\n",
    "\n",
    "    def _perform_manual_adjust_GL_inventory(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            This function reads inventory account balances at the beginning of this fiscal year, and apply the amount to PL accounts for Fert/Chem/Seed\n",
    "        \"\"\"\n",
    "        # fixed column values \n",
    "        date = '2024-11-01'\n",
    "        transaction_type = 'Manual Adjustments'\n",
    "        memo = 'Adjustments from Trial Balance from beginning of this fiscal year'\n",
    "        FY = 2025\n",
    "        month = 'November'\n",
    "        # read adjustments csv file\n",
    "        adjustments = pd.read_csv(self.gold_path[\"finance_operational\"]/\"ManualAdjustments\"/\"2025.csv\",dtype={\"Amount\":float})\n",
    "        location_adj = {\"Camp 4\": \"Billings\", \"Fly Creek\":\"Billings\"}\n",
    "        adjustments[\"Location\"] = adjustments[\"Location\"].replace(location_adj)\n",
    "        # compute various Amount columns to match the other PL entries\n",
    "        adjustments[\"AmountAdj\"] = -adjustments[\"Amount\"]\n",
    "        adjustments[\"AmountCAD\"] = adjustments.apply(lambda x: x[\"AmountAdj\"] * self.fx if x[\"Currency\"]==\"USD\" else x[\"AmountAdj\"],axis=1)\n",
    "        adjustments[\"AmountDisplay\"] = -adjustments[\"AmountCAD\"]\n",
    "        adjustments[\"AccNum\"] = adjustments.apply(lambda x: \"\".join(x[\"DisplayName\"].split(\" \")[:2]),axis=1)\n",
    "        # create additional entries\n",
    "        addition_df = df.head(0).copy(deep=True)\n",
    "        row = {\"TransactionDate\":date, \"TransactionType\":transaction_type, \"Memo\":memo, \"FiscalYear\":FY, \"Month\":month, \"FXRate\": self.fx}\n",
    "        for i in range(len(adjustments)):\n",
    "            entry = row | {\"Amount\": adjustments.loc[i,\"Amount\"], \"AccID\": adjustments.loc[i,\"AccID\"], \"AmountAdj\": adjustments.loc[i,\"AmountAdj\"], \n",
    "                        \"AmountCAD\": adjustments.loc[i,\"AmountCAD\"], \"AmountDisplay\":adjustments.loc[i,\"AmountDisplay\"],\n",
    "                        \"Location\":adjustments.loc[i,\"Location\"], \"Pillar\": adjustments.loc[i,\"Pillar\"],\n",
    "                        \"AccNum\":adjustments.loc[i,\"AccNum\"] }\n",
    "            addition_df.loc[len(addition_df)] = entry\n",
    "        print(f\"\\nManual GL Inventory Accounts Adjustments created {len(addition_df)} entries\\n\")\n",
    "        df = pd.concat([df,addition_df],ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def _finance_operational(self) -> None:\n",
    "        \"\"\" \n",
    "            transform PL data into operational-ready\n",
    "                1. reclassify accounts\n",
    "                2. standardize location, classify pillar\n",
    "                3. revising signs\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Finance Operational Project Transformation\\n\")\n",
    "        # load data from silver space\n",
    "        data = pd.read_csv(self.silver_path[\"QBO\"][\"PL\"]/\"ProfitAndLoss.csv\")\n",
    "        assert len(data.FXRate.value_counts()) == 1, \"different FXRate detected\"\n",
    "        self.fx = data.loc[0,\"FXRate\"]\n",
    "        data[\"TransactionDate\"] = pd.to_datetime(data[\"TransactionDate\"])\n",
    "        data[\"FiscalYear\"] = data.TransactionDate.apply(lambda x: x.year + 1 if x.month >= 11 else x.year)\n",
    "        # add month to the PL\n",
    "        data[\"Month\"] = data[\"TransactionDate\"].dt.month_name()\n",
    "        ## add location for seed operation\n",
    "        data.loc[data[\"Corp\"]==\"MSL\",\"Location\"] = \"Seeds\"\n",
    "        data.loc[data[\"Corp\"]==\"NexGen\",\"Location\"] = \"NexGen\"\n",
    "        data.loc[data[\"Corp\"]==\"MSUSA\",\"Location\"] = \"Seeds USA\"\n",
    "        # clean location\n",
    "        data = data.rename(columns={\"Location\":\"LocationRaw\"})\n",
    "        data[\"Location\"] = data[\"LocationRaw\"]\n",
    "        data = data.fillna(value={\"Location\":\"Missing\"})\n",
    "        # switch seeds usa to AZ produce\n",
    "        data.loc[data[\"Corp\"]==\"MSUSA\",\"Location\"] = \"Arizona (produce)\"\n",
    "        ## clean location\n",
    "        clean_location = {\"Airdrie - Grain\":\"Airdrie\", \"Airdrie - Cattle\":\"Airdrie\", \"Airdrie - General\":\"Airdrie\", \"Airdrie\":\"Airdrie\", \n",
    "                        \"Eddystone - Grain\": \"Eddystone (grain)\", \"Eddystone - Cattle\": \"Eddystone (cattle)\", \"Eddystone - General\":\"Eddystone (corporate)\",\n",
    "                        \"Outlook (JV)\":\"Outlook\", \"AZ Produce\":\"Arizona (produce)\", \"Corporate\":\"Arizona (corporate)\", \"BC Produce\":\"BritishColumbia (produce)\",\n",
    "                        \"Grain\":\"Arizona (grain)\", \"Ashcroft (CC, Fischer, Loon)\":\"Ashcroft\", \n",
    "                        \"Outlook (Capital)\":\"Outlook\", \"Colorado (MF)\":\"Colorado\", \"Colorado (JV)\":\"Colorado\", \"Cattle - General\":\"BritishColumbia (corporate)\",\n",
    "                        \"Home (70 M, LF/W, 105 M)\":\"Home Ranch\", \"Diamond S (BR)\":\"Diamond S\", \"-Corporate\":\"Corporate\",\n",
    "                        \"MT Produce\": \"Montana (produce)\", \"Fly Creek\": \"Billings\", \"Camp 4\":\"Billings\"}\n",
    "        others = {\"North Farm (deleted)\":\"Legacy\", \"Cache/Fischer/Loon - DNU\":\"Legacy\"}\n",
    "        data[\"Location\"] = data[\"Location\"].replace(clean_location)\n",
    "        locations = self.locations[\"Produce\"] + self.locations[\"Grain\"] + self.locations[\"Cattle\"] + self.locations[\"Others\"] + self.locations[\"Seed\"]\n",
    "        unaccounted_location = list(set(data[\"Location\"].unique()) - set(locations))\n",
    "        print(f\"location unaccounted for - {unaccounted_location}\")\n",
    "        # classify pillar\n",
    "        data[\"Pillar\"] = data.apply(lambda x: self._pillar_classification(x),axis=1)\n",
    "        # reorganize corp\n",
    "        ## MPUSA missing location = Arizona (produce)\n",
    "        data.loc[((data[\"Corp\"] == \"MPUSA\")&(data[\"Location\"].isna())), \"Location\"] = \"Arizona (produce)\"\n",
    "        data.loc[((data[\"Corp\"] == \"MPUSA\")&(data[\"Location\"]==\"Missing\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        data.loc[((data[\"Corp\"] == \"MPUSA\")&(data[\"Location\"] == \"Arizona (produce)\")), \"Pillar\"] = \"Produce\"\n",
    "        ## AZ Produce --> MPUSA\n",
    "        data.loc[data[\"Location\"] == \"Arizona (produce)\", \"Corp\"] = \"MPUSA\"\n",
    "        ## move everything for AZ in 2024 to produce\n",
    "        data.loc[((data[\"FiscalYear\"] >= 2024) & (data[\"Location\"].str.contains(\"Arizona\",case=False))),\"Pillar\"] = \"Produce\"\n",
    "        data.loc[((data[\"FiscalYear\"] >= 2024) & (data[\"Location\"].str.contains(\"Arizona\",case=False))),\"Location\"] = \"Arizona (produce)\"\n",
    "        ## BC Produce --> MPL\n",
    "        data.loc[data[\"Location\"] == \"BritishColumbia (produce)\", \"Corp\"] = \"MPL\"\n",
    "        ## Outlook --> MPL\n",
    "        data.loc[data[\"Location\"]==\"Outlook\", \"Corp\"] = \"MPL\"\n",
    "        # reroute accid\n",
    "        data[\"AccID\"] = data[\"AccID\"].replace(self.accid_reroute)\n",
    "        # Reclassify accounts for Operational Purpose\n",
    "        ## read & process operational classification\n",
    "        with open(self.silver_path[\"QBO\"][\"Dimension\"]/\"acc_classification.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_acc = yaml.safe_load(f)\n",
    "        rows = [(l1, l2, l3, v) \n",
    "                for l1, l1_inner in raw_acc.items() \n",
    "                for l2, l2_inner in l1_inner.items() \n",
    "                for l3, l3_inner in l2_inner.items() \n",
    "                for v in l3_inner]\n",
    "        acc_operation = pd.DataFrame(rows, columns=[\"OperationProfType\", \"OperationCategory\", \"OperationSubCategory\", \"AccID\"])\n",
    "        ## read accounts table and apply new classification\n",
    "        accounts = self.silver_acc\n",
    "        accounts = pd.merge(accounts, acc_operation, on = \"AccID\", how = \"left\")\n",
    "        accounts[\"Commodity\"] = accounts.apply(lambda x: self._identify_product(x), axis=1)\n",
    "        commodities = pd.DataFrame(data={\"Commodity\": accounts[\"Commodity\"].unique()})\n",
    "        commodities.to_csv(self.gold_path[\"inventory\"]/\"Tables\"/\"commodities_acc.csv\", index=False)\n",
    "        # prepare account table for mapping AccID from AccNum\n",
    "        self.operation_acc = accounts[accounts[\"AccNum\"].notna()]   # AccNum must be non-missing\n",
    "        self.operation_acc = self.operation_acc[self.operation_acc[\"Active\"]] # avoid non-active accounts what share same AccNum with active accounts\n",
    "        self.operation_acc.to_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\", index=False)\n",
    "        # Revising Signs according to Operational Classification\n",
    "        print(\"Revising Signs ...\")\n",
    "        # expense_accounts = accounts[(accounts[\"OperationCategory\"] == \"Expense\") | (accounts[\"OperationCategory\"] ==\"Inventory Consumption\")] # for my classification\n",
    "        expense_accounts = accounts[accounts[\"ProfitType\"].isin([\"Cost of Goods Sold\", \"Direct Operating Expenses\", \"Operating Overheads\", \"Other Expense\"])]\n",
    "        data[\"AmountDisplay\"] = data.apply(lambda x: -x[\"AmountCAD\"] if x[\"AccID\"] in expense_accounts.AccID.unique() else x[\"AmountCAD\"], axis=1)\n",
    "        data = self._perform_manual_adjust_GL_inventory(data)\n",
    "        self.gold_pl = data\n",
    "        self.gold_acc = accounts\n",
    "        # save files\n",
    "        print(\"Saving ...\")\n",
    "        self.check_file(self.gold_path[\"finance_operational\"])\n",
    "        data.to_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\", index=False)\n",
    "        accounts.to_excel(self.gold_path[\"finance_operational\"]/\"Account_table.xlsx\", sheet_name = \"Account\", index=False)\n",
    "        data.to_excel(self.gold_path[\"finance_operational\"]/\"PL.xlsx\", sheet_name=\"Transactions\", index=False)\n",
    "        for pillar in [\"Grain\", \"Cattle\", \"Seed\", \"Produce\"]:\n",
    "            data[data[\"Pillar\"]==pillar].to_excel(self.gold_path[\"pillar_dashboard\"]/pillar/\"PL.xlsx\", sheet_name=\"Transactions\", index=False)\n",
    "        self.pl_exist = True\n",
    "    \n",
    "    def _process_pp(self, data:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            This function takes original dataframe, apply the payperiod number classification based on transactions date, process payperiod columns, and return the new dataframe,\n",
    "                save the pp table for consolidated tables\n",
    "        \"\"\"\n",
    "        date_col = \"TransactionDate\" if \"TransactionDate\" in data.columns else \"date\"\n",
    "        # load payperiods\n",
    "        payperiods = pd.read_csv(self.gold_path[\"payroll\"]/\"Payperiods.csv\")\n",
    "        payperiods[\"START\"] = pd.to_datetime(payperiods[\"START\"])\n",
    "        payperiods[\"END\"] = pd.to_datetime(payperiods[\"END\"])\n",
    "        payperiods = payperiods.loc[:,[\"PP\",\"START\",\"END\",\"Cycle\",\"FiscalYear\"]]\n",
    "        payperiods = payperiods.rename(columns={\"PP\":\"PPNum\"})\n",
    "        payperiods[\"PPName\"] = payperiods[\"Cycle\"].astype(str).str.slice(2) + \"-\" + \"PP\" + payperiods[\"PPNum\"].astype(str).str.zfill(2)\n",
    "        # shift transaction dates - AZ: left 12 days, others: left 5 days\n",
    "        offset_days = {\"Arizona (produce)\": 12, \"Outlook\": 12}\n",
    "        data[\"days_offset\"] = data[\"Location\"].map(offset_days).fillna(5)\n",
    "        data[\"date_shifted\"] = data[date_col] - pd.to_timedelta(data[\"days_offset\"],unit=\"days\")\n",
    "        data = data[data[\"date_shifted\"]>=dt.datetime(2021,12,20)].copy(deep=True)\n",
    "        # construct interval index object for all periods\n",
    "        idx = pd.IntervalIndex.from_arrays(\n",
    "            left = payperiods[\"START\"],\n",
    "            right = payperiods[\"END\"],\n",
    "            closed = \"both\"\n",
    "        )\n",
    "        # determine which payperiod a transaction date belongs to by identifying the positional index inside the interval index object\n",
    "        pos = idx.get_indexer(data[\"date_shifted\"])\n",
    "        # extract payperiod info based on positional indices\n",
    "        ppnum = payperiods[\"PPNum\"].to_numpy()\n",
    "        ppname = payperiods[\"PPName\"].to_numpy()\n",
    "        cycle = payperiods[\"Cycle\"].to_numpy()\n",
    "        data[\"PPNum\"] = ppnum[pos]\n",
    "        data[\"PPName\"] = ppname[pos]\n",
    "        data[\"Cycle\"] = cycle[pos]\n",
    "        # create mapping for max fiscal year per payperiod to determine which fiscal year a payperiod should bleong to\n",
    "        mapping_table = data.groupby([\"days_offset\",\"PPName\"]).agg({\"FiscalYear\":\"max\"}).reset_index(drop=False)\n",
    "        data = data.drop(columns=[\"FiscalYear\"])\n",
    "        data = pd.merge(data, mapping_table, on=[\"days_offset\", \"PPName\"], how=\"left\")\n",
    "        # drop intermediate columns\n",
    "        data = data.drop(columns=[\"days_offset\", \"date_shifted\"])\n",
    "        # data.loc[:,[\"PPName\", \"PPNum\", \"Cycle\", \"FiscalYear\"]].drop_duplicates().to_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" / \"PayPeriods.csv\", index=False)\n",
    "        return data\n",
    "\n",
    "    def _process_units(self) -> None:\n",
    "        \"\"\" \n",
    "            this function read and process Unit files that contains unit numbers for each location\n",
    "        \"\"\"\n",
    "        acres = pd.read_csv(self.gold_path[\"payroll\"]/\"Unit.csv\",dtype={\"Location\":str, \"Unit\":float})\n",
    "        acres[\"Location\"] = acres[\"Location\"].str.strip()\n",
    "        doc_rename = {\"Airdrie Grain\": \"Airdrie (grain)\", \"Aridrie Cattle (head days 365)\":\"Airdrie\", \"Arizona All\":\"Arizona (produce)\",\n",
    "                    \"BC Cattle (head days 365)\":\"BritishColumbia (cattle)\", \"BC Produce\":\"BritishColumbia (produce)\", \n",
    "                    \"Box Elder\":\"Havre\", \"Eddystone Cattle (head days 365)\":\"Eddystone (cattle)\", \"Eddystone Grain\":\"Eddystone (grain)\",\n",
    "                    \"Monette Seeds CDN (avg met. ton)\":\"Seeds\", \"Monette Seeds USA\":\"Seeds USA\", \"NexGen (avg met. ton)\":\"NexGen\",\n",
    "                    \"Waldeck (head days 365)\":\"Waldeck\", \"Calderbank  (head days 365)\":\"Calderbank\"}\n",
    "        acres[\"Location\"] = acres[\"Location\"].replace(doc_rename)\n",
    "        acres[\"Pillar\"] = acres.apply(lambda x: self._pillar_classification(x),axis=1)\n",
    "        acres.to_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" /\"Unit_PowerBI.csv\",index=False)\n",
    "\n",
    "    def _payroll_project(self) -> None: \n",
    "        \"\"\" \n",
    "            will run _finance_operational() first\n",
    "            output: details + cost per unit (units per location input sheet) + average cost per unit for FY\n",
    "        \"\"\"\n",
    "        self.check_file(self.gold_path[\"payroll\"].parent/ \"OtherTables\")\n",
    "        print(\"\\nStarting Payroll Project Transformation\\n\")\n",
    "\n",
    "        # load and filter accounts for wages and contract labor\n",
    "        account = self.silver_acc[(self.silver_acc[\"Category\"].isin([\"Wages and benefits - direct\",\"Wages and benefits - overhead\"]) | (self.silver_acc[\"AccNum\"].isin([\"MFAZ595001\",\"MFBC536030\"])))] \n",
    "        # load only with transaction date later than 2021-12-20, and without \"Accrual\" in the memo\n",
    "        if self.is_dev:\n",
    "            data = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "        else:\n",
    "            if not self.pl_exist: self._finance_operational()\n",
    "            data = self.gold_pl.copy(deep=True)\n",
    "        data = data[data[\"AccID\"].isin(account.AccID.unique())]\n",
    "        data[\"TransactionDate\"] = pd.to_datetime(data[\"TransactionDate\"])\n",
    "        data = data[data[\"TransactionDate\"]>=dt.datetime(2021,12,20)].reset_index(drop=True)\n",
    "        data = data[~data[\"Memo\"].str.contains(\"Accrual\",case=False,na=False)]\n",
    "        # allocating payperiods\n",
    "        data = self._process_pp(data=data)\n",
    "        # standardizing location\n",
    "        # data.loc[data[\"Location\"]==\"Airdrie (corporate)\", \"Pillar\"] = \"Cattle\"                # deprecated\n",
    "        # data.loc[data[\"Location\"]==\"Airdrie (corporate)\", \"Location\"] = \"Airdrie (cattle)\"    # deprecated\n",
    "        data.loc[data[\"Location\"]==\"Eddystone (corporate)\", \"Pillar\"] = \"Unclassified\"\n",
    "        data.loc[data[\"Location\"]==\"Eddystone (corporate)\", \"Location\"] = \"Unassigned\"\n",
    "        data.loc[data[\"Location\"]==\"Legacy\", \"Location\"] = \"Unassigned\"\n",
    "        data.loc[(data[\"Location\"].str.contains(\"corporate\",case=False,na=False)&(data[\"Location\"]!=\"BritishColumbia (corporate)\")),\"Location\"] = \"Corporate\"\n",
    "        ## move BC ranches into BC Cattle\n",
    "        data.loc[(data[\"Location\"].isin(self.bc_ranches+[\"BritishColumbia (corporate)\"])), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        data.loc[data[\"Location\"] == \"BritishColumbia (cattle)\", \"Pillar\"] = \"Cattle\"\n",
    "        # summarizing data\n",
    "        ## by Location per PP\n",
    "        data_summarized = pd.DataFrame(data.groupby([\"Location\",\"PPName\",\"Pillar\",\"FiscalYear\",\"Cycle\",\"PPNum\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False))\n",
    "        assert len(data_summarized) == len(data.groupby([\"Location\",\"PPName\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)), \"Duplicated value detected for per Location per PP calculation\"\n",
    "        ## join acres data for CostPerUnit compute\n",
    "        print(\"Summarizing ...\")\n",
    "        acres = pd.read_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" /\"Unit_PowerBI.csv\",dtype={\"Location\":str, \"Unit\":float})\n",
    "        acres = acres.loc[:,[\"Location\", \"Unit\"]]\n",
    "        ### create BC cattle total units\n",
    "        total_bc = 0\n",
    "        for l in self.bc_ranches+[\"BritishColumbia (corporate)\"]:\n",
    "            total_bc += acres.loc[acres[\"Location\"]==l, \"Unit\"].item()\n",
    "        acres.loc[acres[\"Location\"]==\"BritishColumbia (cattle)\", \"Unit\"] = total_bc\n",
    "        acres[\"Unit\"] = acres[\"Unit\"].replace({0: 1})\n",
    "        print(f\"Unaccounted location for Acres Doc: {set(acres.Location.unique()) - set(data_summarized.Location.unique())}\")\n",
    "        print(f\"Unaccounted location for QBO Payroll: {set(data_summarized.Location.unique()) - set(acres.Location.unique())}\")\n",
    "        data_summarized = pd.merge(data_summarized, acres, on=\"Location\", how=\"left\")\n",
    "        data_summarized[\"CostPerUnit\"] = data_summarized[\"AmountDisplay\"] / data_summarized[\"Unit\"] * 26\n",
    "        data_summarized[\"Count\"] = 1\n",
    "        ## by Location\n",
    "        data_summarized2 = data_summarized.groupby(by=[\"Location\",\"FiscalYear\",\"Pillar\"]).agg({\"CostPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        data_summarized2 = data_summarized2.rename(columns={\"CostPerUnit\":\"Avg CostPerUnit\"})\n",
    "        assert len(data_summarized2) == len(data_summarized.groupby(by=[\"Location\",\"FiscalYear\"]).agg({\"CostPerUnit\":\"mean\"})), \"Duplicated value detected for per Location calculation\"\n",
    "        ## by pillar\n",
    "        data_summarized3 = data_summarized2.groupby(by=[\"FiscalYear\",\"Pillar\"]).agg({\"Avg CostPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(data_summarized3) == len(data_summarized.groupby(by=[\"Pillar\",\"FiscalYear\"]).agg({\"CostPerUnit\":\"mean\"})), \"Duplicated value detected for per Pillar calculation\"\n",
    "        # saving\n",
    "        print(\"Saving ...\")\n",
    "        self.check_file(self.gold_path[\"payroll\"])\n",
    "        data.to_excel(self.gold_path[\"payroll\"]/\"Payroll.xlsx\", sheet_name=\"Payroll\", index=False)\n",
    "        self.check_file(self.gold_path[\"hr_combined\"] / \"CSV\")\n",
    "        data_summarized.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"payroll_summarized1.csv\", index=False)\n",
    "        data_summarized2.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"payroll_summarized2.csv\", index=False)\n",
    "        data_summarized3.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"payroll_summarized3.csv\", index=False)\n",
    "\n",
    "    def _QBOTime_project(self) -> None:\n",
    "        \"\"\" \n",
    "            apply PP allocation to QBO Time data, clean locaiton, and join relevant info into one table\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting QBO Time Project Transformation\\n\")\n",
    "        # read files\n",
    "        timesheets = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"timesheets.csv\")\n",
    "        jobcode = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"jobcodes.csv\")\n",
    "        users = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"users.csv\")\n",
    "        group = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"group.csv\")\n",
    "        print(f\"Read {len(timesheets)} timesheet records, {len(jobcode)} jobcodes, {len(users)} users, {len(group)} groups\")\n",
    "        timesheets_len, users_len = len(timesheets), len(users)\n",
    "        # clean up location in group table\n",
    "        ## Arizona - all produce\n",
    "        group.loc[((group[\"corp_short\"]==\"A\")&(group[\"location_name\"]==\"Monette Farms AZ\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"A\")&(group[\"location_name\"]==\"Monette Produce USA\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"A\")&(group[\"location_name\"]==\"Monette Seeds USA\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        ## BC\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Ashcroft Ranch\")), \"Location\"] = \"Ashcroft\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Cache/Fischer/Loon\")), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"].str.contains(\"silage\", case=False))), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Diamond S Ranch\")), \"Location\"] = \"Diamond S\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Fraser River Ranch\")), \"Location\"] = \"Fraser River Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Home Ranch (70 Mile, LF/W, BR)\")), \"Location\"] = \"Home Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Moon Ranch\")), \"Location\"] = \"Moon Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Produce\")), \"Location\"] = \"BritishColumbia (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Wolf Ranch\")), \"Location\"] = \"Wolf Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"SAWP\")), \"Location\"] = \"BritishColumbia (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"SAWP Produce\")), \"Location\"] = \"BritishColumbia (produce)\"\n",
    "        ## Outlook\n",
    "        group.loc[((group[\"corp_short\"]==\"O\")), \"Location\"] = \"Outlook\"\n",
    "        ## others\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Airdrie\")), \"Location\"] = \"Airdrie\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"BC\")), \"Location\"] = \"Unassigned\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Calderbank\")), \"Location\"] = \"Calderbank\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Eddystone\")), \"Location\"] = \"Eddystone (unspecified)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Hafford\")), \"Location\"] = \"Hafford\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Kamsack\")), \"Location\"] = \"Kamsack\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"MFUSA Billings\")), \"Location\"] = \"Billings\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"MFUSA Box Elder\")), \"Location\"] = \"Havre\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Nexgen Seeds\")), \"Location\"] = \"NexGen\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Prince Albert\")), \"Location\"] = \"Prince Albert\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Raymore\")), \"Location\"] = \"Raymore\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Regina\")), \"Location\"] = \"Regina\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Russel Approvals\")), \"Location\"] = \"Unassigned\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Seeds\")), \"Location\"] = \"Seeds\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Swift Current\")), \"Location\"] = \"Swift Current\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"The Pas\")), \"Location\"] = \"The Pas\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Waldeck\")), \"Location\"] = \"Waldeck\"\n",
    "        unclassified = group[group[\"Location\"].isna()].location_name.unique()\n",
    "        if len(unclassified) > 0: print(f\"\\nUnclassified location - {unclassified}\\n\")\n",
    "        # create another location column for general location where bc ranches are merged into one\n",
    "        group = group.rename(columns={\"Location\": \"Location (detail)\"})\n",
    "        group[\"Location\"] = group[\"Location (detail)\"]\n",
    "        group.loc[(group[\"Location (detail)\"].isin(self.bc_ranches+[\"BritishColumbia (corporate)\"])), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        # merge tables into one table\n",
    "        ## merge location into users\n",
    "        users = pd.merge(users, group.loc[:,[\"group_id\", \"location_name\", \"Location\", \"Location (detail)\"]].drop_duplicates(), on=\"group_id\", how=\"left\")\n",
    "        ## merge users into timesheets\n",
    "        timesheets = pd.merge(timesheets,users.loc[:,[\"user_id\", \"group_id\", \"username\", \"full_name\", \"location_name\",\"Location\",\"Location (detail)\", \"first_name\", \"last_name\"]], on=\"user_id\", how=\"left\")\n",
    "        ## merge job into timesheets\n",
    "        timesheets = pd.merge(timesheets, jobcode.loc[:,[\"jobcode_id\",\"job_name\",\"type\"]].rename(columns={\"type\":\"job_type\"}), on=\"jobcode_id\", how=\"left\")\n",
    "        assert (len(users) == users_len) and (len(timesheets) == timesheets_len), f\"duplicated records found, timesheets - {timesheets_len} vs {len(timesheets)}; users - {users_len} vs {len(users)}\"\n",
    "        ## determine fiscal year\n",
    "        timesheets[\"date\"] = pd.to_datetime(timesheets[\"date\"])\n",
    "        timesheets[\"Month\"] = timesheets[\"date\"].dt.month_name()\n",
    "        timesheets[\"FiscalYear\"] = timesheets[\"date\"].dt.year \n",
    "        mask = timesheets[\"Month\"].isin([\"November\", \"December\"])\n",
    "        timesheets.loc[mask, \"FiscalYear\"] = timesheets.loc[mask, \"FiscalYear\"] + 1\n",
    "        # classify payperiods\n",
    "        timesheets = self._process_pp(data=timesheets)\n",
    "        # modify location for BC0\n",
    "        timesheets.loc[timesheets[\"user_id\"] == \"BC6107856\", \"Location\"] = \"Unassigned\"\n",
    "        # classify pillars\n",
    "        timesheets[\"Pillar\"] = timesheets.apply(lambda x: self._pillar_classification(x), axis=1)\n",
    "        timesheets.loc[timesheets[\"Pillar\"] == \"Missing\", \"Pillar\"] = \"Unclassified\"\n",
    "        # summarizing data\n",
    "        ## by Location per PP \n",
    "        summarized = timesheets.groupby([\"Location\",\"PPName\",\"FiscalYear\",\"Cycle\",\"PPNum\", \"Pillar\"]).agg({\"duration\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summarized) == len(timesheets.groupby([\"Location\",\"PPName\"]).agg({\"duration\":\"sum\"})), \"duplicated value detected for timsheet per Location per PP summarization\"\n",
    "        ## read units file\n",
    "        acres = pd.read_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" /\"Unit_PowerBI.csv\",dtype={\"Location\":str, \"Unit\":float})\n",
    "        acres = acres.loc[:,[\"Location\", \"Unit\"]]\n",
    "        print(f\"Unaccounted location for Acres Doc: {set(acres.Location.unique()) - set(summarized.Location.unique())}\")\n",
    "        print(f\"Unaccounted location for timesheets: {set(summarized.Location.unique()) - set(acres.Location.unique())}\")\n",
    "        ### create BC cattle + Billings total units\n",
    "        total_bc = 0\n",
    "        for l in self.bc_ranches+[\"BritishColumbia (corporate)\"]:\n",
    "            total_bc += acres.loc[acres[\"Location\"]==l, \"Unit\"].item()\n",
    "        acres.loc[acres[\"Location\"]==\"BritishColumbia (cattle)\", \"Unit\"] = total_bc\n",
    "        # acres.loc[acres[\"Location\"]==\"Billings\", \"Unit\"] = acres[acres[\"Location\"].isin([\"Fly Creek\", \"Camp 4\"])].Unit.sum()\n",
    "        acres[\"Unit\"] = acres[\"Unit\"].replace({0: 1})\n",
    "        ## merge with units file\n",
    "        summarized = pd.merge(summarized, acres, on=\"Location\", how=\"left\")\n",
    "        ## calculate hours per unit\n",
    "        summarized[\"HoursPerUnit\"] = summarized[\"duration\"] / summarized[\"Unit\"] * 26\n",
    "        summarized[\"Count\"] = 1\n",
    "        # summarize per location\n",
    "        summarized2 = summarized.groupby(by=[\"Location\",\"FiscalYear\", \"Pillar\"]).agg({\"HoursPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        summarized2 = summarized2.rename(columns={\"HoursPerUnit\":\"Avg HoursPerUnit\"})\n",
    "        assert len(summarized2) == len(timesheets.groupby([\"Location\",\"FiscalYear\"]).agg({\"duration\":\"sum\"})), \"duplicated value detected for timsheet per Location summarization\"\n",
    "        # summarize per pillar\n",
    "        summarized3 = summarized2.groupby(by=[\"FiscalYear\", \"Pillar\"]).agg({\"Avg HoursPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summarized3) == len(timesheets[timesheets[\"Pillar\"]!=\"Missing\"].groupby([\"Pillar\",\"FiscalYear\"]).agg({\"duration\":\"sum\"})), \"duplicated value detected for timsheet per Pillar summarization\"\n",
    "\n",
    "        # saving\n",
    "        print(\"Saving ...\\n\")\n",
    "        self.check_file(self.gold_path[\"QBOTime\"])\n",
    "        timesheets.to_excel(self.gold_path[\"QBOTime\"]/\"QBOTime.xlsx\", sheet_name = \"QBOTime\", index=False)\n",
    "        self.check_file(self.gold_path[\"hr_combined\"]/ \"CSV\")\n",
    "        summarized.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"time_summarized1.csv\", index=False)\n",
    "        summarized2.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"time_summarized2.csv\", index=False)\n",
    "        summarized3.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"time_summarized3.csv\", index=False)\n",
    "\n",
    "    def _hr_summary(self) -> None:\n",
    "        \"\"\" \n",
    "            This function consolidate payroll and QBO time summaries into one table for consolidated insights\n",
    "        \"\"\"\n",
    "        final_df = [pd.DataFrame(), pd.DataFrame(), pd.DataFrame()]\n",
    "        for i in [1, 2, 3]:\n",
    "            payroll = pd.read_csv(self.gold_path[\"hr_combined\"] / \"CSV\" / f\"payroll_summarized{i}.csv\")\n",
    "            payroll_rename = {\"AmountDisplay\": \"TotalAmount\", \"CostPerUnit\": \"AmountPerUnit\", \"Avg CostPerUnit\": \"Avg AmountPerUnit\"}\n",
    "            payroll = payroll.rename(columns=payroll_rename)\n",
    "            payroll[\"Mode\"] = \"Payroll\"\n",
    "            time = pd.read_csv(self.gold_path[\"hr_combined\"] / \"CSV\" / f\"time_summarized{i}.csv\")\n",
    "            time_rename = {\"duration\": \"TotalAmount\", \"HoursPerUnit\": \"AmountPerUnit\", \"Avg HoursPerUnit\": \"Avg AmountPerUnit\"}\n",
    "            time = time.rename(columns=time_rename)\n",
    "            time[\"Mode\"] = \"Hours\"\n",
    "            final_df[i-1] = pd.concat([payroll, time], ignore_index=True)\n",
    "        final_df[0].to_excel(self.gold_path[\"hr_combined\"]/\"Summarized.xlsx\", sheet_name=\"Summarized\", index=False)\n",
    "        final_df[1].to_excel(self.gold_path[\"hr_combined\"]/\"Summarized2.xlsx\", sheet_name=\"Summarized2\", index=False)\n",
    "        final_df[2].to_excel(self.gold_path[\"hr_combined\"]/\"Summarized3.xlsx\", sheet_name=\"Summarized3\", index=False)\n",
    "\n",
    "    def _temp_get_product(self, entry:str) -> str:\n",
    "        \"\"\" \n",
    "            temporary function for aligning product classification with Traction for QBO accounts, will change for HP\n",
    "        \"\"\"\n",
    "        entry = entry.lower()\n",
    "        if \"durum\" in entry:\n",
    "            return \"Durum\"\n",
    "        elif \"wheat\" in entry:\n",
    "            return \"Wheat\"\n",
    "        elif \"canola\" in entry:\n",
    "            return \"Canola\"\n",
    "        elif (\"chickpea\" in entry) or (\"garbanzo bean\" in entry):\n",
    "            return \"Chickpeas\"\n",
    "        elif (\"peas\" in entry) or (\"field pea\" in entry):\n",
    "            return \"Peas\"\n",
    "        elif \"barley\" in entry:\n",
    "            return \"Barley\"\n",
    "        elif \"green lentil\" in entry:\n",
    "            return \"Green Lentils\"\n",
    "        elif \"red lentil\" in entry:\n",
    "            return \"Red Lentils\"\n",
    "        elif \"oats\" in entry:\n",
    "            return \"Oats\"\n",
    "        elif \"corn\" in entry:\n",
    "            return \"Corn\"\n",
    "        else:\n",
    "            return \"Others\" \n",
    "\n",
    "    def _raw_inventory(self) -> None:\n",
    "        \"\"\" \n",
    "            prepare the data from raw QBO table for inventory project: only extracting partial Invoice, SalesReceipt, and Journal Entry\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Inventory Project Transformation ...\\n\")\n",
    "        corps = [\"MFL\", \"MFUSA\"]\n",
    "        cols = [\"TransactionDate\", \"TransactionType\", \"TransactionID\", \"Corp\", \"Qty\", \"AccID\", \"FarmID\", \"CustomerID\",\n",
    "                \"DocNumber\", \"TransactionEntered\", \"Amount\"]\n",
    "        journal_cols = [col for col in cols if col != \"Qty\"]\n",
    "        # read tables\n",
    "        print(\"Loading raw tables ...\")\n",
    "        account = self.silver_acc.copy(deep=True)\n",
    "        account = account[account[\"Corp\"].isin(corps)]\n",
    "        account = account[account[\"AccountType\"] == \"Income\"]\n",
    "        farm = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension_time\"]/\"Farm.csv\")\n",
    "        farm = farm[farm[\"Corp\"].isin(corps)]\n",
    "        customer = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension_time\"]/\"Customer.csv\")\n",
    "        customer = customer[customer[\"Corp\"].isin(corps)]\n",
    "        first_date = dt.datetime(2023,11,1)\n",
    "        invoice = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"Invoice.csv\")\n",
    "        invoice = invoice[invoice[\"Corp\"].isin(corps)]\n",
    "        invoice[\"TransactionDate\"] = pd.to_datetime(invoice[\"TransactionDate\"])\n",
    "        invoice = invoice[invoice[\"TransactionDate\"]>=first_date]\n",
    "        invoice = invoice[invoice[\"AccID\"].isin(account.AccID.unique())]\n",
    "        sales = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"SalesReceipt.csv\")\n",
    "        sales = sales[sales[\"Corp\"].isin(corps)]\n",
    "        sales[\"TransactionDate\"] = pd.to_datetime(sales[\"TransactionDate\"])\n",
    "        sales = sales[sales[\"TransactionDate\"]>=first_date]\n",
    "        sales = sales[sales[\"AccID\"].isin(account.AccID.unique())]\n",
    "        journal = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"JournalEntry.csv\", dtype={\"FarmID\":str, \"ClassID\":str, \"CustomerID\":str, \"EmployeeID\":str}, usecols=journal_cols)\n",
    "        journal = journal[journal[\"AccID\"].isin(account.AccID.unique())]\n",
    "        journal[\"TransactionDate\"] = pd.to_datetime(journal[\"TransactionDate\"])\n",
    "        journal = journal[journal[\"TransactionDate\"]>=first_date]\n",
    "        journal = journal[~journal[\"TransactionEntered\"].str.contains(\"Delivered and not settled\", na=False)]\n",
    "        journal = journal[~journal[\"TransactionEntered\"].str.contains(\"Grain Inventory Receivable Adjustment\", na=False)]\n",
    "        # combining tables\n",
    "        print(\"Combining Fact Tables ...\")\n",
    "        invoice = invoice.loc[:,[col for col in cols if col in invoice.columns]]\n",
    "        sales = sales.loc[:,[col for col in cols if col in sales.columns]]\n",
    "        journal = journal.loc[:,[col for col in cols if col in journal.columns]]\n",
    "        facts = pd.concat([invoice, sales, journal], ignore_index=True)\n",
    "        del invoice, sales, journal\n",
    "        # join facts with dimension tables\n",
    "        facts = pd.merge(facts, account.loc[:,[\"AccID\",\"AccNum\",\"AccName\",\"Category\",\"Subcategory\"]], on=[\"AccID\"], how=\"left\")\n",
    "        facts = pd.merge(facts, farm.loc[:,[\"FarmID\",\"FarmName\"]], on=[\"FarmID\"], how=\"left\")\n",
    "        facts = pd.merge(facts, customer.loc[:,[\"CustomerID\",\"CustomerName\"]], on=[\"CustomerID\"], how=\"left\")\n",
    "        facts = facts[facts[\"Subcategory\"]==\"Grain - cash settlements\"]\n",
    "        print(f\"Total Fact Entries - {len(facts)}\")\n",
    "        # product column\n",
    "        facts[\"Product\"] = facts[\"AccName\"].apply(lambda x: self._temp_get_product(x))\n",
    "        # saving file\n",
    "        print(\"Saving Files ...\")\n",
    "        self.check_file(self.gold_path[\"inventory\"])\n",
    "        facts.to_excel(self.gold_path[\"inventory\"]/\"Excel\"/\"QBO_Grain_Settlements.xlsx\", sheet_name=\"settlement\", index=False)\n",
    "        print(\"Finished\\n\")\n",
    "\n",
    "    def _buget_process_input(self, inputdata_path:Path, processed_path:Path) -> None:\n",
    "        \"\"\" \n",
    "            this function processes and saves budget totals for production, input (chem/fert/seed), produce budgets, and JD Lease\n",
    "        \"\"\"\n",
    "        ## commodity prices - everything is CAD except Winter Wheat is converted to USD\n",
    "        pricing = pd.read_csv(inputdata_path/\"25-Grain-Pricing.csv\")\n",
    "        pricing.loc[pricing[\"Commodity\"]==\"WW\", \"ForecastPrice\"] *= self.fx\n",
    "        ## production budget\n",
    "        budget_production = pd.read_csv(inputdata_path/\"25-Grain-Revenue.csv\")\n",
    "        budget_production = budget_production.melt(\n",
    "            id_vars=[\"Location\", \"Currency\", \"Type\"],\n",
    "            var_name=\"Commodity\",\n",
    "            value_name = \"Amount\"\n",
    "        )\n",
    "        budget_production = budget_production.fillna(value = {\"Amount\": 0})\n",
    "        budget_production[\"Commodity\"] = budget_production[\"Commodity\"].replace({\"Hay/Silage\":\"Hay\"})\n",
    "        budget_production.loc[((budget_production[\"Location\"]==\"Airdrie\")&(budget_production[\"Commodity\"]==\"Hay\")), \"Commodity\"] = \"Silage\" # only Airdrie has silage, others have hay\n",
    "        budget_production_summary = pd.DataFrame(budget_production.groupby([\"Location\",\"Currency\",\"Commodity\"]).agg({\"Amount\": \"prod\"})).reset_index(drop=False)\n",
    "        budget_production_summary = budget_production_summary.rename(columns={\"Amount\":\"TotalYield\"})\n",
    "        ### merge yield with commodity price to calculate forecast production value of commodities\n",
    "        budget_production_summary = pd.merge(budget_production_summary,pricing,on=[\"Commodity\"], how=\"left\")\n",
    "        ### manual adjustments to prices\n",
    "        budget_production_summary.loc[((budget_production_summary[\"Location\"] == \"Airdrie\") & (budget_production_summary[\"Commodity\"] == \"Hay\")), \"ForecastPrice\"] = 85\n",
    "        budget_production_summary.loc[((budget_production_summary[\"Location\"] == \"Colorado (Genoa)\") & (budget_production_summary[\"Commodity\"] == \"WW\")), \"ForecastPrice\"] = 13.75\n",
    "        budget_production_summary.loc[budget_production_summary[\"Location\"] == \"Yorkton\", \"ForecastPrice\"] *= 2/3\n",
    "        budget_production_summary[\"ForecastProductionCAD\"] = budget_production_summary[\"TotalYield\"] * budget_production_summary[\"ForecastPrice\"]\n",
    "        budget_production_summary = budget_production_summary[budget_production_summary[\"ForecastProductionCAD\"].notna()]\n",
    "        budget_production_summary = budget_production_summary[budget_production_summary[\"ForecastProductionCAD\"]!=0]\n",
    "        ### convert prices back to USD for a adjusted column\n",
    "        budget_production_summary[\"ForecastProductionAdj\"] = budget_production_summary.apply(lambda x: x[\"ForecastProductionCAD\"] / self.fx if x[\"Currency\"] == \"USD\" else x[\"ForecastProductionCAD\"],axis=1)\n",
    "        ### save production budget\n",
    "        budget_production_summary.to_csv(processed_path/\"budget_production.csv\",index=False)\n",
    "        ## input budget\n",
    "        input_budget = pd.read_csv(inputdata_path/\"25-Input-Budget.csv\")\n",
    "        input_budget = input_budget.drop(columns=[\"Total acres\"])\n",
    "        input_budget = input_budget.melt(\n",
    "            id_vars = [\"Location\", \"Type\"],\n",
    "            var_name = \"Commodity\",\n",
    "            value_name = \"Amount\"\n",
    "        )\n",
    "        input_budget = input_budget.fillna(value = {\"Amount\": 0})\n",
    "        input_budget.loc[((input_budget[\"Location\"]==\"Yorkton\")&(input_budget[\"Type\"].isin([\"Fertilizer\",\"Chemical\",\"Seed\"]))), \"Amount\"] *= 2/3\n",
    "        input_budget.to_csv(processed_path/\"input_budget.csv\",index=False)\n",
    "        ## labour budget\n",
    "        labour_budget = pd.read_csv(inputdata_path/\"25-Labour-Budget.csv\")\n",
    "        labour_budget = labour_budget.melt(\n",
    "            id_vars = [\"Location\",\"Currency\"],\n",
    "            var_name = \"Month\",\n",
    "            value_name = \"LabourBudgetCAD\"\n",
    "        )\n",
    "        labour_budget[\"LabourBudgetAdj\"] = labour_budget.apply(lambda x: x[\"LabourBudgetCAD\"]/self.fx if x[\"Currency\"]==\"USD\" else x[\"LabourBudgetCAD\"], axis=1)\n",
    "        labour_budget.to_csv(processed_path/\"labour_budget.csv\",index=False)\n",
    "        ## outlook budget\n",
    "        outlook = pd.read_csv(inputdata_path/\"25-Outlook-Detail.csv\")\n",
    "        outlook = outlook.melt(\n",
    "            id_vars=[\"Type\", \"ProfitType\"],\n",
    "            var_name=\"Commodity\",\n",
    "            value_name=\"Amount\"\n",
    "        )\n",
    "        outlook = outlook.fillna(value={\"Amount\": 0})\n",
    "        outlook.to_csv(processed_path/\"outlook_budget.csv\", index=False)\n",
    "        ## AZ budget\n",
    "        az = pd.read_csv(inputdata_path / \"25-AZ-Detail.csv\")\n",
    "        az = az.melt(\n",
    "            id_vars=[\"Type\", \"ProfitType\"],\n",
    "            var_name=\"CommodityRaw\",\n",
    "            value_name=\"AmountCAD\"\n",
    "        )\n",
    "        az = az.fillna(value={\"AmountCAD\": 0})\n",
    "        az.to_csv(processed_path/\"az_budget.csv\", index=False)\n",
    "        ## BC produce details\n",
    "        bc = pd.read_csv(inputdata_path / \"25-BC-Detail.csv\")\n",
    "        bc = bc.melt(\n",
    "            id_vars=[\"Type\", \"ProfitType\"],\n",
    "            var_name=\"CommodityRaw\",\n",
    "            value_name=\"AmountCAD\"\n",
    "        )\n",
    "        bc = bc.fillna(value={\"AmountCAD\": 0})\n",
    "        bc.to_csv(processed_path/\"bc_budget.csv\", index=False)\n",
    "        ## JD lease\n",
    "        jdlease = pd.read_csv(inputdata_path/\"25-JD-Lease-Summary.csv\")\n",
    "        jdlease = jdlease[jdlease[\"AllocatedCost25\"] != 0]\n",
    "        jdlease.to_csv(processed_path/\"JD_lease.csv\", index=False)\n",
    "\n",
    "    def _budget_read_outsidedata(self,processed_path:Path) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "        \"\"\" \n",
    "            this function reads all the processed outside data and standardize the commodity and location naming\n",
    "        \"\"\"\n",
    "        production_budget = pd.read_csv(processed_path/\"budget_production.csv\")\n",
    "        input_budget = pd.read_csv(processed_path/\"input_budget.csv\")\n",
    "        labour_budget = pd.read_csv(processed_path/\"labour_budget.csv\")\n",
    "        outlook_budget = pd.read_csv(processed_path/\"outlook_budget.csv\")\n",
    "        jdlease = pd.read_csv(processed_path/\"JD_lease.csv\")\n",
    "        az_budget = pd.read_csv(processed_path/\"az_budget.csv\")\n",
    "        bc_budget = pd.read_csv(processed_path/\"bc_budget.csv\")\n",
    "        ## standardizing commodity naming\n",
    "        production_rename_commodity = {\"R Lentils\":\"Red Lentil\", \"G Lentils\":\"Green Lentil\",\"Chickpeas\":\"Chickpea\",\"Peas\":\"Field Pea\", \"WW\": \"Winter Wheat\"}\n",
    "        input_rename_commodity = {\"R Lentils\":\"Red Lentil\", \"G Lentils\":\"Green Lentil\",\"Chickpeas\":\"Chickpea\", \"WW\": \"Winter Wheat\"}\n",
    "        outlook_rename_commodity = {\"Broccoli-cases/ac\":\"Broccoli\", \"Cabbage-lbs/ac\":\"Cabbage\", \"Carrots-lbs\":\"Carrot\", \"Cauliflower-cases/ac\":\"Cauliflower\",\n",
    "                                    \"Table Potato-lbs\":\"Potato\", \"Seed Potato-lbs\":\"Potato\", \"Commercial Pumpkins-Bins/ac\":\"Pumpkin\", \"Strawberry Upick-lbs\":\"Strawberry\",\n",
    "                                    \"Pumpkin Upick-pieces/ac\":\"Pumpkin\", \"Corn Maze-lbs\":\"Prairie Pathways\", \"WW\": \"Winter Wheat\", \"Corn (Sweet) Cobs\":\"Sweet Corn\"}\n",
    "        az_rename_commodity = {\"Broccoli-cases/ac\":\"Broccoli\", \"Cabbage-lbs/ac\":\"Cabbage\", \"Pumpkins-Bins/ac\":\"Pumpkin\", \"WatermelonLG-bins/ac\": \"Watermelon\",\n",
    "                            \"WatermelonMini-cases/ac\": \"Watermelon\"}\n",
    "        bc_rename_commodity = {\"Broccoli-cases/ac\":\"Broccoli\", \"WatermelonLG-bins/ac\": \"Watermelon\", \"WatermelonMini-cases/ac\": \"Watermelon\", \"Pumpkins-Bins/ac\":\"Pumpkin\",\n",
    "                            \"Squash-lbs\": \"Squash\"}\n",
    "        outlook_budget[\"CommodityRaw\"] = outlook_budget[\"Commodity\"]\n",
    "        production_budget[\"Commodity\"] = production_budget[\"Commodity\"].replace(production_rename_commodity)\n",
    "        input_budget[\"Commodity\"] = input_budget[\"Commodity\"].replace(input_rename_commodity)\n",
    "        outlook_budget[\"Commodity\"] = outlook_budget[\"Commodity\"].replace(outlook_rename_commodity)\n",
    "        az_budget[\"Commodity\"] = az_budget[\"CommodityRaw\"].replace(az_rename_commodity)\n",
    "        bc_budget[\"Commodity\"] = bc_budget[\"CommodityRaw\"].replace(bc_rename_commodity)\n",
    "        ## standardizing location naming - merge calderbank grain with Swift Current\n",
    "        jdlease_rename_location = {\"Swift Current Total\":\"Swift Current\", \"Regina Farm\":\"Regina\", \"Calderbank\":\"Swift Current\",\n",
    "                                \"Airdrie\":\"Airdrie (grain)\", \"Eddystone\":\"Eddystone (grain)\"}\n",
    "        labour_rename_location = {\"NexGen (avg met. ton)\":\"NexGen\", \"Cache/Fisher/Look\":\"Aschroft\", \"MF AZ\":\"Arizona (produce)\", \"Box Elder\":\"Havre\", \n",
    "                                \"BC Veg\":\"BritishColumbia (produce)\",\"Monette Seeds CDN (avg met. ton)\":\"Monette Seeds\", \n",
    "                                \"BC Cattle (avg head)\":\"BritishColumbia (cattle)\", \"Eddystone Cattle (avg head)\":\"Eddystone (cattle)\",\n",
    "                                \"Swift Current Cattle (avg head)\":\"Waldeck\", \"Aridrie Cattle (avg head)\":\"Airdrie (cattle)\",\n",
    "                                \"Airdrie Farm\":\"Airdrie (grain)\", \"Eddystone Farm\":\"Eddystone (grain)\",\"Calderbank\":\"Calderbank (cattle)\"}\n",
    "        input_rename_location =  {\"Fly Creek/Camp 1\":\"Fly Creek\", \"Regina Farm\":\"Regina\",\"Swift Current Total\":\"Swift Current\", \"Box Elder\":\"Havre\", \"Regina Farm\":\"Regina\",\n",
    "                                \"Calderbank\":\"Calderbank (grain)\",\"Airdrie\":\"Airdrie (grain)\", \"Eddystone\":\"Eddystone (grain)\"}\n",
    "        production_rename_location = {\"Fly Creek/Camp 1\":\"Fly Creek\", \"Regina Farm\":\"Regina\",\"Swift Current Total\":\"Swift Current\", \"Box Elder\":\"Havre\", \"Regina Farm\":\"Regina\",\n",
    "                                    \"Colorado (Genoa)\":\"Colorado\", \"Calderbank\":\"Swift Current\",\"Airdrie\":\"Airdrie (grain)\", \"Eddystone\":\"Eddystone (grain)\"}\n",
    "        input_budget[\"Location\"] = input_budget[\"Location\"].replace(input_rename_location)\n",
    "        production_budget[\"Location\"] = production_budget[\"Location\"].replace(production_rename_location)\n",
    "        labour_budget[\"Location\"] = labour_budget[\"Location\"].replace(labour_rename_location)\n",
    "        jdlease[\"Location\"] = jdlease[\"Location\"].replace(jdlease_rename_location)\n",
    "        ## put input budget (chem/fert/seed) into aggregated totals\n",
    "        input_budget2 = input_budget.groupby([\"Location\",\"Type\"]).agg({\"Amount\":\"sum\"}).reset_index(drop=False)\n",
    "        ## aggregated totals for production budget and JD Lease\n",
    "        production_budget = pd.DataFrame(production_budget.groupby([\"Location\",\"Currency\",\"Commodity\",\"ForecastPrice\"]).agg({\"TotalYield\":\"sum\", \"ForecastProductionCAD\":\"sum\", \"ForecastProductionAdj\":\"sum\"}).reset_index(drop=False))\n",
    "        jdlease = pd.DataFrame(jdlease.groupby([\"Location\",\"Country\",\"Currency\",\"TotalCost25\"]).agg({\"Acres25\":\"sum\",\"AllocatedCost25\":\"sum\"}).reset_index(drop=False))\n",
    "        return input_budget2, production_budget, labour_budget, jdlease, az_budget, bc_budget, outlook_budget\n",
    "\n",
    "    def _budget_process_produce(self, budget_rules:pd.DataFrame,budget:pd.DataFrame,sheetname:str) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            this function provides a standardized way to process produce budgets\n",
    "        \"\"\"\n",
    "        budget_rules = budget_rules[budget_rules[\"SheetRef\"] == sheetname].copy(deep=True)\n",
    "        budget_rules[\"Commodity\"] = budget_rules.apply(lambda x: self._identify_product(x,for_budget=True), axis=1)\n",
    "        budget[\"Type\"] = budget[\"Type\"].str.strip()\n",
    "        # gross income - by commodity\n",
    "        reference = budget[budget[\"Type\"].isin([\"Acres\",\"Unit Price\",\"YieldPerAc\"])]\n",
    "        reference = reference.groupby([\"Commodity\",\"ProfitType\",\"CommodityRaw\"]).agg({\"AmountCAD\":\"prod\"}).reset_index(drop=False)\n",
    "        reference = reference.groupby([\"Commodity\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False)\n",
    "        reference = reference.rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        reference[\"Category\"] = \"Produce - production\"\n",
    "        if \"outlook\" in sheetname.lower():\n",
    "            for item in [\"Prairie Pathways\", \"Market Garden / CSA\"]:\n",
    "                reference.loc[reference[\"Commodity\"] == item, \"Category\"] = \"Produce - cash settlements\"\n",
    "        # seed expense - by commodity\n",
    "        expense = budget[budget[\"Type\"] == \"Seed\"].copy(deep=True)\n",
    "        expense = expense.drop(columns=\"CommodityRaw\")\n",
    "        expense = expense.groupby([\"Commodity\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False).rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        expense[\"Category\"] = \"Seed\"\n",
    "        # other expense - Fertilizer/Chemical - not by commodity\n",
    "        expense2 = budget[budget[\"Type\"].isin([\"Fertilizer\",\"Chemical\"])]\n",
    "        expense2 = expense2.groupby([\"Type\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False).rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        expense2[\"Commodity\"] = \"Others\"\n",
    "        expense2 = expense2.rename(columns={\"Type\":\"Category\"})\n",
    "        # combine\n",
    "        budget_produce = pd.merge(budget_rules, pd.concat([reference,expense, expense2]), on=[\"Commodity\",\"Category\"], how=\"left\")\n",
    "        budget_produce = budget_produce.fillna(value={\"TotalAmountCAD\":0})\n",
    "        budget_produce[\"AmountCAD\"] = budget_produce.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "        return budget_produce\n",
    "\n",
    "    def _budget_get_transactions(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            get actuals\n",
    "        \"\"\"\n",
    "        if self.is_dev:\n",
    "            transactions = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "            self.operation_acc = pd.read_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\")\n",
    "        else:\n",
    "            transactions = self.gold_pl.copy(deep=True)\n",
    "        transactions = transactions[transactions[\"FiscalYear\"] >= 2024]\n",
    "        transactions[\"AccName\"] = transactions[\"AccName\"].str.strip()\n",
    "        return transactions\n",
    "\n",
    "    def _create_budget(self, process_input:bool = False) -> None:\n",
    "        \"\"\" \n",
    "            this function generates budgets\n",
    "        \"\"\"\n",
    "        location_adj = {\"Camp 4\": \"Billings\", \"Fly Creek\":\"Billings\"}\n",
    "        print(\"\\nCreating Budget\\n\")\n",
    "        if not self.is_dev:\n",
    "            if not self.pl_exist: self._finance_operational()\n",
    "        inputdata_path = self.gold_path[\"budget\"] / \"Outside Data\"\n",
    "        processed_path = self.gold_path[\"budget\"] / \"Processed Data\"\n",
    "        rule_path = self.gold_path[\"budget\"] / \"Budget Rules\"\n",
    "        copied_path = self.gold_path[\"budget\"]/\"Copied Data\"\n",
    "\n",
    "        # load actuals\n",
    "        transactions = self._budget_get_transactions()\n",
    "        \n",
    "        # process outside data\n",
    "        if process_input:\n",
    "            self._buget_process_input(inputdata_path=inputdata_path, processed_path=processed_path)\n",
    "        \n",
    "        # read outside data\n",
    "        input_budget2, production_budget, labour_budget, jdlease, az_budget, bc_budget, outlook_budget = self._budget_read_outsidedata(processed_path=processed_path)\n",
    "\n",
    "        # calculate Budgets\n",
    "        ## outside data\n",
    "        ### read rules\n",
    "        budget_rules = pd.read_csv(rule_path/\"OutsideData.csv\")\n",
    "        budget_rename_category = {\"Seed - farm\":\"Seed\"}\n",
    "        budget_rules[\"Category\"] = budget_rules[\"Category\"].replace(budget_rename_category)\n",
    "        ### separate locations into individual rows when they are separated with + in the rules df\n",
    "        budget_rules[\"Location\"] = budget_rules[\"Location\"].str.split(\"+\")\n",
    "        budget_rules = budget_rules.explode(\"Location\").reset_index(drop=True)\n",
    "        ### extract formula\n",
    "        budget_rules = budget_rules.melt(\n",
    "            id_vars=[\"Location\",\"Category\",\"AccFull\",\"SheetRef\"],\n",
    "            var_name=\"Month\",\n",
    "            value_name=\"Formula\"\n",
    "        )\n",
    "        budget_rules = budget_rules.fillna(value={\"Formula\":\"0\"})\n",
    "        budget_rules[\"Formula\"] = budget_rules[\"Formula\"].astype(str)\n",
    "        budget_rules[\"Formula\"] = budget_rules[\"Formula\"].replace({\"0\": \"*0\"})\n",
    "        ### calculating input budget for accounts per location\n",
    "        budget_rules_input = budget_rules[budget_rules[\"SheetRef\"] == \"Input Budget\"].copy(deep=True)\n",
    "        #### workaround input budget for Airdrie grain \n",
    "        input_budget2.loc[((input_budget2[\"Location\"]==\"Airdrie (grain)\")&(input_budget2[\"Type\"]==\"Acres\")),\"Type\"] = \"Custom work\"\n",
    "        ### merge budget rules with budget total per location\n",
    "        budget_input = pd.merge(budget_rules_input,input_budget2.rename(columns={\"Type\":\"Category\",\"Amount\":\"TotalAmountCAD\"}),on=[\"Location\",\"Category\"],how=\"left\")\n",
    "        #### revert back from workaround\n",
    "        input_budget2.loc[((input_budget2[\"Location\"]==\"Airdrie (grain)\")&(input_budget2[\"Type\"]==\"Custom work\")),\"Type\"] = \"Acres\"\n",
    "        ### apply the formula to compute per month\n",
    "        budget_input[\"AmountCAD\"] = budget_input.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"), axis=1)\n",
    "\n",
    "        ## production budget\n",
    "        ### combine Hay and Silage\n",
    "        production_budget[\"Commodity\"] = production_budget[\"Commodity\"].replace({\"Hay\":\"Hay/Silage\", \"Silage\":\"Hay/Silage\"})\n",
    "        ### add commodity column to budget rules\n",
    "        budget_rules_production = budget_rules[budget_rules[\"SheetRef\"] == \"Production Budget\"].copy(deep=True)\n",
    "        budget_rules_production[\"Commodity\"] = budget_rules_production.apply(lambda x: self._identify_product(x, for_budget=True), axis=1)\n",
    "        ### merge budget rules with budget totals\n",
    "        budget_production = pd.merge(budget_rules_production,production_budget.loc[:,[\"Location\",\"Commodity\",\"ForecastProductionCAD\"]].rename(columns={\"ForecastProductionCAD\":\"TotalAmountCAD\"}),\n",
    "                                         on = [\"Location\", \"Commodity\"], how=\"left\")\n",
    "        budget_production = budget_production.fillna(value={\"TotalAmountCAD\":0})\n",
    "        ### compute budget\n",
    "        budget_production[\"AmountCAD\"] = budget_production.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "\n",
    "        ## labour budget\n",
    "        budget_rules_labour = budget_rules[budget_rules[\"SheetRef\"] == \"Labour Budget\"].copy(deep=True)\n",
    "        budget_labour = pd.merge(budget_rules_labour, labour_budget.loc[:,[\"Location\",\"Month\",\"LabourBudgetCAD\"]].rename(columns={\"LabourBudgetCAD\":\"AmountCAD\"}),\n",
    "                                    on=[\"Location\",\"Month\"],how=\"left\")\n",
    "        \n",
    "        ## produce budgets\n",
    "        ### BC\n",
    "        budget_bc_produce = self._budget_process_produce(budget_rules=budget_rules,budget=bc_budget,sheetname=\"BC Produce Details\")\n",
    "        ### AZ\n",
    "        budget_az_produce = self._budget_process_produce(budget_rules=budget_rules,budget=az_budget,sheetname=\"AZ Details\")\n",
    "        ### outlook\n",
    "        budget_outlook = self._budget_process_produce(budget_rules=budget_rules,budget=outlook_budget.rename(columns={\"Amount\":\"AmountCAD\"}),sheetname=\"Outlook Details\")\n",
    "\n",
    "        ## JD lease\n",
    "        budget_rules_jd = budget_rules[budget_rules[\"SheetRef\"]==\"JD Lease\"].copy(deep=True)\n",
    "        budget_equipment = pd.merge(budget_rules_jd, jdlease.loc[:,[\"Location\",\"AllocatedCost25\"]].rename(columns={\"AllocatedCost25\":\"TotalAmountCAD\"}),\n",
    "                                        on = \"Location\", how = \"left\")\n",
    "        budget_equipment = budget_equipment.fillna(value={\"TotalAmountCAD\":0})\n",
    "        budget_equipment[\"AmountCAD\"] = budget_equipment.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "\n",
    "        ## adjustment for Swift Current\n",
    "        months = [\"April\", \"July\"]\n",
    "        for month in months:\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"].item()\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"].item()\n",
    "        months = [\"June\", \"September\"]\n",
    "        for month in months:\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"].item()\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"].item()\n",
    "        months = [\"May\", \"June\", \"September\"]\n",
    "        for month in months:\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"].item()\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"].item()\n",
    "        \n",
    "        # arithmetic rules\n",
    "        arithmetic = pd.read_csv(rule_path/\"Arithmetic.csv\")\n",
    "        ## faltten location\n",
    "        arithmetic[\"Location\"] = arithmetic[\"Location\"].str.split(\"+\")\n",
    "        arithmetic = arithmetic.explode(\"Location\").reset_index(drop=True)\n",
    "        arithmetic_rules = arithmetic.melt(\n",
    "            id_vars=[\"Location\",\"Category\",\"AccFull\", \"AccRef\", \"FixedRef\"],\n",
    "            var_name=\"Month\",\n",
    "            value_name=\"FormulaFull\"\n",
    "        )\n",
    "        ## housekeeping\n",
    "        arithmetic_rules = arithmetic_rules.fillna(value={\"FormulaFull\":\"FY-1*0\"})\n",
    "        arithmetic_rules[\"FormulaFull\"] = arithmetic_rules[\"FormulaFull\"].astype(str)\n",
    "        arithmetic_rules[\"FormulaFull\"] = arithmetic_rules[\"FormulaFull\"].replace({\"0\":\"FY-1*0\"})\n",
    "        arithmetic_rules[\"ReferenceYear\"] = arithmetic_rules[\"FormulaFull\"].str.slice(0,4)\n",
    "        arithmetic_rules[\"Formula\"] = arithmetic_rules[\"FormulaFull\"].str.slice(4)\n",
    "        arithmetic_rules = arithmetic_rules.fillna(value={\"Formula\": \"0\"})\n",
    "        arithmetic_rules[\"Formula\"] = arithmetic_rules[\"Formula\"].astype(str)\n",
    "        arithmetic_rules[\"Formula\"] = arithmetic_rules[\"Formula\"].replace({\"0\":\"*0\"})\n",
    "        ## separating Fixed records\n",
    "        ### processing arithmetic_rules for billings - 1.ignore camp 4, 2. rename Fly Creek to Billings\n",
    "        ###     this is to avoid applying the arithmetic twice for billings, and all accounts except on are fly creek + camp 4 (identical calculation)\n",
    "        ###     the only exception is for amortization, camp 4 was fixed and fly creek was arithmetic - ignore the fixed cost for camp 4 for fixed as well\n",
    "        ### adjusted in excel sheet instead\n",
    "        # arithmetic_rules = arithmetic_rules[~((arithmetic_rules[\"Location\"]==\"Camp 4\") & (arithmetic_rules[\"Category\"]))]\n",
    "        # arithmetic_rules[\"Location\"] = arithmetic_rules[\"Location\"].replace(location_adj)\n",
    "        arithmetic_rules_fixed = arithmetic_rules[arithmetic_rules[\"AccRef\"] == \"Fixed\"].copy(deep=True)\n",
    "        arithmetic_rules = arithmetic_rules[arithmetic_rules[\"AccRef\"]!=\"Fixed\"].copy(deep=True)\n",
    "\n",
    "        ## process fixed records\n",
    "        arithmetic_rules_fixed = arithmetic_rules_fixed.drop(columns=[\"FormulaFull\",\"ReferenceYear\"]).rename(columns={\"FixedRef\":\"TotalAmountCAD\"})\n",
    "        arithmetic_rules_fixed[\"AmountCAD\"] = arithmetic_rules_fixed.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "\n",
    "        ## Extract Account Info\n",
    "        arithmetic_rules[\"AccNum\"] = arithmetic_rules[\"AccRef\"].apply(lambda x: \"\".join(x.split(\" \")[0:2]))\n",
    "        arithmetic_rules[\"AccName\"] = arithmetic_rules[\"AccRef\"].apply(lambda x: (\" \".join(x.split(\" \")[2:]).strip()))\n",
    "        assert \"Fixd\" not in arithmetic_rules.ReferenceYear.unique(), \"Fixd records incorrectly classified\"\n",
    "        ## separate FY-1 & FY+1\n",
    "        arithmetic_rules_prior = arithmetic_rules[arithmetic_rules[\"ReferenceYear\"] == \"FY-1\"].copy(deep=True)\n",
    "        arithmetic_rules = arithmetic_rules[arithmetic_rules[\"ReferenceYear\"] == \"FY+1\"].copy(deep=True)\n",
    "\n",
    "        ## process FY-1 with actuals\n",
    "        actuals = transactions.groupby([\"Location\", \"AccNum\", \"AccName\", \"FiscalYear\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        arithmetic_rules_prior[\"FiscalYear\"] = self.currentFY - 1\n",
    "        assert len(actuals[actuals.duplicated(subset=[\"AccNum\",\"FiscalYear\",\"Location\"],keep=False)]) == 0, \"Duplicated AccNum detected for FY-1 Actuals\"\n",
    "        budget_prior = pd.merge(arithmetic_rules_prior,actuals.rename(columns={\"AmountDisplay\":\"TotalAmountCAD\"}),\n",
    "                                on = [\"Location\",\"AccNum\",\"FiscalYear\"], how=\"left\")\n",
    "        budget_prior = budget_prior.fillna(value={\"TotalAmountCAD\": 0})\n",
    "        budget_prior[\"AmountCAD\"] = budget_prior.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"), axis=1)\n",
    "\n",
    "        ## processing FY+1 with current budget\n",
    "        ### budget sales that is based on production budget input sheet\n",
    "        arithmetic_rules_sales = arithmetic_rules[arithmetic_rules[\"Category\"].str.contains(\"cash settlements\")].copy(deep=True)\n",
    "        production_reference = pd.concat([budget_production.copy(deep=True), budget_outlook.copy(deep=True), budget_az_produce.copy(deep=True),budget_bc_produce.copy(deep=True)])\n",
    "        production_reference = production_reference.groupby([\"Location\",\"AccFull\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False)\n",
    "        budget_sales = pd.merge(arithmetic_rules_sales,production_reference.rename(columns={\"AccFull\":\"AccRef\"}), on=[\"Location\",\"AccRef\"], how=\"left\")\n",
    "        budget_sales = budget_sales.rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        budget_sales[\"AmountCAD\"] = budget_sales.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "        budget_prior = pd.concat([budget_prior, budget_sales],ignore_index=True)\n",
    "\n",
    "        ### budget inventory adjustment \n",
    "        arithmetic_rules_inventory = arithmetic_rules[arithmetic_rules[\"Category\"].str.contains(\"inventory adjustment\",case=False)].copy(deep=True)\n",
    "        budget_inventory = pd.merge(arithmetic_rules_inventory, budget_prior.loc[:,[\"Location\",\"AccFull\",\"Month\",\"AmountCAD\"]].rename(columns={\"AccFull\":\"AccRef\"}),\n",
    "                                on = [\"Location\",\"AccRef\", \"Month\"], how = \"left\")\n",
    "        budget_inventory[\"AmountCAD\"] = -budget_inventory[\"AmountCAD\"]\n",
    "        budget_prior = pd.concat([budget_prior, budget_inventory], ignore_index=True)\n",
    "\n",
    "        ## combine with fixed budgets\n",
    "        budget_prior = pd.concat([budget_prior,arithmetic_rules_fixed],ignore_index=True)\n",
    "\n",
    "        # copied data\n",
    "        budget_copy = pd.read_csv(copied_path/\"Copied Data.csv\")\n",
    "        budget_copy = budget_copy.melt(\n",
    "            id_vars=[\"Location\",\"Category\",\"AccFull\"],\n",
    "            var_name = \"Month\",\n",
    "            value_name = \"AmountCAD\"\n",
    "        )\n",
    "        budget_copy = budget_copy.fillna(value={\"AmountCAD\":0})\n",
    "        budget_copy[\"AmountCAD\"] = budget_copy[\"AmountCAD\"].astype(float)\n",
    "        budget_copy[\"FiscalYear\"] = self.currentFY\n",
    "        budget_copy[\"AccRef\"] = \"Copy\"\n",
    "        budget_copy[\"ReferenceYear\"] = \"NA\"\n",
    "        budget_copy[\"Formula\"] = \"NA\"\n",
    "        budget_copy[\"TotalAmountCAD\"] = budget_copy[\"AmountCAD\"]\n",
    "        budget_copy.loc[budget_copy[\"Location\"]==\"Seeds USA\", \"AmountCAD\"] *= self.fx\n",
    "\n",
    "        # combining all budgets\n",
    "        budget_outside = pd.concat([budget_input,budget_production,budget_labour,budget_equipment, budget_outlook, budget_az_produce, budget_bc_produce],ignore_index=True)\n",
    "        budget_outside = budget_outside.drop(columns=[\"Commodity\"])\n",
    "        budget_prior = budget_prior.drop(columns=[\"FormulaFull\",\"AccNum\",\"AccName_x\", \"AccName_y\", \"AccName\", \"FixedRef\"])\n",
    "        budget_all = pd.concat([budget_outside,budget_prior,budget_copy],ignore_index=True)\n",
    "        budget_all[\"AccNum\"] = budget_all[\"AccFull\"].apply(lambda x: \"\".join(x.split(\" \")[0:2]))\n",
    "        budget_all[\"AccName\"] = budget_all[\"AccFull\"].apply(lambda x: \" \".join(x.split(\" \")[2:]))\n",
    "        budget_all[\"FiscalYear\"] = self.currentFY \n",
    "        budget_all[\"DataType\"] = \"Budget\"\n",
    "        # budget_all.loc[budget_all[\"Category\"].str.contains(\"inventory adjustment\",case=False), \"AmountCAD\"] *= -1 # turn the sign positive for inventory adjustments (my classification only)\n",
    "\n",
    "        # enbale location to be modified, e.g., for Camp 4 + Fly Creek = Billings\n",
    "        budget_all[\"LocationRaw\"] = budget_all[\"Location\"]\n",
    "        budget_all[\"Location\"] = budget_all[\"Location\"].replace(location_adj)\n",
    "\n",
    "        # save\n",
    "        self.check_file(self.gold_path[\"budget\"]/\"OutputFile\")\n",
    "        budget_all.to_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"budget_all.csv\", index=False)\n",
    "\n",
    "    def _budget_update(self, force_create:bool=True, force_process_input:bool=False) -> None:\n",
    "        \"\"\" \n",
    "            generate/update the actuals from the budget system\n",
    "        \"\"\"\n",
    "        print(\"\\nGenerating/Updating Actuals for budget system\\n\")\n",
    "        if self.is_dev:\n",
    "            self.operation_acc = pd.read_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\")\n",
    "            self.gold_pl = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "            self.fx = 1.3807\n",
    "        else:\n",
    "            if not self.pl_exist:\n",
    "                self._finance_operational()\n",
    "        budget_path = self.gold_path[\"budget\"]/\"OutputFile\"/\"budget_all.csv\"\n",
    "        if (not Path.exists(budget_path)) or force_create:\n",
    "            self._create_budget(process_input=force_process_input)\n",
    "        budget = pd.read_csv(budget_path)\n",
    "        budget = budget.loc[:,[\"Location\", \"SheetRef\", \"Month\", \"Formula\", \"TotalAmountCAD\", \"AmountCAD\", \"AccRef\", \"ReferenceYear\",\"FiscalYear\", \"AccNum\", \"DataType\", \"Category\"]]\n",
    "        budget_location_rename = {\"Airdrie (grain)\": \"Airdrie\", \"Airdrie (cattle)\": \"Airdrie\", \"Calderbank (cattle)\": \"Calderbank\",\n",
    "                                  \"Airdrie (corporate)\": \"Airdrie\", \"Seeds USA\":\"Arizona (produce)\"}\n",
    "        budget[\"Location\"] = budget[\"Location\"].replace(budget_location_rename)\n",
    "        # category_mapping = budget.loc[:,[\"AccNum\", \"Category\"]].drop_duplicates()     # problem with old changed AccNum mapped to incorrect Category\n",
    "        # organize Actuals\n",
    "        transactions = self._budget_get_transactions()\n",
    "        actuals_all = transactions.groupby([\"Location\",\"AccNum\", \"FiscalYear\", \"Month\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        actuals_all = actuals_all[actuals_all[\"FiscalYear\"] == self.currentFY]\n",
    "        actuals_all[\"DataType\"] = \"Actual\"\n",
    "        actuals_all = actuals_all.rename(columns={\"AmountDisplay\": \"AmountCAD\"})\n",
    "        # actuals_all = pd.merge(actuals_all,category_mapping,on=\"AccNum\",how=\"left\")   # problem with old changed AccNum mapped to incorrect Category\n",
    "        actuals_all.to_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"actuals_all.csv\", index=False)\n",
    "        print(f\"Location Unaccounted for in budget: {(set(budget.Location.unique()) - set(actuals_all.Location.unique()))}\")\n",
    "        # combine everything\n",
    "        all_all = pd.concat([budget,actuals_all],ignore_index=True)\n",
    "        all_all[\"FXRate\"] = self.fx\n",
    "        # reroute accounts for changing acc numbers\n",
    "        all_all[\"AccNum\"] = all_all[\"AccNum\"].replace(self.accnum_reroute)\n",
    "        # operational classification - AccNum to AccID mapping processed from _finance_operational() function\n",
    "        assert len(self.operation_acc[self.operation_acc.duplicated(subset=[\"AccNum\"],keep=False)]) == 0, \"Duplicated AccNum Detected - Operational Accounts Classification\"\n",
    "        self._extract_accnum_accid()\n",
    "        all_all[\"AccID\"] = all_all[\"AccNum\"].map(self.acc_map)\n",
    "        mismatch = all_all[all_all[\"AccID\"].isna()]\n",
    "        mismatch = mismatch[mismatch['AmountCAD']!=0]\n",
    "        print(f\"Total amount unaccounted for because of accnum mismatching - ${mismatch.AmountCAD.sum()}\")\n",
    "        print(f\"AccIDs with non-zero amount: {mismatch.AccNum.unique()}\")\n",
    "        # classify pillars\n",
    "        all_all[\"Pillar\"] = all_all.apply(lambda x: self._pillar_classification(x), axis=1)\n",
    "        # save\n",
    "        all_all.to_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"all_all.csv\", index=False)\n",
    "        self.check_file(self.gold_path[\"budget\"]/\"OutputPowerBI\")\n",
    "        if not self.is_dev: \n",
    "            print(\"Saving ...\")\n",
    "            all_all.to_excel(self.gold_path[\"budget\"]/\"OutputPowerBI\"/\"BudgetActual.xlsx\", sheet_name=\"Budget\", index=False)\n",
    "            for pillar in [\"Grain\", \"Cattle\", \"Seed\", \"Produce\"]:\n",
    "                all_all[all_all[\"Pillar\"]==pillar].to_excel(self.gold_path[\"pillar_dashboard\"]/pillar/\"BudgetActual.xlsx\", sheet_name=\"Budget\", index=False)\n",
    "            \n",
    "    def _create_additional_financial(self, summary:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            this function create Gross Margin, Contribution Margin, EBITDA, Net Income financial terms in the summary table\n",
    "                summary table must be broken down to fiscal year, month, location\n",
    "        \"\"\"\n",
    "        complement_data = summary.head(0).copy(deep=True)\n",
    "        for y in summary.FiscalYear.unique():\n",
    "            subset_year = summary[summary[\"FiscalYear\"] == y]\n",
    "            for m in summary.Month.unique():\n",
    "                subset_month = subset_year[subset_year[\"Month\"] == m]\n",
    "                for l in summary.Location.unique():\n",
    "                    subset_location = subset_month[subset_month[\"Location\"] == l]\n",
    "                    if len(subset_location) == 0:\n",
    "                        continue\n",
    "                    items = [\"Sales Revenue\", \"Cost of Goods Sold\", \"Direct Operating Expenses\", \"Other Operating Revenue\", \"Operating Overheads\", \"Other Income\", \"Other Expense\"]\n",
    "                    values = dict.fromkeys(items, 0)\n",
    "                    for i in items:\n",
    "                        if i in subset_location.ProfitType.unique():\n",
    "                            values[i] = subset_location.loc[subset_location[\"ProfitType\"]==i, \"AmountDisplay\"].item()\n",
    "                    gross_margin = values[\"Sales Revenue\"] - values[\"Cost of Goods Sold\"]\n",
    "                    contribution_margin = gross_margin - values[\"Direct Operating Expenses\"] + values[\"Other Operating Revenue\"]\n",
    "                    ebitda = contribution_margin - values[\"Operating Overheads\"]\n",
    "                    net_income = ebitda + values[\"Other Income\"] - values[\"Other Expense\"]\n",
    "                    pillar = subset_location.Pillar.unique().item()\n",
    "                    row = {\"FiscalYear\": y, \"Month\": m, \"Location\": l, \"Pillar\":pillar}\n",
    "                    row_GM = row | {\"ProfitType\": \"Gross Margin\", \"AmountDisplay\": gross_margin}\n",
    "                    row_CM = row | {\"ProfitType\": \"Contribution Margin\", \"AmountDisplay\": contribution_margin}\n",
    "                    row_ebitda = row | {\"ProfitType\": \"EBITDA\", \"AmountDisplay\": ebitda}\n",
    "                    row_NI = row | {\"ProfitType\": \"Net Income\", \"AmountDisplay\": net_income}\n",
    "                    complement_data.loc[len(complement_data)] = row_GM\n",
    "                    complement_data.loc[len(complement_data)] = row_CM \n",
    "                    complement_data.loc[len(complement_data)] = row_ebitda \n",
    "                    complement_data.loc[len(complement_data)] = row_NI \n",
    "        return complement_data\n",
    "\n",
    "    def _finance_summary(self, create_allocation_reference = True) -> None:\n",
    "        \"\"\" \n",
    "            this function assemble summary tables for financial income statement style, including Gross Margin, EBITDA, Net Income, \n",
    "                compared to (with % change compared to last year and budget)\n",
    "                    1. Last Year\n",
    "                    2. Budget\n",
    "                    3. month-by-month\n",
    "                includes Units (e.g., Acres)\n",
    "        \"\"\"\n",
    "        if self.is_dev:\n",
    "            self.operation_acc = pd.read_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\")\n",
    "            self.gold_pl = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "            self.fx = 1.3844\n",
    "        else:\n",
    "            if not self.pl_exist:\n",
    "                self._finance_operational()\n",
    "        # prepare dfs \n",
    "        data = self.gold_pl[self.gold_pl[\"FiscalYear\"] >=  2024].copy(deep=True)\n",
    "        account = self.gold_acc[self.gold_acc[\"AccountingType\"] == \"Income Statement\"]\n",
    "        # create AccID -> ProfitType mapping\n",
    "        id_prof_map = account.set_index(\"AccID\")[\"ProfitType\"]\n",
    "        data[\"ProfitType\"] = data[\"AccID\"].map(id_prof_map)\n",
    "        # summary by location, by pillar, by ProfitType, by Fiscal Year, by Month\n",
    "        summary = data.groupby([\"FiscalYear\", \"Month\", \"Location\", \"Pillar\", \"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summary) == len(data.groupby([\"FiscalYear\", \"Month\", \"Location\", \"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"})), \"duplicated location-pillar detected\"\n",
    "        # prepare df for additional financial lines, e.g., Gross Margin, Net Income, ...\n",
    "        complement_data = self._create_additional_financial(summary)\n",
    "        # concat two dfs\n",
    "        summary = pd.concat([summary, complement_data],ignore_index=True)\n",
    "        # assign datatype before budget summary\n",
    "        summary[\"DataType\"] = \"Actual\"\n",
    "        # read processed budget transactions\n",
    "        budget = pd.read_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"all_all.csv\")\n",
    "        budget = budget[budget[\"DataType\"] == \"Budget\"]\n",
    "        budget = budget.loc[:,[\"Location\", \"Month\", \"FiscalYear\", \"AmountCAD\", \"DataType\", \"AccID\", \"Pillar\",\"AccNum\"]]\n",
    "        budget = budget.rename(columns={\"AmountCAD\":\"AmountDisplay\"})\n",
    "        budget = budget[~((budget[\"AccID\"].isna())&(budget[\"AmountDisplay\"] == 0))].reset_index(drop=True)\n",
    "        assert len(budget[budget[\"AccID\"].isna()]) == 0, f\"Unaccounted accounts - {budget[budget[\"AccID\"].isna()].AccNum.unique()}\"\n",
    "        budget = budget.drop(columns=[\"AccNum\"])\n",
    "        # map ProfitType\n",
    "        budget[\"ProfitType\"] = budget[\"AccID\"].map(id_prof_map)\n",
    "        # budget summary\n",
    "        summary_budget = budget.groupby([\"FiscalYear\",\"Month\",\"Location\",\"Pillar\",\"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summary_budget) == len(budget.groupby([\"FiscalYear\",\"Month\",\"Location\",\"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"})), \"repeat Pillar detected when summarizing budget\"\n",
    "        # additional financial terms\n",
    "        complement_data_budget = self._create_additional_financial(summary_budget)\n",
    "        # final processing\n",
    "        summary_budget = pd.concat([summary_budget,complement_data_budget],ignore_index=True)\n",
    "        summary_budget[\"DataType\"] = \"Budget\"\n",
    "        print(f\"Location missing from budget - {set(summary.Location.unique()) - set(summary_budget.Location.unique())}\")\n",
    "        print(f\"Location missing from actual - {set(summary_budget.Location.unique()) - set(summary.Location.unique())}\")\n",
    "        # save\n",
    "        summary_all = pd.concat([summary, summary_budget],ignore_index=True)\n",
    "        summary_all.to_csv(self.gold_path[\"finance_operational\"]/\"ProfitTypeSummary.csv\", index=False) # for reclassifying accounts\n",
    "        summary_all.to_excel(self.gold_path[\"finance_operational\"]/\"ProfitTypeSummary.xlsx\", sheet_name=\"ProfitTypeSummary\", index=False)\n",
    "        for pillar in [\"Grain\", \"Cattle\", \"Seed\", \"Produce\"]:\n",
    "            summary_all[summary_all[\"Pillar\"]==pillar].to_excel(self.gold_path[\"pillar_dashboard\"]/pillar/\"ProfitTypeSummary.xlsx\", sheet_name=\"ProfitTypeSummary\", index=False)\n",
    "\n",
    "    def _APAR_concat_memo(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            This function concatenates all 'TransactionEntered' column per TransactionID_partial\n",
    "        \"\"\"\n",
    "        df_map = df.loc[:,[\"TransactionID_partial\", \"TransactionEntered\", \"Line_Id\", \"PrivateNote\", \"TransactionType\"]]\n",
    "        # fill missing TransactionEntered as Missing so it is string and can be concatenated\n",
    "        df_map = df_map.fillna(value={\"TransactionEntered\":\"Missing\", \"PrivateNote\": \"Missing\"})\n",
    "        # concatenate the line number into TransactionEntered so it's not too messy when concatenate TransactionEntered for multiple lines\n",
    "        df_map[\"TransactionEntered\"] = df_map[\"Line_Id\"].astype(str) + \". \" + df_map[\"TransactionEntered\"]\n",
    "        # concatenate -> one TransactionEntered per TransactionID_partial\n",
    "        df_map2 = df_map.sort_values(by=[\"TransactionID_partial\", \"Line_Id\"],ignore_index=True)\\\n",
    "                        .groupby([\"TransactionID_partial\",\"PrivateNote\",\"TransactionType\"])[\"TransactionEntered\"].agg(\" \".join).reset_index(drop=False)\n",
    "        assert len(df_map2[df_map2.duplicated(subset=\"TransactionID_partial\")]) == 0, \"duplicated transactionID spotted when creating TransactionEntered Concatenation\"\n",
    "        return df_map2\n",
    "    \n",
    "    def _APRporting_project(self, date:set[int] = None) -> None:\n",
    "        \"\"\" \n",
    "            This function connects APAgingDetails report from QBO API, and combined with raw tables such as Bill to form a comprehensive report that meets finance team's need\n",
    "                This function supports date input for processing the report at that exact date\n",
    "        \"\"\"\n",
    "        print(\"\\nProcessing AP Rerpot\\n\")\n",
    "        if date is not None:\n",
    "            assert len(date) == 3, f\"please pass the date as (YYYY, M, D), passed {date}\"\n",
    "            year, month, day = date\n",
    "        else:\n",
    "            year, month, day = self.today.year, self.today.month, self.today.day\n",
    "        # read APAging report from silver space\n",
    "        try:\n",
    "            report = pd.read_csv(self.silver_path[\"QBO\"][\"APAR\"] / \"AgedPayableDetail\" / str(year) / str(month) / f\"{day}.csv\")\n",
    "        except:\n",
    "            print(f'csv file not found at {self.silver_path[\"QBO\"][\"APAR\"] / \"AgedPayableDetail\" / str(year) / str(month) / f\"{day}.csv\"}')\n",
    "        report = report.rename(columns={\"TransactionTypeID\":\"TransactionID_partial\"})\n",
    "        report_transactiontype_rename = {\"Bill Payment (Cheque)\":\"BillPaymentCheck\", \"Bill Payment (Credit Card)\":\"BillPaymentCheck\", \"Bill Payment (Check)\":\"BillPaymentCheck\",\n",
    "                                        \"Cheque Expense\":\"Purchase\", \"Supplier Credit\": \"Vendor Credit\"}\n",
    "        # standardize the Transaction Types\n",
    "        report[\"TransactionType\"] = report[\"TransactionType\"].replace(report_transactiontype_rename)\n",
    "        # create mapping table from facts\n",
    "        cols = [\"TransactionDate\", \"TotalAmt\", \"PrivateNote\", \"APAccID\", \"DocNumber\", \"TransactionEntered\", \"Amount\", \"TransactionID\", \"VendorID\", \"FarmID\", \"TransactionID_partial\", \"Line_Id\",\n",
    "                \"AccID\"]\n",
    "        ## get bill table ready for mapping - only taking bill transactions that are relevant to AP report\n",
    "        bill_col = cols + [\"TermID\"]\n",
    "        bill = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"Bill.csv\",usecols=bill_col)\n",
    "        bill = bill[bill[\"TransactionID_partial\"].isin(report[report[\"TransactionType\"]==\"Bill\"][\"TransactionID_partial\"].unique())]\n",
    "        bill[\"TransactionType\"] = \"Bill\"\n",
    "        bill_map = self._APAR_concat_memo(bill) # results should only have 4 columns:  TransactionID_partial, PrivateNote, TransactionType, TransactionEntered (concatenated)\n",
    "        ## vendorcredit\n",
    "        vc = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"VendorCredit.csv\", usecols=cols)\n",
    "        vc = vc[vc[\"TransactionID_partial\"].isin(report[report[\"TransactionType\"]==\"Vendor Credit\"][\"TransactionID_partial\"].unique())]\n",
    "        vc[\"TransactionType\"] = \"Vendor Credit\"\n",
    "        vc_map = self._APAR_concat_memo(vc)\n",
    "        ## journal entry\n",
    "        journal_cols = cols + [\"JEType\"]\n",
    "        journal = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"JournalEntry.csv\", usecols=[x for x in journal_cols if x not in ['TotalAmt', 'APAccID']])\n",
    "        journal = journal[journal[\"TransactionID_partial\"].isin(report[report[\"TransactionType\"]==\"Journal Entry\"][\"TransactionID_partial\"].unique())]\n",
    "        journal[\"TransactionType\"] = \"Journal Entry\"\n",
    "        journal_map = self._APAR_concat_memo(journal)\n",
    "        # merging and save facts\n",
    "        fact = pd.concat([bill, vc, journal], ignore_index=True)\n",
    "        fact.to_excel(self.gold_path[\"APReporting\"]/\"Facts.xlsx\", index=False, sheet_name = \"Facts\")\n",
    "        # use map table to concate PrivateNote and TransactionEntered into AP Report\n",
    "        fact_map = pd.concat([bill_map, vc_map, journal_map],ignore_index=True)\n",
    "        report2 = pd.merge(report, fact_map, on=[\"TransactionID_partial\", \"TransactionType\"], how=\"left\")\n",
    "        assert len(report) == len(report2), \"After concatenating the TransactionEntered column, duplicated transactionID detected\"\n",
    "        report2.to_excel(self.gold_path[\"APReporting\"]/\"APReport.xlsx\", index=False, sheet_name=\"APReport\")\n",
    "        # save the Vendor and Location table\n",
    "        vendor = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension\"]/\"CSV\"/\"Vendor.csv\")\n",
    "        vendor.to_excel(self.gold_path[\"APReporting\"].parent / \"Vendor.xlsx\", sheet_name = \"Vendor\", index=False)\n",
    "        farm = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension\"]/\"CSV\"/\"Farm.csv\")\n",
    "        farm.to_excel(self.gold_path[\"APReporting\"].parent / \"Location.xlsx\", sheet_name = \"Location\", index=False)\n",
    "        print(\"AP Report Done \\n\")\n",
    "\n",
    "    def _HP_transformation(self) -> None:\n",
    "        \"\"\" \n",
    "            This function transforms the raw data from Harvest Profit in the csv format from Silver space to curated data in Gold space for Power BI\n",
    "        \"\"\"\n",
    "        print(\"\\nTransforming Harvest Profit Data\\n\")\n",
    "        # file paths\n",
    "        silver_path_Delivery_HP = self.silver_path[\"Delivery\"][\"HP\"]\n",
    "        gold_path_inventory = self.gold_path[\"inventory\"]\n",
    "        gold_HP = gold_path_inventory/\"Grain\"\n",
    "        product_path = gold_path_inventory / \"Tables\" / \"commodities_acc.csv\"\n",
    "        folder_fields = silver_path_Delivery_HP / \"Fields\" / \"2025\"\n",
    "        # read raw data\n",
    "        df = pd.read_csv(silver_path_Delivery_HP/f\"Loads_{self.today.year}_{self.today.month}.csv\")\n",
    "        # standardizing df\n",
    "        df_rename = {\"crop\": \"ProductRaw\", \"amount\":\"AmountRaw\", \"accepted_amount\":\"AcceptedAmountRaw\", \"amount_unit\":\"AmountRawUnit\", \"date\":\"TransactionDate\", \"entity_share\":\"LocationRaw\",\n",
    "                    \"contract\":\"ContractRaw\"}\n",
    "        df = df.rename(columns=df_rename)\n",
    "        df[\"TransactionDate\"] = pd.to_datetime(df[\"TransactionDate\"], format=\"%m/%d/%Y %I:%M %p\")\n",
    "        df[\"TransactionDate\"] = df[\"TransactionDate\"].dt.date\n",
    "        # standardize location\n",
    "        df_location_map = {\"MFB Billings - US\": \"Billings\", \"MFS Swift Current - CA\": \"SwiftCurrent\", \"MFH Hafford - CA\": \"Hafford\", \"MFK Kamsack - CA\":\"Kamsack\",\n",
    "                   \"MFR Regina - CA\": \"Regina\", \"MFPAS The Pas - CA\": \"ThePas\", \"MFPA  Prince Albert - CA\":\"PA\", \"MFRAY Raymore - CA\":\"Raymore\", \n",
    "                   \"MFE Eddystone - CA\": \"Eddystone\", \"MFO Outlook - CA\": \"Outlook\", \"MFAIR Airdrie - CA\": \"Airdrie\"}\n",
    "        df[\"Location\"] = df[\"LocationRaw\"].replace(df_location_map)\n",
    "        # compute MT\n",
    "        ## convert Bu to MT for every location excpet Billings where they might have legit Bu measures\n",
    "        # df.loc[((df[\"Location\"]!=\"Billings\")&(df[\"AmountRawUnit\"]==\"Bu\")&(df[\"harvest_profit_id\"]!=1010153)), \"AmountRawUnit\"] = \"Mt\"\n",
    "        bushels = df[df[\"AmountRawUnit\"] == \"Bu\"].copy(deep=True)\n",
    "        ## when units are not Bu\n",
    "        df = df[df[\"AmountRawUnit\"] != \"Bu\"].copy(deep=True)\n",
    "        ## create unit mapping table\n",
    "        units = [\"Lbs\", \"Kg\", \"Mt\"]\n",
    "        divisor = [self.conversion_mt_to_lb, 1000, 1]\n",
    "        mapping_table = pd.DataFrame(data={\"AmountRawUnit\": units, \"divisor\":divisor})\n",
    "        mapping_table = mapping_table.set_index(\"AmountRawUnit\")[\"divisor\"]\n",
    "        ## map mapping table and perform arithmetics\n",
    "        df[\"divisor\"] = df[\"AmountRawUnit\"].map(mapping_table)\n",
    "        df[\"AcceptedAmount\"] = df[\"AcceptedAmountRaw\"] / df[\"divisor\"]\n",
    "        df[\"Amount\"] = df[\"AmountRaw\"] / df[\"divisor\"]\n",
    "        df = df.drop(columns=[\"divisor\"])\n",
    "        ## when units are bushels\n",
    "        bushels[\"Amount\"] = bushels.apply(lambda x: x[\"AmountRaw\"] * self.conversion_bu_to_lb_canola / self.conversion_mt_to_lb if \"canola\" in x[\"ProductRaw\"].lower() \n",
    "                                          else x[\"AmountRaw\"] * self.conversion_bu_to_lb_others / self.conversion_mt_to_lb, axis=1)\n",
    "        bushels[\"AcceptedAmount\"] = bushels.apply(lambda x: x[\"AcceptedAmountRaw\"] * self.conversion_bu_to_lb_canola / self.conversion_mt_to_lb if \"canola\" in x[\"ProductRaw\"].lower() \n",
    "                                                  else x[\"AcceptedAmountRaw\"] * self.conversion_bu_to_lb_others / self.conversion_mt_to_lb, axis=1)\n",
    "        df = pd.concat([df, bushels], ignore_index=True)\n",
    "        # standardizing product - matching with QBO except Uncategorized Lentil\n",
    "        def _create_hp_product_mapping(products:list[str]) -> dict[str,str]:\n",
    "            \"\"\" \n",
    "                This function create mapping ProductRaw -> Product for Harvest Profit, only process strings, ignores missing values\n",
    "            \"\"\"\n",
    "            mapping = {}\n",
    "            for p in products:\n",
    "                if isinstance(p, str):\n",
    "                    p_lower = p.lower()\n",
    "                    if 'green lentil' in p_lower:\n",
    "                        mapping[p] = \"Green Lentil\"\n",
    "                    elif 'red lentil' in p_lower:\n",
    "                        mapping[p] = \"Red Lentil\"\n",
    "                    elif 'lentil' in p_lower:\n",
    "                        mapping[p] = \"Unclassified Lentil\"\n",
    "                    elif 'barley' in p_lower:\n",
    "                        mapping[p] = \"Barley\"\n",
    "                    elif 'canola' in p_lower:\n",
    "                        mapping[p] = \"Canola\"\n",
    "                    elif 'durum' in p_lower:\n",
    "                        mapping[p] = \"Durum\"\n",
    "                    elif 'winter' in p_lower and 'wheat' in p_lower:\n",
    "                        mapping[p] = \"Winter Wheat\"\n",
    "                    elif 'wheat' in p_lower:\n",
    "                        mapping[p] = \"Wheat\"\n",
    "                    elif 'chickpea' in p_lower:\n",
    "                        mapping[p] = \"Chickpea\"\n",
    "                    elif 'pea' in p_lower:\n",
    "                        mapping[p] = \"Field Pea\"\n",
    "                    elif 'carrots' in p_lower:\n",
    "                        mapping[p] = \"Carrots\"\n",
    "                    else:\n",
    "                        mapping[p] = \"Unrecognized\"\n",
    "            return mapping\n",
    "        product_mapping = _create_hp_product_mapping(df.ProductRaw.unique())\n",
    "        df[\"Product\"] = df[\"ProductRaw\"].map(product_mapping)\n",
    "        \n",
    "        # determine transfer mode\n",
    "        \n",
    "        ## use Location + From/To name to match fields_df \n",
    "        df[\"FromExtended\"] = df[\"Location\"] + \"-\" + df[\"from\"].str.strip()\n",
    "        df[\"FromExtended\"] = df[\"FromExtended\"].str.split(\",\").str[0]   # only look at first portion of multiple inputs to determine whether it's harvest records\n",
    "        df[\"ToExtended\"] = df[\"Location\"] + \"-\" + df[\"to\"].str.strip()\n",
    "        \n",
    "        ## create fields df\n",
    "        files = os.listdir(folder_fields)\n",
    "        fields_df = pd.DataFrame()\n",
    "        for f in files:\n",
    "            location = f.split(\".\")[0].split(\"_\")[-1]   # extract the last part (location name) after _ separater before .xlsx suffix\n",
    "            temp = pd.read_excel(folder_fields/f,dtype={\"Field\": str, \"Acres\":float})\n",
    "            temp[\"Location\"] = location\n",
    "            fields_df = pd.concat([fields_df, temp],ignore_index=True)\n",
    "        ### Location + Field to match with HP data\n",
    "        fields_df[\"FieldRaw\"] = fields_df[\"Field\"]\n",
    "        fields_df[\"Field\"] = fields_df[\"Location\"] + \"-\" + fields_df[\"FieldRaw\"]\n",
    "        fields_df.to_csv(gold_HP / \"HPFields.csv\", index=False)\n",
    "        \n",
    "        ## determine whether a load is from field\n",
    "        ### determine if a load is harvest from a field\n",
    "        df[\"FromField\"] = df[\"FromExtended\"].isin(fields_df[\"Field\"].unique())\n",
    "        df[\"ToField\"] = df[\"ToExtended\"].isin(fields_df[\"Field\"].unique())\n",
    "        ### when the load is from field (harvest) and to is missing, flag those loads\n",
    "        df.loc[((df[\"FromField\"])&(df[\"to\"].isna())), \"Flag\"] = \"Harvest with Unknown Destination (Bins)\"\n",
    "        \n",
    "        ## determine whether a load is from/to bins and identify direct deliveries\n",
    "        ### read bins records\n",
    "        bins_df = pd.read_csv(gold_HP / \"HPBins.csv\", dtype={\"Bins\":str, \"Location\":str})\n",
    "        bins_df[\"IsDirect\"] = bins_df[\"Bins\"].str.contains(r\"(?i:direct)|FTC|D\\.C\\.|(?i:hardin)|(?i:huntley)\", regex=True)\n",
    "        bins_df[\"Bins\"] = bins_df[\"Location\"] + \"-\" + bins_df[\"Bins\"]\n",
    "        ### determine load from/to bins\n",
    "        df[\"FromBins\"] = df[\"FromExtended\"].isin(bins_df[~bins_df[\"IsDirect\"]][\"Bins\"].unique())\n",
    "        df[\"ToBins\"] = df[\"ToExtended\"].isin(bins_df[~bins_df[\"IsDirect\"]][\"Bins\"].unique())\n",
    "        ## transfer_mode - applicable for valid records only\n",
    "        ### transfer_mode = Harvest -> FromField==True & ToBins==True\n",
    "        df.loc[(df[\"FromField\"]&df[\"ToBins\"]), \"TransferMode\"] = \"Harvest\"\n",
    "        ### transfer_mode = Sales -> FromBins==True & ToBins==False & ToField==False\n",
    "        df.loc[((df[\"FromBins\"])&(~df[\"ToBins\"])&(~df[\"ToField\"])), \"TransferMode\"] = \"Sales\"\n",
    "        ### transfer_mode = BinTransfer -> FromBins==True & ToBins==True\n",
    "        df.loc[(df[\"FromBins\"]&df[\"ToBins\"]), \"TransferMode\"] = \"BinTransfer\"\n",
    "        ## identify direct deliveries\n",
    "        mask = (\n",
    "            df[\"FromExtended\"].isin(bins_df[bins_df[\"IsDirect\"]].Bins.unique()) | \n",
    "            df[\"ToExtended\"].isin(bins_df[bins_df[\"IsDirect\"]].Bins.unique())\n",
    "\n",
    "        )\n",
    "        df[\"IsDirect\"] = mask \n",
    "        ### in addition, if ToBins==False & FromField==True & to is not missing , this is also direct delivery\n",
    "        df.loc[((df[\"FromField\"])&(df[\"to\"].notna())&(~df[\"ToBins\"])), \"IsDirect\"] = True\n",
    "        ### transfer_mode = Sales -> Direct Delivery\n",
    "        df.loc[df[\"IsDirect\"], \"TransferMode\"] = \"Sales\"\n",
    "        \n",
    "        ## invalid loads\n",
    "        df = df.fillna(value={\"TransferMode\": \"Invalid\"})\n",
    "        df.loc[df[\"TransferMode\"]!=\"Invalid\", \"Flag\"] = \"Valid Loads\"\n",
    "        ### flag invalid loads\n",
    "        #### when both from & to is missing, flag those records\n",
    "        df.loc[((df[\"from\"].isna())&(df[\"to\"].isna())), \"Flag\"] = \"[from] and [to] Unassigned\"\n",
    "        #### when from is missing and to is not a field or a bin, assume those records are to a customer, without stating which inventory it is from\n",
    "        df.loc[((df[\"TransferMode\"]==\"Invalid\")&(df[\"from\"].isna())&(~df[\"ToBins\"])&(df[\"to\"].notna())), \"Flag\"] = \"Deliveries with Unknown Origin (Bins)\"\n",
    "        #### when to is a bin (most likely from harvest), and from is missing\n",
    "        df.loc[((df[\"TransferMode\"]==\"Invalid\")&(df[\"from\"].isna())&(df[\"ToBins\"])), \"Flag\"] = \"Harvest with Unknown Origin (Field)\"\n",
    "        exception = df[df[\"Flag\"].isna()]\n",
    "        if len(exception) != 0:\n",
    "            print(f\"\\nThere are {len(exception)} number of exceptions unaccounted for in \\n{exception.Location.value_counts()}\\n\")\n",
    "        \n",
    "        # create LoadType label to include invalid loads\n",
    "        df.loc[(df[\"IsDirect\"]), \"LoadType\"] = \"Direct Delivery\"\n",
    "        df.loc[(df[\"FromField\"] | df[\"ToBins\"]) & (~df[\"IsDirect\"]), \"LoadType\"] = \"Harvest\"\n",
    "        df.loc[((~(df[\"FromField\"] | df[\"ToBins\"])) & (~df[\"IsDirect\"])&(df[\"to\"].notna())), \"LoadType\"] = \"Delivery from Inventory\"\n",
    "        df.loc[((~(df[\"FromField\"] | df[\"ToBins\"])) & (~df[\"IsDirect\"])&(df[\"to\"].isna())), \"LoadType\"] = \"Undefined Loads\"\n",
    "        df.loc[((~df[\"FromBins\"])&(~df[\"FromField\"])&(df[\"from\"].notna())&(~df[\"IsDirect\"])), \"LoadType\"] = \\\n",
    "            df.loc[((~df[\"FromBins\"])&(~df[\"FromField\"])&(df[\"from\"].notna())&(~df[\"IsDirect\"])), \"TransferMode\"] = \"Not Load\"\n",
    "        df.loc[df[\"TransferMode\"]==\"BinTransfer\", \"LoadType\"] = \"BinTransfer\"\n",
    "\n",
    "        # bin inventory - create a column to store bin no matter if it is from or to a bin\n",
    "        direct_others = df[df[\"LoadType\"].isin([\"Direct Delivery\",\"Undefined Loads\",\"Not Load\",\"BinTransfer\"])].copy(deep=True)\n",
    "        harvest = df[df[\"LoadType\"]==\"Harvest\"].copy(deep=True)\n",
    "        harvest[\"Bin\"] = harvest.loc[harvest[\"ToBins\"], \"to\"]\n",
    "        others = df[df[\"LoadType\"]==\"Delivery from Inventory\"].copy(deep=True)\n",
    "        others[\"Bin\"] = others[\"from\"]\n",
    "        df = pd.concat([harvest, others, direct_others], ignore_index=True)\n",
    "\n",
    "        # create MT monitoring mechanism\n",
    "        ranges = [0, 10, 36, 48, 95, 10000]\n",
    "        ranges_indexer = pd.IntervalIndex.from_arrays(left=ranges[:-1], right=ranges[1:], closed=\"left\")\n",
    "        range_names = np.array([\"Invalid-Small:<10\", \"Unusual-Small:10-36\", \"Valid:36-48\", \"Invalid-Large:48-95\", \"Invalid-Large:>95\"], dtype=str)\n",
    "        amounts_category = ranges_indexer.get_indexer(df[\"AcceptedAmount\"])\n",
    "        assert -1 not in amounts_category, \"Amount MT monitoring detected <0 or >10,000 values\"\n",
    "        df[\"AmountCategory\"] = range_names[amounts_category]\n",
    "        ## for The Pas and Billings, 48 - 95 is valid\n",
    "        df.loc[(df[\"Location\"].isin([\"Billings\",\"ThePas\"])& (df[\"AmountCategory\"]==\"Invalid-Large:48-95\")), \"AmountCategory\"] = \"Valid:48-95\"\n",
    "\n",
    "\n",
    "        # saving\n",
    "        print(f\"Transformed {len(df)} loads, saving ...\")\n",
    "        df.to_csv(gold_HP / \"hp.csv\", index=False)\n",
    "        df.to_excel(gold_HP / \"hp.xlsx\", sheet_name=\"Loads\", index=False)\n",
    "\n",
    "\n",
    "    def run(self, force_run_time:bool=False, force_create_budget:bool=True, force_process_budget_input:bool=False) -> None:\n",
    "        start = perf_counter()\n",
    "\n",
    "        self._process_units()\n",
    "\n",
    "        # pure finance projects\n",
    "        self._weekly_banking()\n",
    "        self._APRporting_project()\n",
    "\n",
    "\n",
    "        # financial operational related projects\n",
    "        self._finance_operational()\n",
    "        self._budget_update(force_create=force_create_budget, force_process_input=force_process_budget_input)\n",
    "        self._finance_summary()\n",
    "\n",
    "        # payroll related\n",
    "        self._payroll_project()\n",
    "        if force_run_time or (self.today.weekday() in [0, 2, 6]): self._QBOTime_project()\n",
    "        self._hr_summary()\n",
    "\n",
    "        # inventory\n",
    "        self._raw_inventory()\n",
    "        self._HP_transformation()\n",
    "\n",
    "        end = perf_counter()\n",
    "        print(f\"\\nProjects Transformation Finished with {(end-start)/60:.3f} minutes\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1830974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e74dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17746399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b89404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672615f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8c6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c657ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2b27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091ec61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273d38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d2586d9",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16da3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Projects()\n",
    "gold_HP = self.gold_path[\"inventory\"]/\"Grain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee85e673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductRaw</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>AmountRaw</th>\n",
       "      <th>AcceptedAmountRaw</th>\n",
       "      <th>AmountRawUnit</th>\n",
       "      <th>details</th>\n",
       "      <th>harvest_profit_id</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>truck</th>\n",
       "      <th>...</th>\n",
       "      <th>FromField</th>\n",
       "      <th>ToField</th>\n",
       "      <th>FromBins</th>\n",
       "      <th>ToBins</th>\n",
       "      <th>TransferMode</th>\n",
       "      <th>IsDirect</th>\n",
       "      <th>Flag</th>\n",
       "      <th>LoadType</th>\n",
       "      <th>Bin</th>\n",
       "      <th>AmountCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barley</td>\n",
       "      <td>Tamrack</td>\n",
       "      <td>MFE Bag 01</td>\n",
       "      <td>59208.90</td>\n",
       "      <td>59208.90</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>983835</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>MFE Bag 01</td>\n",
       "      <td>Unusual-Small:10-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barley</td>\n",
       "      <td>Tamrack</td>\n",
       "      <td>MFE Bag 01</td>\n",
       "      <td>2027.92</td>\n",
       "      <td>2027.92</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>983838</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>MFE Bag 01</td>\n",
       "      <td>Invalid-Small:&lt;10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barley</td>\n",
       "      <td>Tamrack</td>\n",
       "      <td>Bin 12</td>\n",
       "      <td>27594.41</td>\n",
       "      <td>27594.41</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991930</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>Bin 12</td>\n",
       "      <td>Unusual-Small:10-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barley</td>\n",
       "      <td>Tamrack</td>\n",
       "      <td>MFE Bag 01</td>\n",
       "      <td>23603.71</td>\n",
       "      <td>23603.71</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>992098</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>MFE Bag 01</td>\n",
       "      <td>Unusual-Small:10-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barley</td>\n",
       "      <td>Tamrack</td>\n",
       "      <td>Bin 13</td>\n",
       "      <td>50998.35</td>\n",
       "      <td>50998.35</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>992109</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>Bin 13</td>\n",
       "      <td>Unusual-Small:10-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19962</th>\n",
       "      <td>Canola</td>\n",
       "      <td>FC Bag 40</td>\n",
       "      <td>FC Bin 24</td>\n",
       "      <td>-768655.00</td>\n",
       "      <td>-768655.00</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>oct 8 2025</td>\n",
       "      <td>1126578</td>\n",
       "      <td>2025-10-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>FC Bag 40</td>\n",
       "      <td>Invalid-Large:&gt;95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19963</th>\n",
       "      <td>Wheat (Hard Red Winter)</td>\n",
       "      <td>FC Bag 14</td>\n",
       "      <td>FC Wheat Seed Cleaned and planted fall 2025</td>\n",
       "      <td>-12000.00</td>\n",
       "      <td>-12000.00</td>\n",
       "      <td>Bu</td>\n",
       "      <td>Cleaned seed</td>\n",
       "      <td>1027883</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>FC Bag 14</td>\n",
       "      <td>Invalid-Large:&gt;95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19964</th>\n",
       "      <td>Canola</td>\n",
       "      <td>FC Bag 39</td>\n",
       "      <td>FC Bin 24</td>\n",
       "      <td>-17334.00</td>\n",
       "      <td>-17334.00</td>\n",
       "      <td>Bu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1201200</td>\n",
       "      <td>2025-10-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>FC Bag 39</td>\n",
       "      <td>Invalid-Large:&gt;95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19965</th>\n",
       "      <td>Canola</td>\n",
       "      <td>FC Bag 38</td>\n",
       "      <td>FC Bin 24</td>\n",
       "      <td>-17084.00</td>\n",
       "      <td>-17084.00</td>\n",
       "      <td>Bu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1201201</td>\n",
       "      <td>2025-10-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>FC Bag 38</td>\n",
       "      <td>Invalid-Large:&gt;95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19966</th>\n",
       "      <td>Canola</td>\n",
       "      <td>FC Bag 32</td>\n",
       "      <td>FC Bin 14</td>\n",
       "      <td>-1241.00</td>\n",
       "      <td>-1241.00</td>\n",
       "      <td>Bu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1201202</td>\n",
       "      <td>2025-10-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>False</td>\n",
       "      <td>Valid Loads</td>\n",
       "      <td>BinTransfer</td>\n",
       "      <td>FC Bag 32</td>\n",
       "      <td>Unusual-Small:10-36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19967 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ProductRaw       from  \\\n",
       "0                       Barley    Tamrack   \n",
       "1                       Barley    Tamrack   \n",
       "2                       Barley    Tamrack   \n",
       "3                       Barley    Tamrack   \n",
       "4                       Barley    Tamrack   \n",
       "...                        ...        ...   \n",
       "19962                   Canola  FC Bag 40   \n",
       "19963  Wheat (Hard Red Winter)  FC Bag 14   \n",
       "19964                   Canola  FC Bag 39   \n",
       "19965                   Canola  FC Bag 38   \n",
       "19966                   Canola  FC Bag 32   \n",
       "\n",
       "                                                to  AmountRaw  \\\n",
       "0                                       MFE Bag 01   59208.90   \n",
       "1                                       MFE Bag 01    2027.92   \n",
       "2                                           Bin 12   27594.41   \n",
       "3                                       MFE Bag 01   23603.71   \n",
       "4                                           Bin 13   50998.35   \n",
       "...                                            ...        ...   \n",
       "19962                                    FC Bin 24 -768655.00   \n",
       "19963  FC Wheat Seed Cleaned and planted fall 2025  -12000.00   \n",
       "19964                                    FC Bin 24  -17334.00   \n",
       "19965                                    FC Bin 24  -17084.00   \n",
       "19966                                    FC Bin 14   -1241.00   \n",
       "\n",
       "       AcceptedAmountRaw AmountRawUnit       details  harvest_profit_id  \\\n",
       "0               59208.90           Lbs           NaN             983835   \n",
       "1                2027.92           Lbs           NaN             983838   \n",
       "2               27594.41           Lbs           NaN             991930   \n",
       "3               23603.71           Lbs           NaN             992098   \n",
       "4               50998.35           Lbs           NaN             992109   \n",
       "...                  ...           ...           ...                ...   \n",
       "19962         -768655.00           Lbs    oct 8 2025            1126578   \n",
       "19963          -12000.00            Bu  Cleaned seed            1027883   \n",
       "19964          -17334.00            Bu           NaN            1201200   \n",
       "19965          -17084.00            Bu           NaN            1201201   \n",
       "19966           -1241.00            Bu           NaN            1201202   \n",
       "\n",
       "      TransactionDate truck  ... FromField  ToField  FromBins  ToBins  \\\n",
       "0          2025-08-07   NaN  ...      True    False     False    True   \n",
       "1          2025-08-07   NaN  ...      True    False     False    True   \n",
       "2          2025-08-16   NaN  ...      True    False     False    True   \n",
       "3          2025-08-16   NaN  ...      True    False     False    True   \n",
       "4          2025-08-16   NaN  ...      True    False     False    True   \n",
       "...               ...   ...  ...       ...      ...       ...     ...   \n",
       "19962      2025-10-08   NaN  ...     False    False      True    True   \n",
       "19963      2025-09-05   NaN  ...     False    False      True    True   \n",
       "19964      2025-10-26   NaN  ...     False    False      True    True   \n",
       "19965      2025-10-26   NaN  ...     False    False      True    True   \n",
       "19966      2025-10-26   NaN  ...     False    False      True    True   \n",
       "\n",
       "       TransferMode  IsDirect         Flag     LoadType         Bin  \\\n",
       "0           Harvest     False  Valid Loads      Harvest  MFE Bag 01   \n",
       "1           Harvest     False  Valid Loads      Harvest  MFE Bag 01   \n",
       "2           Harvest     False  Valid Loads      Harvest      Bin 12   \n",
       "3           Harvest     False  Valid Loads      Harvest  MFE Bag 01   \n",
       "4           Harvest     False  Valid Loads      Harvest      Bin 13   \n",
       "...             ...       ...          ...          ...         ...   \n",
       "19962   BinTransfer     False  Valid Loads  BinTransfer   FC Bag 40   \n",
       "19963   BinTransfer     False  Valid Loads  BinTransfer   FC Bag 14   \n",
       "19964   BinTransfer     False  Valid Loads  BinTransfer   FC Bag 39   \n",
       "19965   BinTransfer     False  Valid Loads  BinTransfer   FC Bag 38   \n",
       "19966   BinTransfer     False  Valid Loads  BinTransfer   FC Bag 32   \n",
       "\n",
       "            AmountCategory  \n",
       "0      Unusual-Small:10-36  \n",
       "1        Invalid-Small:<10  \n",
       "2      Unusual-Small:10-36  \n",
       "3      Unusual-Small:10-36  \n",
       "4      Unusual-Small:10-36  \n",
       "...                    ...  \n",
       "19962    Invalid-Large:>95  \n",
       "19963    Invalid-Large:>95  \n",
       "19964    Invalid-Large:>95  \n",
       "19965    Invalid-Large:>95  \n",
       "19966  Unusual-Small:10-36  \n",
       "\n",
       "[19967 rows x 41 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(gold_HP / \"hp.csv\", dtype={\"Bin\":str})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036e2304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductRaw</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>AmountRaw</th>\n",
       "      <th>AcceptedAmountRaw</th>\n",
       "      <th>AmountRawUnit</th>\n",
       "      <th>details</th>\n",
       "      <th>harvest_profit_id</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>truck</th>\n",
       "      <th>...</th>\n",
       "      <th>FromField</th>\n",
       "      <th>ToField</th>\n",
       "      <th>FromBins</th>\n",
       "      <th>ToBins</th>\n",
       "      <th>TransferMode</th>\n",
       "      <th>IsDirect</th>\n",
       "      <th>Flag</th>\n",
       "      <th>LoadType</th>\n",
       "      <th>Bin</th>\n",
       "      <th>AmountCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19057</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>41645.00</td>\n",
       "      <td>41103.62</td>\n",
       "      <td>Kg</td>\n",
       "      <td>Ticket 1195846</td>\n",
       "      <td>1163963</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19095</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1679499</td>\n",
       "      <td>1210573</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19096</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>44.01</td>\n",
       "      <td>44.01</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1679500</td>\n",
       "      <td>1210574</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19097</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>44.12</td>\n",
       "      <td>44.12</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1679505</td>\n",
       "      <td>1210575</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19099</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>42.58</td>\n",
       "      <td>42.58</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1679510</td>\n",
       "      <td>1210577</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19102</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>39.24</td>\n",
       "      <td>39.24</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1679516</td>\n",
       "      <td>1210580</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19103</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>42.62</td>\n",
       "      <td>42.62</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1679516</td>\n",
       "      <td>1210581</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19104</th>\n",
       "      <td>Canola</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cargill Crush Clavet</td>\n",
       "      <td>41.86</td>\n",
       "      <td>41.86</td>\n",
       "      <td>Mt</td>\n",
       "      <td>Ticket 1200108</td>\n",
       "      <td>1210584</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Load</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ProductRaw from                    to  AmountRaw  AcceptedAmountRaw  \\\n",
       "19057     Canola   C1  Cargill Crush Clavet   41645.00           41103.62   \n",
       "19095     Canola   C1  Cargill Crush Clavet      41.00              41.00   \n",
       "19096     Canola   C1  Cargill Crush Clavet      44.01              44.01   \n",
       "19097     Canola   C1  Cargill Crush Clavet      44.12              44.12   \n",
       "19099     Canola   C1  Cargill Crush Clavet      42.58              42.58   \n",
       "19102     Canola   C1  Cargill Crush Clavet      39.24              39.24   \n",
       "19103     Canola   C1  Cargill Crush Clavet      42.62              42.62   \n",
       "19104     Canola   C1  Cargill Crush Clavet      41.86              41.86   \n",
       "\n",
       "      AmountRawUnit         details  harvest_profit_id TransactionDate truck  \\\n",
       "19057            Kg  Ticket 1195846            1163963      2025-10-06   NaN   \n",
       "19095            Mt  Ticket 1679499            1210573      2025-10-29   NaN   \n",
       "19096            Mt  Ticket 1679500            1210574      2025-10-29   NaN   \n",
       "19097            Mt  Ticket 1679505            1210575      2025-10-29   NaN   \n",
       "19099            Mt  Ticket 1679510            1210577      2025-10-29   NaN   \n",
       "19102            Mt  Ticket 1679516            1210580      2025-10-29   NaN   \n",
       "19103            Mt  Ticket 1679516            1210581      2025-10-29   NaN   \n",
       "19104            Mt  Ticket 1200108            1210584      2025-10-29   NaN   \n",
       "\n",
       "       ... FromField  ToField  FromBins  ToBins  TransferMode  IsDirect  Flag  \\\n",
       "19057  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19095  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19096  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19097  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19099  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19102  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19103  ...     False    False     False   False      Not Load     False   NaN   \n",
       "19104  ...     False    False     False   False      Not Load     False   NaN   \n",
       "\n",
       "       LoadType  Bin  AmountCategory  \n",
       "19057  Not Load  NaN     Valid:36-48  \n",
       "19095  Not Load  NaN     Valid:36-48  \n",
       "19096  Not Load  NaN     Valid:36-48  \n",
       "19097  Not Load  NaN     Valid:36-48  \n",
       "19099  Not Load  NaN     Valid:36-48  \n",
       "19102  Not Load  NaN     Valid:36-48  \n",
       "19103  Not Load  NaN     Valid:36-48  \n",
       "19104  Not Load  NaN     Valid:36-48  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[((df[\"Flag\"].isna()))]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84c9cf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location\n",
       "Hafford    8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.Location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37031d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductRaw</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>AmountRaw</th>\n",
       "      <th>AcceptedAmountRaw</th>\n",
       "      <th>AmountRawUnit</th>\n",
       "      <th>details</th>\n",
       "      <th>harvest_profit_id</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>truck</th>\n",
       "      <th>...</th>\n",
       "      <th>FromField</th>\n",
       "      <th>ToField</th>\n",
       "      <th>FromBins</th>\n",
       "      <th>ToBins</th>\n",
       "      <th>TransferMode</th>\n",
       "      <th>IsDirect</th>\n",
       "      <th>Flag</th>\n",
       "      <th>LoadType</th>\n",
       "      <th>Bin</th>\n",
       "      <th>AmountCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Small Red Lentils</td>\n",
       "      <td>Lienan</td>\n",
       "      <td>Wiens W8</td>\n",
       "      <td>29345.53</td>\n",
       "      <td>29345.53</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991791</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>Truck C 009</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unusual-Small:10-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Small Red Lentils</td>\n",
       "      <td>Lienan</td>\n",
       "      <td>Wiens W8</td>\n",
       "      <td>92550.70</td>\n",
       "      <td>92550.70</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991792</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>Truck C 007</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Small Red Lentils</td>\n",
       "      <td>Lienan</td>\n",
       "      <td>Wiens W8</td>\n",
       "      <td>94059.38</td>\n",
       "      <td>94059.38</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991793</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>Truck C 032</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Small Red Lentils</td>\n",
       "      <td>Lienan</td>\n",
       "      <td>Wiens W8</td>\n",
       "      <td>92632.76</td>\n",
       "      <td>92632.76</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991794</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>Truck C 078</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>Small Red Lentils</td>\n",
       "      <td>Lienan</td>\n",
       "      <td>Wiens W8</td>\n",
       "      <td>91934.33</td>\n",
       "      <td>91934.33</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991795</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>Truck C 083</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Small Red Lentils</td>\n",
       "      <td>Lienan</td>\n",
       "      <td>Wiens W8</td>\n",
       "      <td>94421.00</td>\n",
       "      <td>94421.00</td>\n",
       "      <td>Lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>991817</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>Truck C 013</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valid:36-48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ProductRaw    from        to  AmountRaw  AcceptedAmountRaw  \\\n",
       "555  Small Red Lentils  Lienan  Wiens W8   29345.53           29345.53   \n",
       "556  Small Red Lentils  Lienan  Wiens W8   92550.70           92550.70   \n",
       "557  Small Red Lentils  Lienan  Wiens W8   94059.38           94059.38   \n",
       "558  Small Red Lentils  Lienan  Wiens W8   92632.76           92632.76   \n",
       "559  Small Red Lentils  Lienan  Wiens W8   91934.33           91934.33   \n",
       "560  Small Red Lentils  Lienan  Wiens W8   94421.00           94421.00   \n",
       "\n",
       "    AmountRawUnit details  harvest_profit_id TransactionDate        truck  \\\n",
       "555           Lbs     NaN             991791      2025-08-16  Truck C 009   \n",
       "556           Lbs     NaN             991792      2025-08-16  Truck C 007   \n",
       "557           Lbs     NaN             991793      2025-08-16  Truck C 032   \n",
       "558           Lbs     NaN             991794      2025-08-16  Truck C 078   \n",
       "559           Lbs     NaN             991795      2025-08-16  Truck C 083   \n",
       "560           Lbs     NaN             991817      2025-08-16  Truck C 013   \n",
       "\n",
       "     ... FromField  ToField  FromBins  ToBins  TransferMode  IsDirect  Flag  \\\n",
       "555  ...      True    False     False   False       Invalid     False   NaN   \n",
       "556  ...      True    False     False   False       Invalid     False   NaN   \n",
       "557  ...      True    False     False   False       Invalid     False   NaN   \n",
       "558  ...      True    False     False   False       Invalid     False   NaN   \n",
       "559  ...      True    False     False   False       Invalid     False   NaN   \n",
       "560  ...      True    False     False   False       Invalid     False   NaN   \n",
       "\n",
       "     LoadType  Bin       AmountCategory  \n",
       "555   Harvest  NaN  Unusual-Small:10-36  \n",
       "556   Harvest  NaN          Valid:36-48  \n",
       "557   Harvest  NaN          Valid:36-48  \n",
       "558   Harvest  NaN          Valid:36-48  \n",
       "559   Harvest  NaN          Valid:36-48  \n",
       "560   Harvest  NaN          Valid:36-48  \n",
       "\n",
       "[6 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[temp[\"Location\"]==\"SwiftCurrent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42785734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cdcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c4534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d63c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76d503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b88c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c7bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e75ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053bd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af1a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37ed12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e10ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4be6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83de2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac2e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ac8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014b99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93207a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ccd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea055e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de776738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b3849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60db634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45572513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _report_transform(self, report_type:str = \"PL\", light_load:bool = True) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "        transform PL or GL from json nested format to flat pandas dataframe and save as csv\n",
    "    \"\"\"\n",
    "    assert report_type in [\"PL\", \"GL\"], \"please pass report_type as one of (PL, GL)\"\n",
    "    mode = \"Light Load\" if light_load else \"Full Load\"\n",
    "    # prepare for extraction \n",
    "    records_all = []\n",
    "    print(f\"\\nBegin {report_type} Transformation - {mode}...\")\n",
    "    self.log.write(f\"\\n\\nBegin {report_type} Transformation - {mode}\\n\\n\")\n",
    "    # extraction\n",
    "    for name in self.company_names:\n",
    "        print(f\"Processing {name}...\")\n",
    "        self.log.write(f\"\\nProcessing {name}\\n\")\n",
    "        # determine the columns in raw report for the current company - PL only ******* also APAR\n",
    "        if report_type == \"PL\":\n",
    "            if name in [\"MSUSA\", \"MSL\"]: # these companies don't have Class column\n",
    "                report_columns = [x for x in self.PL_raw_cols if x != \"Location\"]        \n",
    "            elif name in [\"NexGen\"]:\n",
    "                report_columns = [x for x in self.PL_raw_cols if ((x != \"Location\") & (x != \"Class\"))]  # this company doesn't have Location and Class column\n",
    "            else:\n",
    "                report_columns = self.PL_raw_cols\n",
    "        else:\n",
    "            report_columns = self.GL_raw_cols\n",
    "        # iterate through all json files (saved by year_month) for the current company\n",
    "        path = self.raw_path[\"QBO\"][report_type] / name\n",
    "        files = os.listdir(path)\n",
    "        # if light_load, only load and transform data in this FY\n",
    "        if light_load:\n",
    "            files = [file1 for file1 in files if str(self.today.year) in file1 or file1 == f\"{self.today.year-1}_10.json\"]\n",
    "        for file_name in files:\n",
    "            with open(path/file_name, \"r\") as f:\n",
    "                df = json.load(f)\n",
    "            records = []    # records for the company\n",
    "            # not separating this function because access to records list\n",
    "            def _extract(current_level: dict[str,any], parent1_id: str=\"\", parent2_id: str=\"\", parent2_name: str=\"\") -> None:\n",
    "                \"\"\" \n",
    "                    recursive function that focus on current_leve, if reached the leaf node, append, if not, call itself with next_level, while recording current id as parent_id\n",
    "                \"\"\"\n",
    "                if current_level[\"type\"] == \"Data\":  # reached a leaf node, append and stop\n",
    "                    sub_record = {}\n",
    "                    for i in range(len(report_columns)): # iterate through each column + value inside the json format\n",
    "                        if (i == 2) and (current_level[\"ColData\"][i][\"value\"]): \n",
    "                            # for DocNumber: use format: MFl-XXXXX, don't need to extract id\n",
    "                            sub_record[\"DocNumber\"] = name + \"-\" + current_level[\"ColData\"][i][\"value\"]\n",
    "                            continue \n",
    "                        sub_record[report_columns[i]] = current_level[\"ColData\"][i][\"value\"]\n",
    "                        # add id and append company in front of the id for SplitAccID, TransactionID, NameID (vendorID,...)\n",
    "                        if (report_columns[i] == \"SplitAcc\") and (current_level[\"ColData\"][i].get(\"id\", -1) != -1):\n",
    "                            sub_record[\"SplitAccID\"] = name + current_level[\"ColData\"][i][\"id\"]\n",
    "                        elif (report_columns[i] == \"TransactionType\") and (current_level[\"ColData\"][i].get(\"id\", -1) != -1):\n",
    "                            sub_record[\"TransactionID_partial\"] = name + current_level[\"ColData\"][i][\"id\"]\n",
    "                        elif (report_columns[i] == \"Name\") and (current_level[\"ColData\"][i].get(\"id\", -1) != -1):\n",
    "                            sub_record[\"NameID\"] = name + current_level[\"ColData\"][i][\"id\"]\n",
    "                        elif (report_columns[i] == \"Class\") and (current_level[\"ColData\"][i].get(\"id\", \"\") != \"\"):\n",
    "                            sub_record[\"ClassID\"] = name + current_level[\"ColData\"][i][\"id\"]\n",
    "                        elif (report_columns[i] == \"Location\") and (current_level[\"ColData\"][i].get(\"id\", \"\") != \"\"):\n",
    "                            sub_record[\"FarmID\"] = name + current_level[\"ColData\"][i][\"id\"]\n",
    "                    # add account information, which is at the parent level of the records\n",
    "                    sub_record[\"AccName\"] = parent2_name[6:] \n",
    "                    sub_record[\"AccID\"] = name + parent2_id if len(parent2_id)>=1 else name + parent1_id \n",
    "                    sub_record[\"Corp\"] = name\n",
    "                    sub_record[\"AccNum\"] = name + parent2_name[:6]\n",
    "                    # append to records for this company\n",
    "                    records.append(sub_record)\n",
    "                else:       # if not leaf node\n",
    "                    if current_level.get(\"Header\", -1) != -1:   # reached a node that contains a legit path, record account information \n",
    "                        idx = current_level[\"Header\"][\"ColData\"][0].get(\"id\", -1)\n",
    "                        parent2_name = current_level[\"Header\"][\"ColData\"][0][\"value\"]\n",
    "                        if idx != -1:   # if we are looking at an account\n",
    "                            parent1_id = parent2_id     # account id 2 levels up to any transactions \n",
    "                            parent2_id = idx    # account id that is the parent account of transactions \n",
    "                    # recursively call _extract on every sub-records\n",
    "                    if current_level.get(\"Rows\", -1) != -1:\n",
    "                        if current_level[\"Rows\"].get(\"Row\",-1) != -1:\n",
    "                            for j in range(len(current_level[\"Rows\"][\"Row\"])):\n",
    "                                _extract(current_level[\"Rows\"][\"Row\"][j], parent1_id = parent1_id, parent2_id = parent2_id, parent2_name = parent2_name)\n",
    "            # start the recursive process\n",
    "            for k in range(len(df[\"Rows\"][\"Row\"])):\n",
    "                _extract(df[\"Rows\"][\"Row\"][k])\n",
    "            self.log.write(f\"\\nFinished Processing {file_name}, total records added - {len(records)}\\n\")\n",
    "            records_all.extend(records)\n",
    "    self.log.write(f\"\\n\\nFinished Processing All Files, Total Records {len(records_all)}\\n\\n\")\n",
    "    # processing all records\n",
    "    records_all = pd.DataFrame(records_all)\n",
    "    records_all = records_all[records_all[\"TransactionDate\"]!=\"Beginning Balance\"] # ignore beginning blanace summary records, because its format is different than others\n",
    "    if report_type == \"GL\":\n",
    "        records_all[\"TransactionType\"] = records_all[\"TransactionType\"].str.replace(\"Bill Payment (Cheque)\", \"BillPaymentCheck\")\n",
    "        records_all[\"TransactionType\"] = records_all[\"TransactionType\"].str.replace(\"Bill Payment (Check)\", \"BillPaymentCheck\")\n",
    "    # self.records_dev = records_all\n",
    "    records_all[\"TransactionDate\"] = pd.to_datetime(records_all[\"TransactionDate\"])\n",
    "    records_all[\"AmountAdj\"] = records_all.apply(lambda x: self._report_adjust_sign(x, target_col=\"AmountAdj\"), axis=1) # can be improved, see _report_APAR_transform()\n",
    "    records_all[\"AmountCAD\"] = records_all.apply(lambda x: self._report_adjust_sign(x, target_col=\"AmountCAD\"), axis=1) # can be improved, see _report_APAR_transform()\n",
    "    records_all[\"FXRate\"] = self.fx\n",
    "    records_all[\"Country\"] = records_all[\"Corp\"].apply(lambda x: \"USA\" if x in self.us_companies else \"Canada\")  # can be improved, see _report_APAR_transform()\n",
    "    self.log.write(f\"\\nAfter omitting Beginning Balance entries, Total length is {len(records_all)}\\n\")\n",
    "    self.log.write(f\"\\nSpot USD/CAD ={self.fx}\\n\")\n",
    "    self.log.write(f\"\\nFinished Processing all Files for {report_type}, Total Records {len(records_all)}\\n\\n\")\n",
    "    print(f\"\\nFinished Processing all Files for {report_type}, Total Records {len(records_all)}\\n\")\n",
    "    return records_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
