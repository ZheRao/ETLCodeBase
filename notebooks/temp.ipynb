{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projects(Job):\n",
    "    \"\"\" \n",
    "        for project specific data transformations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, focus_last_FY:bool = False, is_dev:bool=False):\n",
    "        super().__init__()\n",
    "        self.gold_path = {\n",
    "            \"weekly_banking\": self.base_dir / \"Gold\" / \"FinanceProject\" / \"WeeklyBanking\",\n",
    "            \"inventory\": self.base_dir / \"Gold\" / \"InventoryProject\",\n",
    "            \"payroll\": self.base_dir / \"Gold\" / \"HRProject\" /\"PayrollProject\",\n",
    "            \"finance_operational\": self.base_dir / \"Gold\" / \"FinanceOperationalProject\",\n",
    "            \"budget\": self.base_dir / \"Gold\" / \"BudgetProject\",\n",
    "            \"QBOTime\": self.base_dir / \"Gold\" / \"HRProject\" / \"QBOTimeProject\",\n",
    "            \"hr_combined\": self.base_dir / \"Gold\" / \"HRProject\" / \"CombinedSummary\",\n",
    "            \"pillar_dashboard\": self.base_dir / \"Gold\" / \"DirectorDashboards\",\n",
    "            \"APReporting\": self.base_dir / \"Gold\" / \"FinanceProject\" / \"APReporting\"\n",
    "        }\n",
    "        self.silver_acc = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension_time\"]/\"Account.csv\")\n",
    "        self.commodities = {\n",
    "            \"Produce\": [\"Strawberry\", \"Watermelon\", \"Cantaloupe\", \"Market Garden\", \"Broccoli\", \"Pumpkin\", \"Sweet Corn\", \"Cauliflower\", \"Squash\", \"Honeydew Melon\", \"Potato\", \"Carrot\", \"Cabbage\",\n",
    "                        \"Lettuce\", \"Brussel Sprouts\", \"Prairie Pathways\", \"Beet\", \"Corn Maze\", \"CSA\"],\n",
    "            \"Grain\": [\"Blackeye Pea\", \"Winter Wheat\", \"Durum\", \"Cotton\", \"Chickpea\", \"Barley\", \"Green Lentil\", \"Red Lentil\", \"Canola\", \n",
    "                        \"Wheat\",\"Field Pea\", \"Corn\", \"Oat\", \"Soybean\", \"Bean\"],\n",
    "            \"Cattle\": [\"Weaned Calves\", \"Cull Bull\", \"Cull Cow\", \"Bred Heifer\", \"Purebred Yealing Bull\", \"Purebred Heifer\", \n",
    "                        \"Purebred Cow\", \"Purebred Bull\", \"Cow\", \"Bull\", \"Steer\", \"Heifer\", \"Yearling\", \"Calf\"]\n",
    "        }\n",
    "        self.locations = {\n",
    "            \"Produce\": [\"BritishColumbia (produce)\", \"Outlook\", \"Arizona (produce)\", \"Montana (produce)\", \"Seeds USA\"],\n",
    "            \"Cattle\": [\"Airdrie\", \"Eddystone (cattle)\", \"Ashcroft\", \"Home Ranch\", \"Diamond S\", \"Wolf Ranch\", \"Fraser River Ranch\", \"Moon Ranch\", \"Waldeck\", \"Calderbank\",\"BC Cattle MFL\"],\n",
    "            \"Grain\": [\"Eddystone (grain)\", \"Arizona (grain)\", \"Colorado\", \"Swift Current\", \"Regina\", \"Raymore\", \"Prince Albert\", \"The Pas\",\n",
    "                      \"Kamsack\", \"Hafford\", \"Yorkton\", \"Fly Creek\", \"Camp 4\", \"Havre\", \"Billings\"],\n",
    "            \"Seed\": [\"NexGen\", \"Seeds\"],\n",
    "            \"Others\": [\"Eddystone (corporate)\", \"Arizona (corporate)\", \"Legacy\", \"BritishColumbia (corporate)\", \"Corporate\"]\n",
    "        }\n",
    "        self.bc_ranches = [\"Ashcroft\", \"Fraser River Ranch\", \"Moon Ranch\", \"Wolf Ranch\", \"Diamond S\",\"Home Ranch\"]\n",
    "        self.pl_exist = False # determines whether _financial_operational has run and gold_pl is stored in self, if not, any subsequent downstream projects will run _financial_operational first\n",
    "        self.currentFY = self.today.year if self.today.month<=10 else self.today.year + 1\n",
    "        if focus_last_FY: self.currentFY -= 1\n",
    "        self.is_dev = is_dev\n",
    "        self.accnum_reroute = {\"MFL405101\": \"MFL405110\", \"MFL405102\":\"MFL405120\", \"MFL405103\":\"MFL405130\", \"MSL585000\":\"MSL562505\", \"MSL402110\": \"MSL402112\", \"MFBC575000\": \"MFBC575020\",\n",
    "                               \"MFBC629000\": \"MFBC629010\", \"MFBC531010\": \"MFBC531050\"}\n",
    "        self.accid_reroute = {\"MFBC250\": \"MFBC210\", \"MFBC216\":\"MFBC210\", \"MFBC358\":\"MFBC210\", \"MFBC255\":\"MFBC210\", \"MFBC314\":\"MFBC272\", \"MFBC268\":\"MFBC584\", \"MFBC188\":\"MFBC192\", \"MFBC374\":\"MFBC190\",\n",
    "                              \"MFBC61\":\"MFBC190\", \"MFBC26\":\"MFBC585\", \"MFBC412\":\"MFBC242\", \"MFBC66\": \"MFBC592\", \"MFBC65\":\"MFBC220\", \"MFBC19\":\"MFBC210\", \"MFBC60\":\"MFBC187\",\n",
    "                              \"MPUSA1150040007\":\"MPUSA79\"}\n",
    "        self.conversion_mt_to_lb = 2204.62262185\n",
    "        self.conversion_bu_to_lb_canola = 55\n",
    "        self.conversion_bu_to_lb_others = 60\n",
    "        self.customers_dict = {\"Billings\": [\"Viterra Huntley\"], \n",
    "                               \"Eddystone\": [\"Cargill Crush Clavet\"],\n",
    "                               \"Hafford\": [\"P&H North Battleford\",\"Bunge North Battleford\",\"Grains Connect Hafford\", \"Cargill Crush Clavet\", \"Bunge Saskatoon\"],\n",
    "                               \"Kamsack\": [\"Bunge Canora\", \"Bunge Kamsack\", \"LDC Crush\"], # -> viterra kamsack to bunge kamsack\n",
    "                               \"Outlook\": [\"Monette produce Carrots\",\"Viterra Saskatoon\"],\n",
    "                               \"PA\": [\"Cargill Clavet Crush\",\"Viterra White Star\"],\n",
    "                               \"Raymore\": [\"Viterra Raymore\", \"Bunge Raymore\"],\n",
    "                               \"Regina\": [\"Viterra Moose Jaw\"],\n",
    "                               \"SwiftCurrent\": [\"Paterson Grain Swift Current\", \"Viterra Swift Current\"]}\n",
    "        self.customers_list = []\n",
    "        for v in self.customers_dict.values():\n",
    "            self.customers_list.extend(v)\n",
    "    \n",
    "    def _weekly_banking(self) -> None:\n",
    "        \"\"\" \n",
    "            weekly banking project: match latest GL bank transactions with raw activities - extract accounts for those activities\n",
    "                assumptions: a raw entry (e.g., invoice) can have multiple lines - multiple associated accounts, only considering the first one \n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Weekly Banking Project Transformation\\n\")\n",
    "        # determine minal date to keep for GL\n",
    "        if self.today.month > 6:\n",
    "            year = self.today.year \n",
    "            month = self.today.month - 6 \n",
    "        else:\n",
    "            year = self.today.year - 1\n",
    "            month = self.today.month + 12 - 6\n",
    "        # load and prepare data\n",
    "        ## account\n",
    "        account = self.silver_acc.copy(deep=True)\n",
    "        ## change some accounts to Transfer category\n",
    "        acc_list = [\"MFL264\", \"MSL250\"]\n",
    "        account.loc[account[\"AccID\"].isin(acc_list), \"ProfitType\"] = \"Asset\"\n",
    "        account.loc[account[\"AccID\"].isin(acc_list), \"Category\"] = \"Transfer\"\n",
    "        account_bank = account[account[\"AccountType\"]==\"Bank\"]\n",
    "        ## LinkedTxn for invoice and bill\n",
    "        invoice_linked = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"LinkedTxn\"/ \"LinkedTxn_Mapping_Invoice.csv\")\n",
    "        bill_linked = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"LinkedTxn\"/ \"LinkedTxn_Mapping_Bill.csv\")\n",
    "        mapping = pd.concat([invoice_linked, bill_linked])\n",
    "        mapping = mapping.drop(columns=[\"Corp\"])\n",
    "        # define customized function for processing other raw table\n",
    "        def _process_facts(df_type:str) -> pd.DataFrame:\n",
    "            \"\"\" \n",
    "                function for processing raw tables for mapping table - TransactionID_partial to AccID\n",
    "            \"\"\"\n",
    "            df = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/(df_type+\".csv\"), usecols = [\"TransactionID\", \"AccID\"])\n",
    "            df[\"TransactionID\"] = df[\"TransactionID\"].apply(lambda x: x.split(\"-\")[1])\n",
    "            df = df.drop_duplicates()\n",
    "            df = df.rename(columns={\"TransactionID\":\"TxnId\"})\n",
    "            return df\n",
    "        ## purchase table for expense transactions\n",
    "        purchase = _process_facts(\"Purchase\")\n",
    "        purchase[\"TxnType\"] = \"Expense\"\n",
    "        mapping = pd.concat([mapping,purchase])\n",
    "        ## journal entries - exclude most entries related to bank\n",
    "        journal = _process_facts(\"JournalEntry\")\n",
    "        journal[\"TxnType\"] = \"Journal Entry\"\n",
    "        # for journal entries, exclude most of entires where the activity account ID is a bank ID\n",
    "        exclude_list = list(account_bank.AccID.unique())\n",
    "        # mylist = [\"MFL51\", \"MFBC470\", \"MFBC471\", \"MFL28\", \"MFL27\", \"MFL1150040024\"]\n",
    "        mylist = [\"MFBC470\", \"MFBC471\"] # should include these accounts\n",
    "        for acc in mylist:\n",
    "            exclude_list.remove(acc)\n",
    "        journal = journal[~journal[\"AccID\"].isin(exclude_list)]\n",
    "        mapping = pd.concat([mapping,journal])\n",
    "        ## deposit\n",
    "        deposit = _process_facts(\"Deposit\")\n",
    "        deposit[\"TxnType\"] = \"Deposit\"\n",
    "        mapping = pd.concat([mapping,deposit])\n",
    "        ## salesreceipts\n",
    "        sales = _process_facts(\"SalesReceipt\")\n",
    "        sales[\"TxnType\"] = \"Sales Receipt\"\n",
    "        mapping = pd.concat([mapping,sales])\n",
    "        # process mapping table - dedup\n",
    "        mapping = mapping.drop_duplicates(subset=[\"TxnId\"],keep=\"first\")\n",
    "        ## load GL transacitons\n",
    "        cols = [\"TransactionType\",\"TransactionID_partial\",\"AccID\",\"AccNum\",\"AccName\", \"TransactionDate\", \"Amount\", \"SplitAcc\", \"SplitAccID\", \"Memo\", \"Corp\", \"Balance\"]\n",
    "        transactions = pd.read_csv(self.silver_path[\"QBO\"][\"GL\"]/\"GeneralLedger.csv\",dtype={\"TransactionID_partial\":str}, usecols=cols)\n",
    "        transactions = transactions[transactions[\"AccID\"].isin(account_bank.AccID.unique())]\n",
    "        transactions[\"TransactionDate\"] = pd.to_datetime(transactions[\"TransactionDate\"])\n",
    "        transactions = transactions[transactions[\"TransactionDate\"]>=dt.datetime(year, month, 1)]\n",
    "        transactions = transactions.rename(columns={\"TransactionType\":\"TxnType\",\"TransactionID_partial\":\"TxnId\",\n",
    "                                                    \"AccID\":\"BankAccID\",\"AccNum\":\"BankAccNum\",\"AccName\":\"BankAccName\",\n",
    "                                                    \"TransactionDate\":\"BankActivityDate\",\"Amount\":\"BankAmount\"})\n",
    "        transactions[\"Sign\"] = transactions[\"BankAmount\"].apply(lambda x: \"Positive\" if x>=0 else \"Negative\")\n",
    "        # merge to get CurrencyID for bank_acc\n",
    "        transactions = pd.merge(transactions, account_bank.loc[:,[\"AccID\",\"CurrencyID\"]], left_on=[\"BankAccID\"], right_on=[\"AccID\"], how=\"left\")\n",
    "        transactions = transactions.drop(columns=[\"AccID\"])\n",
    "        # separating transfers - don't merge with mapping table\n",
    "        transfers = transactions[transactions[\"TxnType\"] == \"Transfer\"].copy(deep=True)\n",
    "        transactions = transactions[transactions[\"TxnType\"]!=\"Transfer\"]\n",
    "        transactions = transactions.drop(columns=[\"SplitAcc\", \"SplitAccID\"])\n",
    "        transactions[\"BankActivityDate\"] = pd.to_datetime(transactions[\"BankActivityDate\"])\n",
    "        transactions[\"TxnType\"] = transactions[\"TxnType\"].replace({\"Cheque Expense\":\"Expense\", \"Check\": \"Expense\"})\n",
    "        # merge with mapping table\n",
    "        transactions_mapped = pd.merge(transactions,mapping,on=[\"TxnId\",\"TxnType\"],how=\"left\")\n",
    "        non_match = transactions_mapped[transactions_mapped[\"AccID\"].isna()]\n",
    "        print(\"None Match Transaction Types\")\n",
    "        print(non_match.TxnType.value_counts())\n",
    "        print(f\"Non matches - {len(non_match)}\")\n",
    "        # function to determine transfer type\n",
    "        def _determine_transfer_type(entry:str) -> str:\n",
    "            \"\"\" \n",
    "                determine whether the transfer is for visa, bank, or other transfer\n",
    "            \"\"\"\n",
    "            if \"visa\" in entry.lower():\n",
    "                return \"Visa Payment\"\n",
    "            elif \"due\" in entry.lower():\n",
    "                return \"Bank Transfer\"\n",
    "            else:\n",
    "                return \"Other Transfer\"\n",
    "        # allocate transfer type \n",
    "        transfers[\"TransferType\"] = transfers[\"SplitAcc\"].apply(lambda x: _determine_transfer_type(x))\n",
    "        transfers = transfers.rename(columns={\"SplitAccID\":\"AccID\"})\n",
    "        transfers = transfers.drop(columns=[\"SplitAcc\"])\n",
    "        transactions_mapped = pd.concat([transactions_mapped,transfers], ignore_index=True)\n",
    "        # clean up the dataframe\n",
    "        transactions_mapped = transactions_mapped.rename(columns={\"CurrencyID\":\"BankCurrencyID\"})\n",
    "        transactions_mapped = pd.merge(transactions_mapped, account.loc[:,[\"AccID\",\"AccName\",\"AccNum\",\"Category\",\"ProfitType\",\"CurrencyID\"]], on=\"AccID\", how=\"left\")\n",
    "        transactions_mapped.loc[transactions_mapped[\"TransferType\"]==\"Bank Transfer\",\"Category\"] = \"Bank Transfer\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"BankAccNum\"].str.startswith(\"MSL\"))&(transactions_mapped[\"AccNum\"]==\"MSL120001\")), \"Category\"] = \"Seed Processing Revenue\"\n",
    "        transactions_mapped = transactions_mapped.rename(columns={\"AccNum\":\"ActivityAccNum\", \"AccName\":\"ActivityAccName\"})\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Positive\")), \"ProfitType\"] = \"Other Operating Revenue\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Positive\")), \"Category\"] = \"Miscellaneous income\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Negative\")), \"ProfitType\"] = \"Operating Overheads\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")&(transactions_mapped[\"Sign\"]==\"Negative\")), \"Category\"] = \"Office and miscellaneous\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")), \"ActivityAccNum\"] = \"Manual Adjustment\"\n",
    "        transactions_mapped.loc[((transactions_mapped[\"TxnType\"]==\"Sales Tax Payment\")), \"ActivityAccName\"] = \"Manual Adjustment\"\n",
    "        # csv from sharepoint is unstable, and produced unpredictable readings from Power BI\n",
    "        self.check_file(self.gold_path[\"weekly_banking\"])\n",
    "        transactions_mapped.to_excel(self.gold_path[\"weekly_banking\"]/\"BankingActivity.xlsx\", sheet_name=\"transactions\", index=False)\n",
    "\n",
    "    def _extract_accnum_accid(self) -> None:\n",
    "        \"\"\" \n",
    "            this function creates a accnum to accID mapping table to avoid repeated merges\n",
    "        \"\"\"\n",
    "        self.acc_map = self.operation_acc.set_index([\"AccNum\"])[\"AccID\"]\n",
    "\n",
    "    def _perform_manual_adjust_GL_inventory(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            This function reads inventory account balances at the beginning of this fiscal year, and apply the amount to PL accounts for Fert/Chem/Seed\n",
    "        \"\"\"\n",
    "        # fixed column values \n",
    "        date = '2024-11-01'\n",
    "        transaction_type = 'Manual Adjustments'\n",
    "        memo = 'Adjustments from Trial Balance from beginning of this fiscal year'\n",
    "        FY = 2025\n",
    "        month = 'November'\n",
    "        # read adjustments csv file\n",
    "        adjustments = pd.read_csv(self.gold_path[\"finance_operational\"]/\"ManualAdjustments\"/\"2025.csv\",dtype={\"Amount\":float})\n",
    "        location_adj = {\"Camp 4\": \"Billings\", \"Fly Creek\":\"Billings\"}\n",
    "        adjustments[\"Location\"] = adjustments[\"Location\"].replace(location_adj)\n",
    "        # compute various Amount columns to match the other PL entries\n",
    "        adjustments[\"AmountAdj\"] = -adjustments[\"Amount\"]\n",
    "        adjustments[\"AmountCAD\"] = adjustments.apply(lambda x: x[\"AmountAdj\"] * self.fx if x[\"Currency\"]==\"USD\" else x[\"AmountAdj\"],axis=1)\n",
    "        adjustments[\"AmountDisplay\"] = -adjustments[\"AmountCAD\"]\n",
    "        adjustments[\"AccNum\"] = adjustments.apply(lambda x: \"\".join(x[\"DisplayName\"].split(\" \")[:2]),axis=1)\n",
    "        # create additional entries\n",
    "        addition_df = df.head(0).copy(deep=True)\n",
    "        row = {\"TransactionDate\":date, \"TransactionType\":transaction_type, \"Memo\":memo, \"FiscalYear\":FY, \"Month\":month, \"FXRate\": self.fx}\n",
    "        for i in range(len(adjustments)):\n",
    "            entry = row | {\"Amount\": adjustments.loc[i,\"Amount\"], \"AccID\": adjustments.loc[i,\"AccID\"], \"AmountAdj\": adjustments.loc[i,\"AmountAdj\"], \n",
    "                        \"AmountCAD\": adjustments.loc[i,\"AmountCAD\"], \"AmountDisplay\":adjustments.loc[i,\"AmountDisplay\"],\n",
    "                        \"Location\":adjustments.loc[i,\"Location\"], \"Pillar\": adjustments.loc[i,\"Pillar\"],\n",
    "                        \"AccNum\":adjustments.loc[i,\"AccNum\"] }\n",
    "            addition_df.loc[len(addition_df)] = entry\n",
    "        print(f\"\\nManual GL Inventory Accounts Adjustments created {len(addition_df)} entries\\n\")\n",
    "        df = pd.concat([df,addition_df],ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def _finance_operational(self) -> None:\n",
    "        \"\"\" \n",
    "            transform PL data into operational-ready\n",
    "                1. reclassify accounts\n",
    "                2. standardize location, classify pillar\n",
    "                3. revising signs\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Finance Operational Project Transformation\\n\")\n",
    "        # load data from silver space\n",
    "        data = pd.read_csv(self.silver_path[\"QBO\"][\"PL\"]/\"ProfitAndLoss.csv\", dtype={\"Class\":str, \"ClassID\":str})\n",
    "        assert len(data.FXRate.value_counts()) == 1, \"different FXRate detected\"\n",
    "        self.fx = data.loc[0,\"FXRate\"]\n",
    "        data[\"TransactionDate\"] = pd.to_datetime(data[\"TransactionDate\"])\n",
    "        data[\"FiscalYear\"] = data.TransactionDate.apply(lambda x: x.year + 1 if x.month >= 11 else x.year)\n",
    "        # add month to the PL\n",
    "        data[\"Month\"] = data[\"TransactionDate\"].dt.month_name()\n",
    "        ## add location for seed operation\n",
    "        data.loc[data[\"Corp\"]==\"MSL\",\"Location\"] = \"Seeds\"\n",
    "        data.loc[data[\"Corp\"]==\"NexGen\",\"Location\"] = \"NexGen\"\n",
    "        data.loc[data[\"Corp\"]==\"MSUSA\",\"Location\"] = \"Seeds USA\"\n",
    "        # clean location\n",
    "        data = data.rename(columns={\"Location\":\"LocationRaw\"})\n",
    "        data[\"Location\"] = data[\"LocationRaw\"]\n",
    "        data = data.fillna(value={\"Location\":\"Missing\"})\n",
    "        # switch seeds usa to AZ produce\n",
    "        data.loc[data[\"Corp\"]==\"MSUSA\",\"Location\"] = \"Arizona (produce)\"\n",
    "        ## clean location\n",
    "        clean_location = {\"Airdrie - Grain\":\"Airdrie\", \"Airdrie - Cattle\":\"Airdrie\", \"Airdrie - General\":\"Airdrie\", \"Airdrie\":\"Airdrie\", \n",
    "                        \"Eddystone - Grain\": \"Eddystone (grain)\", \"Eddystone - Cattle\": \"Eddystone (cattle)\", \"Eddystone - General\":\"Eddystone (corporate)\",\n",
    "                        \"Outlook (JV)\":\"Outlook\", \"AZ Produce\":\"Arizona (produce)\", \"Corporate\":\"Arizona (corporate)\", \"BC Produce\":\"BritishColumbia (produce)\",\n",
    "                        \"Grain\":\"Arizona (grain)\", \"Ashcroft (CC, Fischer, Loon)\":\"Ashcroft\", \n",
    "                        \"Outlook (Capital)\":\"Outlook\", \"Colorado (MF)\":\"Colorado\", \"Colorado (JV)\":\"Colorado\", \"Cattle - General\":\"BritishColumbia (corporate)\",\n",
    "                        \"Home (70 M, LF/W, 105 M)\":\"Home Ranch\", \"Diamond S (BR)\":\"Diamond S\", \"-Corporate\":\"Corporate\",\n",
    "                        \"MT Produce\": \"Montana (produce)\", \"Fly Creek\": \"Billings\", \"Camp 4\":\"Billings\", \"BC Cattle\":\"BC Cattle MFL\"}\n",
    "        others = {\"North Farm (deleted)\":\"Legacy\", \"Cache/Fischer/Loon - DNU\":\"Legacy\"}\n",
    "        data[\"Location\"] = data[\"Location\"].replace(clean_location)\n",
    "        locations = self.locations[\"Produce\"] + self.locations[\"Grain\"] + self.locations[\"Cattle\"] + self.locations[\"Others\"] + self.locations[\"Seed\"]\n",
    "        unaccounted_location = list(set(data[\"Location\"].unique()) - set(locations))\n",
    "        print(f\"location unaccounted for - {unaccounted_location}\")\n",
    "        # classify pillar\n",
    "        data[\"Pillar\"] = data.apply(lambda x: self._pillar_classification(x),axis=1)\n",
    "        # reorganize corp\n",
    "        ## MPUSA missing location = Arizona (produce)\n",
    "        data.loc[((data[\"Corp\"] == \"MPUSA\")&(data[\"Location\"].isna())), \"Location\"] = \"Arizona (produce)\"\n",
    "        data.loc[((data[\"Corp\"] == \"MPUSA\")&(data[\"Location\"]==\"Missing\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        data.loc[((data[\"Corp\"] == \"MPUSA\")&(data[\"Location\"] == \"Arizona (produce)\")), \"Pillar\"] = \"Produce\"\n",
    "        ## AZ Produce --> MPUSA\n",
    "        data.loc[data[\"Location\"] == \"Arizona (produce)\", \"Corp\"] = \"MPUSA\"\n",
    "        ## move everything for AZ in 2024 to produce\n",
    "        data.loc[((data[\"FiscalYear\"] >= 2024) & (data[\"Location\"].str.contains(\"Arizona\",case=False))),\"Pillar\"] = \"Produce\"\n",
    "        data.loc[((data[\"FiscalYear\"] >= 2024) & (data[\"Location\"].str.contains(\"Arizona\",case=False))),\"Location\"] = \"Arizona (produce)\"\n",
    "        ## BC Produce --> MPL\n",
    "        data.loc[data[\"Location\"] == \"BritishColumbia (produce)\", \"Corp\"] = \"MPL\"\n",
    "        ## Outlook --> MPL\n",
    "        data.loc[data[\"Location\"]==\"Outlook\", \"Corp\"] = \"MPL\"\n",
    "        # reroute accid\n",
    "        data[\"AccID\"] = data[\"AccID\"].replace(self.accid_reroute)\n",
    "        # Reclassify accounts for Operational Purpose\n",
    "        ## read & process operational classification\n",
    "        with open(self.silver_path[\"QBO\"][\"Dimension\"]/\"acc_classification.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_acc = yaml.safe_load(f)\n",
    "        rows = [(l1, l2, l3, v) \n",
    "                for l1, l1_inner in raw_acc.items() \n",
    "                for l2, l2_inner in l1_inner.items() \n",
    "                for l3, l3_inner in l2_inner.items() \n",
    "                for v in l3_inner]\n",
    "        acc_operation = pd.DataFrame(rows, columns=[\"OperationProfType\", \"OperationCategory\", \"OperationSubCategory\", \"AccID\"])\n",
    "        ## read accounts table and apply new classification\n",
    "        accounts = self.silver_acc\n",
    "        accounts = pd.merge(accounts, acc_operation, on = \"AccID\", how = \"left\")\n",
    "        accounts[\"Commodity\"] = accounts.apply(lambda x: self._identify_product(x), axis=1)\n",
    "        commodities = pd.DataFrame(data={\"Commodity\": accounts[\"Commodity\"].unique()})\n",
    "        commodities.to_csv(self.gold_path[\"inventory\"]/\"Tables\"/\"commodities_acc.csv\", index=False)\n",
    "        # prepare account table for mapping AccID from AccNum\n",
    "        self.operation_acc = accounts[accounts[\"AccNum\"].notna()]   # AccNum must be non-missing\n",
    "        self.operation_acc = self.operation_acc[self.operation_acc[\"Active\"]] # avoid non-active accounts what share same AccNum with active accounts\n",
    "        self.operation_acc.to_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\", index=False)\n",
    "        # Revising Signs according to Operational Classification\n",
    "        print(\"Revising Signs ...\")\n",
    "        # expense_accounts = accounts[(accounts[\"OperationCategory\"] == \"Expense\") | (accounts[\"OperationCategory\"] ==\"Inventory Consumption\")] # for my classification\n",
    "        expense_accounts = accounts[accounts[\"ProfitType\"].isin([\"Cost of Goods Sold\", \"Direct Operating Expenses\", \"Operating Overheads\", \"Other Expense\"])]\n",
    "        data[\"AmountDisplay\"] = data.apply(lambda x: -x[\"AmountCAD\"] if x[\"AccID\"] in expense_accounts.AccID.unique() else x[\"AmountCAD\"], axis=1)\n",
    "        # data = self._perform_manual_adjust_GL_inventory(data)\n",
    "        self.gold_pl = data\n",
    "        self.gold_acc = accounts\n",
    "        # save files\n",
    "        print(\"Saving ...\")\n",
    "        self.check_file(self.gold_path[\"finance_operational\"])\n",
    "        data.to_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\", index=False)\n",
    "        accounts.to_csv(self.gold_path[\"finance_operational\"]/\"Account_table.csv\", index=False)\n",
    "        accounts.to_excel(self.gold_path[\"finance_operational\"]/\"Account_table.xlsx\", sheet_name = \"Account\", index=False)\n",
    "        data.to_excel(self.gold_path[\"finance_operational\"]/\"PL.xlsx\", sheet_name=\"Transactions\", index=False)\n",
    "        for pillar in [\"Grain\", \"Cattle\", \"Seed\", \"Produce\"]:\n",
    "            data[data[\"Pillar\"]==pillar].to_excel(self.gold_path[\"pillar_dashboard\"]/pillar/\"PL.xlsx\", sheet_name=\"Transactions\", index=False)\n",
    "        self.pl_exist = True\n",
    "    \n",
    "    def _process_pp(self, data:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            This function takes original dataframe, apply the payperiod number classification based on transactions date, process payperiod columns, and return the new dataframe,\n",
    "                save the pp table for consolidated tables\n",
    "        \"\"\"\n",
    "        date_col = \"TransactionDate\" if \"TransactionDate\" in data.columns else \"date\"\n",
    "        # load payperiods\n",
    "        payperiods = pd.read_csv(self.gold_path[\"payroll\"]/\"Payperiods.csv\")\n",
    "        payperiods[\"START\"] = pd.to_datetime(payperiods[\"START\"])\n",
    "        payperiods[\"END\"] = pd.to_datetime(payperiods[\"END\"])\n",
    "        payperiods = payperiods.loc[:,[\"PP\",\"START\",\"END\",\"Cycle\",\"FiscalYear\"]]\n",
    "        payperiods = payperiods.rename(columns={\"PP\":\"PPNum\"})\n",
    "        payperiods[\"PPName\"] = payperiods[\"Cycle\"].astype(str).str.slice(2) + \"-\" + \"PP\" + payperiods[\"PPNum\"].astype(str).str.zfill(2)\n",
    "        # shift transaction dates - AZ: left 12 days, others: left 5 days\n",
    "        offset_days = {\"Arizona (produce)\": 12, \"Outlook\": 12}\n",
    "        data[\"days_offset\"] = data[\"Location\"].map(offset_days).fillna(5)\n",
    "        data[\"date_shifted\"] = data[date_col] - pd.to_timedelta(data[\"days_offset\"],unit=\"days\")\n",
    "        data = data[data[\"date_shifted\"]>=dt.datetime(2021,12,20)].copy(deep=True)\n",
    "        # construct interval index object for all periods\n",
    "        idx = pd.IntervalIndex.from_arrays(\n",
    "            left = payperiods[\"START\"],\n",
    "            right = payperiods[\"END\"],\n",
    "            closed = \"both\"\n",
    "        )\n",
    "        # determine which payperiod a transaction date belongs to by identifying the positional index inside the interval index object\n",
    "        pos = idx.get_indexer(data[\"date_shifted\"])\n",
    "        # extract payperiod info based on positional indices\n",
    "        ppnum = payperiods[\"PPNum\"].to_numpy()\n",
    "        ppname = payperiods[\"PPName\"].to_numpy()\n",
    "        cycle = payperiods[\"Cycle\"].to_numpy()\n",
    "        data[\"PPNum\"] = ppnum[pos]\n",
    "        data[\"PPName\"] = ppname[pos]\n",
    "        data[\"Cycle\"] = cycle[pos]\n",
    "        # create mapping for max fiscal year per payperiod to determine which fiscal year a payperiod should bleong to\n",
    "        mapping_table = data.groupby([\"days_offset\",\"PPName\"]).agg({\"FiscalYear\":\"max\"}).reset_index(drop=False)\n",
    "        data = data.drop(columns=[\"FiscalYear\"])\n",
    "        data = pd.merge(data, mapping_table, on=[\"days_offset\", \"PPName\"], how=\"left\")\n",
    "        # drop intermediate columns\n",
    "        data = data.drop(columns=[\"days_offset\", \"date_shifted\"])\n",
    "        # data.loc[:,[\"PPName\", \"PPNum\", \"Cycle\", \"FiscalYear\"]].drop_duplicates().to_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" / \"PayPeriods.csv\", index=False)\n",
    "            # take care of the duplicated problem - e.g., 23-PP22 has Fiscal year 2023 and 2024 - rank by PPName and FiscalYear, than drop the earlier FiscalYear record\n",
    "        return data\n",
    "\n",
    "    def _process_units(self) -> None:\n",
    "        \"\"\" \n",
    "            this function read and process Unit files that contains unit numbers for each location\n",
    "        \"\"\"\n",
    "        acres = pd.read_csv(self.gold_path[\"payroll\"]/\"Unit.csv\",dtype={\"Location\":str, \"Unit\":float})\n",
    "        acres[\"Location\"] = acres[\"Location\"].str.strip()\n",
    "        doc_rename = {\"Airdrie Grain\": \"Airdrie (grain)\", \"Aridrie Cattle\":\"Airdrie\", \"Arizona All\":\"Arizona (produce)\",\n",
    "                    \"BC Cattle (head days 365)\":\"BritishColumbia (cattle)\", \"BC Produce\":\"BritishColumbia (produce)\", \n",
    "                    \"Box Elder\":\"Havre\", \"Eddystone Cattle\":\"Eddystone (cattle)\", \"Eddystone Grain\":\"Eddystone (grain)\",\n",
    "                    \"Monette Seeds CDN (avg met. ton)\":\"Seeds\", \"Monette Seeds USA\":\"Seeds USA\", \"NexGen (avg met. ton)\":\"NexGen\",\n",
    "                    \"Waldeck\":\"Waldeck\", \"Calderbank\":\"Calderbank\"}\n",
    "        acres[\"Location\"] = acres[\"Location\"].replace(doc_rename)\n",
    "        acres[\"Pillar\"] = acres.apply(lambda x: self._pillar_classification(x),axis=1)\n",
    "        acres.to_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" /\"Unit_PowerBI.csv\",index=False)\n",
    "\n",
    "    def _payroll_project(self) -> None: \n",
    "        \"\"\" \n",
    "            will run _finance_operational() first\n",
    "            output: details + cost per unit (units per location input sheet) + average cost per unit for FY\n",
    "        \"\"\"\n",
    "        self.check_file(self.gold_path[\"payroll\"].parent/ \"OtherTables\")\n",
    "        print(\"\\nStarting Payroll Project Transformation\\n\")\n",
    "\n",
    "        # load and filter accounts for wages and contract labor\n",
    "        account = self.silver_acc[(self.silver_acc[\"Category\"].isin([\"Wages and benefits - direct\",\"Wages and benefits - overhead\"]) | (self.silver_acc[\"AccNum\"].isin([\"MFAZ595001\",\"MFBC536030\"])))] \n",
    "        # load only with transaction date later than 2021-12-20, and without \"Accrual\" in the memo\n",
    "        if self.is_dev:\n",
    "            data = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "        else:\n",
    "            if not self.pl_exist: self._finance_operational()\n",
    "            data = self.gold_pl.copy(deep=True)\n",
    "        data = data[data[\"AccID\"].isin(account.AccID.unique())]\n",
    "        data[\"TransactionDate\"] = pd.to_datetime(data[\"TransactionDate\"])\n",
    "        data = data[data[\"TransactionDate\"]>=dt.datetime(2021,12,20)].reset_index(drop=True)\n",
    "        data = data[~data[\"Memo\"].str.contains(\"Accrual\",case=False,na=False)]\n",
    "        # allocating payperiods\n",
    "        data = self._process_pp(data=data)\n",
    "        # standardizing location\n",
    "        # data.loc[data[\"Location\"]==\"Airdrie (corporate)\", \"Pillar\"] = \"Cattle\"                # deprecated\n",
    "        # data.loc[data[\"Location\"]==\"Airdrie (corporate)\", \"Location\"] = \"Airdrie (cattle)\"    # deprecated\n",
    "        data.loc[data[\"Location\"]==\"Eddystone (corporate)\", \"Pillar\"] = \"Unclassified\"\n",
    "        data.loc[data[\"Location\"]==\"Eddystone (corporate)\", \"Location\"] = \"Unassigned\"\n",
    "        data.loc[data[\"Location\"]==\"Legacy\", \"Location\"] = \"Unassigned\"\n",
    "        data.loc[(data[\"Location\"].str.contains(\"corporate\",case=False,na=False)&(data[\"Location\"]!=\"BritishColumbia (corporate)\")),\"Location\"] = \"Corporate\"\n",
    "        ## move BC ranches into BC Cattle\n",
    "        data.loc[(data[\"Location\"].isin(self.bc_ranches+[\"BritishColumbia (corporate)\"])), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        data.loc[data[\"Location\"] == \"BritishColumbia (cattle)\", \"Pillar\"] = \"Cattle-CowCalf\"\n",
    "        # summarizing data\n",
    "        ## by Location per PP\n",
    "        data_summarized = pd.DataFrame(data.groupby([\"Location\",\"PPName\",\"Pillar\",\"FiscalYear\",\"Cycle\",\"PPNum\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False))\n",
    "        assert len(data_summarized) == len(data.groupby([\"Location\",\"PPName\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)), \"Duplicated value detected for per Location per PP calculation\"\n",
    "        ## join acres data for CostPerUnit compute\n",
    "        print(\"Summarizing ...\")\n",
    "        acres = pd.read_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" /\"Unit_PowerBI.csv\",dtype={\"Location\":str, \"Unit\":float})\n",
    "        acres = acres.loc[:,[\"Location\", \"Unit\"]]\n",
    "        ### create BC cattle total units\n",
    "        total_bc = 0\n",
    "        for l in self.bc_ranches+[\"BritishColumbia (corporate)\"]:\n",
    "            total_bc += acres.loc[acres[\"Location\"]==l, \"Unit\"].item()\n",
    "        acres.loc[acres[\"Location\"]==\"BritishColumbia (cattle)\", \"Unit\"] = total_bc\n",
    "        acres[\"Unit\"] = acres[\"Unit\"].replace({0: 1})\n",
    "        print(f\"Unaccounted location for Acres Doc: {set(acres.Location.unique()) - set(data_summarized.Location.unique())}\")\n",
    "        print(f\"Unaccounted location for QBO Payroll: {set(data_summarized.Location.unique()) - set(acres.Location.unique())}\")\n",
    "        data_summarized = pd.merge(data_summarized, acres, on=\"Location\", how=\"left\")\n",
    "        data_summarized[\"CostPerUnit\"] = data_summarized[\"AmountDisplay\"] / data_summarized[\"Unit\"] * 26\n",
    "        data_summarized[\"Count\"] = 1\n",
    "        ## by Location\n",
    "        data_summarized2 = data_summarized.groupby(by=[\"Location\",\"FiscalYear\",\"Pillar\"]).agg({\"CostPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        data_summarized2 = data_summarized2.rename(columns={\"CostPerUnit\":\"Avg CostPerUnit\"})\n",
    "        assert len(data_summarized2) == len(data_summarized.groupby(by=[\"Location\",\"FiscalYear\"]).agg({\"CostPerUnit\":\"mean\"})), \"Duplicated value detected for per Location calculation\"\n",
    "        ## by pillar\n",
    "        data_summarized3 = data_summarized2.groupby(by=[\"FiscalYear\",\"Pillar\"]).agg({\"Avg CostPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(data_summarized3) == len(data_summarized.groupby(by=[\"Pillar\",\"FiscalYear\"]).agg({\"CostPerUnit\":\"mean\"})), \"Duplicated value detected for per Pillar calculation\"\n",
    "        # saving\n",
    "        print(\"Saving ...\")\n",
    "        self.check_file(self.gold_path[\"payroll\"])\n",
    "        data.to_excel(self.gold_path[\"payroll\"]/\"Payroll.xlsx\", sheet_name=\"Payroll\", index=False)\n",
    "        self.check_file(self.gold_path[\"hr_combined\"] / \"CSV\")\n",
    "        data_summarized.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"payroll_summarized1.csv\", index=False)\n",
    "        data_summarized2.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"payroll_summarized2.csv\", index=False)\n",
    "        data_summarized3.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"payroll_summarized3.csv\", index=False)\n",
    "\n",
    "    def _QBOTime_project(self) -> None:\n",
    "        \"\"\" \n",
    "            apply PP allocation to QBO Time data, clean locaiton, and join relevant info into one table\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting QBO Time Project Transformation\\n\")\n",
    "        # read files\n",
    "        timesheets = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"timesheets.csv\")\n",
    "        jobcode = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"jobcodes.csv\")\n",
    "        users = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"users.csv\")\n",
    "        group = pd.read_csv(self.silver_path[\"QBO\"][\"Time\"]/\"group.csv\")\n",
    "        print(f\"Read {len(timesheets)} timesheet records, {len(jobcode)} jobcodes, {len(users)} users, {len(group)} groups\")\n",
    "        timesheets_len, users_len = len(timesheets), len(users)\n",
    "        # clean up location in group table\n",
    "        ## Arizona - all produce\n",
    "        group.loc[((group[\"corp_short\"]==\"A\")&(group[\"location_name\"]==\"Monette Farms AZ\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"A\")&(group[\"location_name\"]==\"Monette Produce USA\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"A\")&(group[\"location_name\"]==\"Monette Seeds USA\")), \"Location\"] = \"Arizona (produce)\"\n",
    "        ## BC\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Ashcroft Ranch\")), \"Location\"] = \"Ashcroft\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Cache/Fischer/Loon\")), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"].str.contains(\"silage\", case=False))), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Diamond S Ranch\")), \"Location\"] = \"Diamond S\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Fraser River Ranch\")), \"Location\"] = \"Fraser River Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Home Ranch (70 Mile, LF/W, BR)\")), \"Location\"] = \"Home Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Moon Ranch\")), \"Location\"] = \"Moon Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Produce\")), \"Location\"] = \"BritishColumbia (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"Wolf Ranch\")), \"Location\"] = \"Wolf Ranch\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"SAWP\")), \"Location\"] = \"BritishColumbia (produce)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"BC\")&(group[\"location_name\"]==\"SAWP Produce\")), \"Location\"] = \"BritishColumbia (produce)\"\n",
    "        ## Outlook\n",
    "        group.loc[((group[\"corp_short\"]==\"O\")), \"Location\"] = \"Outlook\"\n",
    "        ## others\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Yorkton\")), \"Location\"] = \"Yorkton\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Airdrie\")), \"Location\"] = \"Airdrie\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"BC\")), \"Location\"] = \"Unassigned\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Calderbank\")), \"Location\"] = \"Calderbank\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Eddystone\")), \"Location\"] = \"Eddystone (unspecified)\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Hafford\")), \"Location\"] = \"Hafford\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Kamsack\")), \"Location\"] = \"Kamsack\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"MFUSA Billings\")), \"Location\"] = \"Billings\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"MFUSA Box Elder\")), \"Location\"] = \"Havre\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Nexgen Seeds\")), \"Location\"] = \"NexGen\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Prince Albert\")), \"Location\"] = \"Prince Albert\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Raymore\")), \"Location\"] = \"Raymore\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Regina\")), \"Location\"] = \"Regina\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Russel Approvals\")), \"Location\"] = \"Unassigned\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Seeds\")), \"Location\"] = \"Seeds\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Swift Current\")), \"Location\"] = \"Swift Current\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"The Pas\")), \"Location\"] = \"The Pas\"\n",
    "        group.loc[((group[\"corp_short\"]==\"CM\")&(group[\"location_name\"]==\"Waldeck\")), \"Location\"] = \"Waldeck\"\n",
    "        unclassified = group[group[\"Location\"].isna()].location_name.unique()\n",
    "        if len(unclassified) > 0: print(f\"\\nUnclassified location - {unclassified}\\n\")\n",
    "        # create another location column for general location where bc ranches are merged into one\n",
    "        group = group.rename(columns={\"Location\": \"Location (detail)\"})\n",
    "        group[\"Location\"] = group[\"Location (detail)\"]\n",
    "        group.loc[(group[\"Location (detail)\"].isin(self.bc_ranches+[\"BritishColumbia (corporate)\"])), \"Location\"] = \"BritishColumbia (cattle)\"\n",
    "        # merge tables into one table\n",
    "        ## merge location into users\n",
    "        users = pd.merge(users, group.loc[:,[\"group_id\", \"location_name\", \"Location\", \"Location (detail)\"]].drop_duplicates(), on=\"group_id\", how=\"left\")\n",
    "        ## merge users into timesheets\n",
    "        timesheets = pd.merge(timesheets,users.loc[:,[\"user_id\", \"group_id\", \"username\", \"full_name\", \"location_name\",\"Location\",\"Location (detail)\", \"first_name\", \"last_name\"]], on=\"user_id\", how=\"left\")\n",
    "        ## merge job into timesheets\n",
    "        timesheets = pd.merge(timesheets, jobcode.loc[:,[\"jobcode_id\",\"job_name\",\"type\"]].rename(columns={\"type\":\"job_type\"}), on=\"jobcode_id\", how=\"left\")\n",
    "        assert (len(users) == users_len) and (len(timesheets) == timesheets_len), f\"duplicated records found, timesheets - {timesheets_len} vs {len(timesheets)}; users - {users_len} vs {len(users)}\"\n",
    "        ## determine fiscal year\n",
    "        timesheets[\"date\"] = pd.to_datetime(timesheets[\"date\"])\n",
    "        timesheets[\"Month\"] = timesheets[\"date\"].dt.month_name()\n",
    "        timesheets[\"FiscalYear\"] = timesheets[\"date\"].dt.year \n",
    "        mask = timesheets[\"Month\"].isin([\"November\", \"December\"])\n",
    "        timesheets.loc[mask, \"FiscalYear\"] = timesheets.loc[mask, \"FiscalYear\"] + 1\n",
    "        # classify payperiods\n",
    "        timesheets = self._process_pp(data=timesheets)\n",
    "        # modify location for BC0\n",
    "        timesheets.loc[timesheets[\"user_id\"] == \"BC6107856\", \"Location\"] = \"Unassigned\"\n",
    "        # classify pillars\n",
    "        timesheets[\"Pillar\"] = timesheets.apply(lambda x: self._pillar_classification(x), axis=1)\n",
    "        timesheets.loc[timesheets[\"Pillar\"] == \"Missing\", \"Pillar\"] = \"Unclassified\"\n",
    "        # summarizing data\n",
    "        ## by Location per PP \n",
    "        summarized = timesheets.groupby([\"Location\",\"PPName\",\"FiscalYear\",\"Cycle\",\"PPNum\", \"Pillar\"]).agg({\"duration\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summarized) == len(timesheets.groupby([\"Location\",\"PPName\"]).agg({\"duration\":\"sum\"})), \"duplicated value detected for timsheet per Location per PP summarization\"\n",
    "        ## read units file\n",
    "        acres = pd.read_csv(self.gold_path[\"payroll\"].parent/ \"OtherTables\" /\"Unit_PowerBI.csv\",dtype={\"Location\":str, \"Unit\":float})\n",
    "        acres = acres.loc[:,[\"Location\", \"Unit\"]]\n",
    "        print(f\"Unaccounted location for Acres Doc: {set(acres.Location.unique()) - set(summarized.Location.unique())}\")\n",
    "        print(f\"Unaccounted location for timesheets: {set(summarized.Location.unique()) - set(acres.Location.unique())}\")\n",
    "        ### create BC cattle + Billings total units\n",
    "        total_bc = 0\n",
    "        for l in self.bc_ranches+[\"BritishColumbia (corporate)\"]:\n",
    "            total_bc += acres.loc[acres[\"Location\"]==l, \"Unit\"].item()\n",
    "        acres.loc[acres[\"Location\"]==\"BritishColumbia (cattle)\", \"Unit\"] = total_bc\n",
    "        # acres.loc[acres[\"Location\"]==\"Billings\", \"Unit\"] = acres[acres[\"Location\"].isin([\"Fly Creek\", \"Camp 4\"])].Unit.sum()\n",
    "        acres[\"Unit\"] = acres[\"Unit\"].replace({0: 1})\n",
    "        ## merge with units file\n",
    "        summarized = pd.merge(summarized, acres, on=\"Location\", how=\"left\")\n",
    "        ## calculate hours per unit\n",
    "        summarized[\"HoursPerUnit\"] = summarized[\"duration\"] / summarized[\"Unit\"] * 26\n",
    "        summarized[\"Count\"] = 1\n",
    "        # summarize per location\n",
    "        summarized2 = summarized.groupby(by=[\"Location\",\"FiscalYear\", \"Pillar\"]).agg({\"HoursPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        summarized2 = summarized2.rename(columns={\"HoursPerUnit\":\"Avg HoursPerUnit\"})\n",
    "        assert len(summarized2) == len(timesheets.groupby([\"Location\",\"FiscalYear\"]).agg({\"duration\":\"sum\"})), \"duplicated value detected for timsheet per Location summarization\"\n",
    "        # summarize per pillar\n",
    "        summarized3 = summarized2.groupby(by=[\"FiscalYear\", \"Pillar\"]).agg({\"Avg HoursPerUnit\":\"mean\", \"Count\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summarized3) == len(timesheets[timesheets[\"Pillar\"]!=\"Missing\"].groupby([\"Pillar\",\"FiscalYear\"]).agg({\"duration\":\"sum\"})), \"duplicated value detected for timsheet per Pillar summarization\"\n",
    "\n",
    "        # saving\n",
    "        print(\"Saving ...\\n\")\n",
    "        self.check_file(self.gold_path[\"QBOTime\"])\n",
    "        timesheets.to_excel(self.gold_path[\"QBOTime\"]/\"QBOTime.xlsx\", sheet_name = \"QBOTime\", index=False)\n",
    "        self.check_file(self.gold_path[\"hr_combined\"]/ \"CSV\")\n",
    "        summarized.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"time_summarized1.csv\", index=False)\n",
    "        summarized2.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"time_summarized2.csv\", index=False)\n",
    "        summarized3.to_csv(self.gold_path[\"hr_combined\"]/ \"CSV\" / \"time_summarized3.csv\", index=False)\n",
    "\n",
    "    def _hr_summary(self) -> None:\n",
    "        \"\"\" \n",
    "            This function consolidate payroll and QBO time summaries into one table for consolidated insights\n",
    "        \"\"\"\n",
    "        final_df = [pd.DataFrame(), pd.DataFrame(), pd.DataFrame()]\n",
    "        for i in [1, 2, 3]:\n",
    "            payroll = pd.read_csv(self.gold_path[\"hr_combined\"] / \"CSV\" / f\"payroll_summarized{i}.csv\")\n",
    "            payroll_rename = {\"AmountDisplay\": \"TotalAmount\", \"CostPerUnit\": \"AmountPerUnit\", \"Avg CostPerUnit\": \"Avg AmountPerUnit\"}\n",
    "            payroll = payroll.rename(columns=payroll_rename)\n",
    "            payroll[\"Mode\"] = \"Payroll\"\n",
    "            time = pd.read_csv(self.gold_path[\"hr_combined\"] / \"CSV\" / f\"time_summarized{i}.csv\")\n",
    "            time_rename = {\"duration\": \"TotalAmount\", \"HoursPerUnit\": \"AmountPerUnit\", \"Avg HoursPerUnit\": \"Avg AmountPerUnit\"}\n",
    "            time = time.rename(columns=time_rename)\n",
    "            time[\"Mode\"] = \"Hours\"\n",
    "            final_df[i-1] = pd.concat([payroll, time], ignore_index=True)\n",
    "        final_df[0].to_excel(self.gold_path[\"hr_combined\"]/\"Summarized.xlsx\", sheet_name=\"Summarized\", index=False)\n",
    "        final_df[1].to_excel(self.gold_path[\"hr_combined\"]/\"Summarized2.xlsx\", sheet_name=\"Summarized2\", index=False)\n",
    "        final_df[2].to_excel(self.gold_path[\"hr_combined\"]/\"Summarized3.xlsx\", sheet_name=\"Summarized3\", index=False)\n",
    "\n",
    "    def _inventory_settlement(self) -> None:\n",
    "        \"\"\" \n",
    "            prepare the data from raw QBO table for inventory project: only extracting partial Invoice, SalesReceipt, and Journal Entry\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Inventory Project Transformation ...\\n\")\n",
    "        corps = [\"MFL\", \"MFUSA\"]\n",
    "        cols = [\"TransactionDate\", \"TransactionType\", \"TransactionID\", \"Corp\", \"Qty\", \"AccID\", \"FarmID\", \"CustomerID\",\n",
    "                \"DocNumber\", \"TransactionEntered\", \"Amount\"]\n",
    "        journal_cols = [col for col in cols if col not in [\"Qty\",\"AveragePrice\"]]\n",
    "        # read tables\n",
    "        print(\"Loading raw tables ...\")\n",
    "        account = pd.read_csv(self.gold_path[\"finance_operational\"]/\"Account_table.csv\")\n",
    "        account = account[account[\"Corp\"].isin(corps)]\n",
    "        account = account[account[\"AccountType\"] == \"Income\"]\n",
    "        farm = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension_time\"]/\"Farm.csv\")\n",
    "        farm = farm[farm[\"Corp\"].isin(corps)]\n",
    "        customer = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension_time\"]/\"Customer.csv\")\n",
    "        customer = customer[customer[\"Corp\"].isin(corps)]\n",
    "        first_date = dt.datetime(2023,11,1)\n",
    "        invoice = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"Invoice.csv\")\n",
    "        invoice = invoice[invoice[\"Corp\"].isin(corps)]\n",
    "        invoice[\"TransactionDate\"] = pd.to_datetime(invoice[\"TransactionDate\"])\n",
    "        invoice = invoice[invoice[\"TransactionDate\"]>=first_date]\n",
    "        invoice = invoice[invoice[\"AccID\"].isin(account.AccID.unique())]\n",
    "        # if len(invoice) >= 1:\n",
    "        #     invoice[\"AveragePrice\"] = invoice[\"Amount\"] / invoice[\"Qty\"]        # compute price per MT\n",
    "        sales = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"SalesReceipt.csv\")\n",
    "        sales = sales[sales[\"Corp\"].isin(corps)]\n",
    "        sales[\"TransactionDate\"] = pd.to_datetime(sales[\"TransactionDate\"])\n",
    "        sales = sales[sales[\"TransactionDate\"]>=first_date]\n",
    "        sales = sales[sales[\"AccID\"].isin(account.AccID.unique())]\n",
    "        # if len(sales) >= 1:\n",
    "        #     sales[\"AveragePrice\"] = sales[\"Amount\"] / sales[\"Qty\"]        # compute price per MT\n",
    "        journal = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"JournalEntry.csv\", dtype={\"FarmID\":str, \"ClassID\":str, \"CustomerID\":str, \"EmployeeID\":str}, usecols=journal_cols)\n",
    "        journal = journal[journal[\"AccID\"].isin(account.AccID.unique())]\n",
    "        journal[\"TransactionDate\"] = pd.to_datetime(journal[\"TransactionDate\"])\n",
    "        journal = journal[journal[\"TransactionDate\"]>=first_date]\n",
    "        journal = journal[~journal[\"TransactionEntered\"].str.contains(\"Delivered and not settled\", na=False)]\n",
    "        journal = journal[~journal[\"TransactionEntered\"].str.contains(\"Grain Inventory Receivable Adjustment\", na=False)]\n",
    "        # combining tables\n",
    "        print(\"Combining Fact Tables ...\")\n",
    "        invoice = invoice.loc[:,[col for col in cols if col in invoice.columns]]\n",
    "        sales = sales.loc[:,[col for col in cols if col in sales.columns]]\n",
    "        journal = journal.loc[:,[col for col in cols if col in journal.columns]]\n",
    "        facts = pd.concat([invoice, sales, journal], ignore_index=True)\n",
    "        del invoice, sales, journal\n",
    "        # join facts with dimension tables\n",
    "        facts = pd.merge(facts, account.loc[:,[\"AccID\",\"AccNum\",\"AccName\",\"Category\",\"Subcategory\",\"Commodity\"]], on=[\"AccID\"], how=\"left\")\n",
    "        facts = pd.merge(facts, farm.loc[:,[\"FarmID\",\"FarmName\"]], on=[\"FarmID\"], how=\"left\")\n",
    "        facts = pd.merge(facts, customer.loc[:,[\"CustomerID\",\"CustomerName\"]], on=[\"CustomerID\"], how=\"left\")\n",
    "        facts = facts[facts[\"Subcategory\"]==\"Grain - cash settlements\"]\n",
    "        print(f\"Total Fact Entries - {len(facts)}\")\n",
    "        # product column\n",
    "        # facts[\"Product\"] = facts[\"AccName\"].apply(lambda x: self._temp_get_product(x)) - now using commodity from account table\n",
    "        # saving file\n",
    "        print(\"Saving Files ...\")\n",
    "        self.check_file(self.gold_path[\"inventory\"])\n",
    "        facts.to_excel(self.gold_path[\"inventory\"]/\"Settlement\"/\"QBO_Grain_Settlements.xlsx\", sheet_name=\"settlement\", index=False)\n",
    "        print(\"Finished\\n\")\n",
    "\n",
    "    def _buget_process_input(self, inputdata_path:Path, processed_path:Path) -> None:\n",
    "        \"\"\" \n",
    "            this function processes and saves budget totals for production, input (chem/fert/seed), produce budgets, and JD Lease\n",
    "        \"\"\"\n",
    "        ## commodity prices - everything is CAD except Winter Wheat is in USD - convert everything to CAD\n",
    "        pricing = pd.read_csv(inputdata_path/\"25-Grain-Pricing.csv\")\n",
    "        pricing.loc[pricing[\"Commodity\"]==\"WW\", \"ForecastPrice\"] *= self.fx\n",
    "        ## production budget\n",
    "        budget_production = pd.read_csv(inputdata_path/\"25-Grain-Revenue.csv\")\n",
    "        budget_production = budget_production.melt(\n",
    "            id_vars=[\"Location\", \"Currency\", \"Type\"],\n",
    "            var_name=\"Commodity\",\n",
    "            value_name = \"Amount\"\n",
    "        )\n",
    "        budget_production = budget_production.fillna(value = {\"Amount\": 0})\n",
    "        budget_production[\"Commodity\"] = budget_production[\"Commodity\"].replace({\"Hay/Silage\":\"Hay\"})\n",
    "        budget_production.loc[((budget_production[\"Location\"]==\"Airdrie\")&(budget_production[\"Commodity\"]==\"Hay\")), \"Commodity\"] = \"Silage\" # only Airdrie has silage, others have hay\n",
    "        budget_production_summary = pd.DataFrame(budget_production.groupby([\"Location\",\"Currency\",\"Commodity\"]).agg({\"Amount\": \"prod\"})).reset_index(drop=False)\n",
    "        budget_production_summary = budget_production_summary.rename(columns={\"Amount\":\"TotalYield\"})\n",
    "        ### merge yield with commodity price to calculate forecast production value of commodities\n",
    "        budget_production_summary = pd.merge(budget_production_summary,pricing,on=[\"Commodity\"], how=\"left\")\n",
    "        ### manual adjustments to prices\n",
    "        budget_production_summary.loc[((budget_production_summary[\"Location\"] == \"Airdrie\") & (budget_production_summary[\"Commodity\"] == \"Hay\")), \"ForecastPrice\"] = 85\n",
    "        budget_production_summary.loc[((budget_production_summary[\"Location\"] == \"Colorado (Genoa)\") & (budget_production_summary[\"Commodity\"] == \"WW\")), \"ForecastPrice\"] = 12.5 * self.fx\n",
    "        budget_production_summary.loc[budget_production_summary[\"Location\"] == \"Yorkton\", \"ForecastPrice\"] *= 2/3\n",
    "        budget_production_summary[\"ForecastProductionCAD\"] = budget_production_summary[\"TotalYield\"] * budget_production_summary[\"ForecastPrice\"]\n",
    "        budget_production_summary = budget_production_summary[budget_production_summary[\"ForecastProductionCAD\"].notna()]\n",
    "        budget_production_summary = budget_production_summary[budget_production_summary[\"ForecastProductionCAD\"]!=0]\n",
    "        ### convert prices back to USD for a adjusted column\n",
    "        budget_production_summary[\"ForecastProductionAdj\"] = budget_production_summary.apply(lambda x: x[\"ForecastProductionCAD\"] / self.fx if x[\"Currency\"] == \"USD\" else x[\"ForecastProductionCAD\"],axis=1)\n",
    "        ### save production budget\n",
    "        budget_production_summary.to_csv(processed_path/\"budget_production.csv\",index=False)\n",
    "        ## input budget\n",
    "        input_budget = pd.read_csv(inputdata_path/\"25-Input-Budget.csv\")\n",
    "        input_budget = input_budget.drop(columns=[\"Total acres\"])\n",
    "        input_budget = input_budget.melt(\n",
    "            id_vars = [\"Location\", \"Type\"],\n",
    "            var_name = \"Commodity\",\n",
    "            value_name = \"Amount\"\n",
    "        )\n",
    "        input_budget = input_budget.fillna(value = {\"Amount\": 0})\n",
    "        input_budget.loc[((input_budget[\"Location\"]==\"Yorkton\")&(input_budget[\"Type\"].isin([\"Fertilizer\",\"Chemical\",\"Seed\"]))), \"Amount\"] *= 2/3\n",
    "        input_budget.to_csv(processed_path/\"input_budget.csv\",index=False)\n",
    "        ## labour budget\n",
    "        labour_budget = pd.read_csv(inputdata_path/\"25-Labour-Budget.csv\")\n",
    "        labour_budget = labour_budget.melt(\n",
    "            id_vars = [\"Location\",\"Currency\"],\n",
    "            var_name = \"Month\",\n",
    "            value_name = \"LabourBudgetCAD\"\n",
    "        )\n",
    "        labour_budget[\"LabourBudgetAdj\"] = labour_budget.apply(lambda x: x[\"LabourBudgetCAD\"]/self.fx if x[\"Currency\"]==\"USD\" else x[\"LabourBudgetCAD\"], axis=1)\n",
    "        labour_budget.to_csv(processed_path/\"labour_budget.csv\",index=False)\n",
    "        ## outlook budget\n",
    "        outlook = pd.read_csv(inputdata_path/\"25-Outlook-Detail.csv\")\n",
    "        outlook = outlook.melt(\n",
    "            id_vars=[\"Type\", \"ProfitType\"],\n",
    "            var_name=\"Commodity\",\n",
    "            value_name=\"Amount\"\n",
    "        )\n",
    "        outlook = outlook.fillna(value={\"Amount\": 0})\n",
    "        outlook.to_csv(processed_path/\"outlook_budget.csv\", index=False)\n",
    "        ## AZ budget\n",
    "        az = pd.read_csv(inputdata_path / \"25-AZ-Detail.csv\")\n",
    "        az = az.melt(\n",
    "            id_vars=[\"Type\", \"ProfitType\"],\n",
    "            var_name=\"CommodityRaw\",\n",
    "            value_name=\"AmountCAD\"\n",
    "        )\n",
    "        az = az.fillna(value={\"AmountCAD\": 0})\n",
    "        az.to_csv(processed_path/\"az_budget.csv\", index=False)\n",
    "        ## BC produce details\n",
    "        bc = pd.read_csv(inputdata_path / \"25-BC-Detail.csv\")\n",
    "        bc = bc.melt(\n",
    "            id_vars=[\"Type\", \"ProfitType\"],\n",
    "            var_name=\"CommodityRaw\",\n",
    "            value_name=\"AmountCAD\"\n",
    "        )\n",
    "        bc = bc.fillna(value={\"AmountCAD\": 0})\n",
    "        bc.to_csv(processed_path/\"bc_budget.csv\", index=False)\n",
    "        ## JD lease\n",
    "        jdlease = pd.read_csv(inputdata_path/\"25-JD-Lease-Summary.csv\")\n",
    "        jdlease = jdlease[jdlease[\"AllocatedCost25\"] != 0]\n",
    "        jdlease.to_csv(processed_path/\"JD_lease.csv\", index=False)\n",
    "\n",
    "    def _budget_read_outsidedata(self,processed_path:Path) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "        \"\"\" \n",
    "            this function reads all the processed outside data and standardize the commodity and location naming\n",
    "        \"\"\"\n",
    "        production_budget = pd.read_csv(processed_path/\"budget_production.csv\")\n",
    "        input_budget = pd.read_csv(processed_path/\"input_budget.csv\")\n",
    "        labour_budget = pd.read_csv(processed_path/\"labour_budget.csv\")\n",
    "        outlook_budget = pd.read_csv(processed_path/\"outlook_budget.csv\")\n",
    "        jdlease = pd.read_csv(processed_path/\"JD_lease.csv\")\n",
    "        az_budget = pd.read_csv(processed_path/\"az_budget.csv\")\n",
    "        bc_budget = pd.read_csv(processed_path/\"bc_budget.csv\")\n",
    "        ## standardizing commodity naming\n",
    "        production_rename_commodity = {\"R Lentils\":\"Red Lentil\", \"G Lentils\":\"Green Lentil\",\"Chickpeas\":\"Chickpea\",\"Peas\":\"Field Pea\", \"WW\": \"Winter Wheat\"}\n",
    "        input_rename_commodity = {\"R Lentils\":\"Red Lentil\", \"G Lentils\":\"Green Lentil\",\"Chickpeas\":\"Chickpea\", \"WW\": \"Winter Wheat\"}\n",
    "        outlook_rename_commodity = {\"Broccoli-cases/ac\":\"Broccoli\", \"Cabbage-lbs/ac\":\"Cabbage\", \"Carrots-lbs\":\"Carrot\", \"Cauliflower-cases/ac\":\"Cauliflower\",\n",
    "                                    \"Table Potato-lbs\":\"Potato\", \"Seed Potato-lbs\":\"Potato\", \"Commercial Pumpkins-Bins/ac\":\"Pumpkin\", \"Strawberry Upick-lbs\":\"Strawberry\",\n",
    "                                    \"Pumpkin Upick-pieces/ac\":\"Pumpkin\", \"Corn Maze-lbs\":\"Prairie Pathways\", \"WW\": \"Winter Wheat\", \"Corn (Sweet) Cobs\":\"Sweet Corn\"}\n",
    "        az_rename_commodity = {\"Broccoli-cases/ac\":\"Broccoli\", \"Cabbage-lbs/ac\":\"Cabbage\", \"Pumpkins-Bins/ac\":\"Pumpkin\", \"WatermelonLG-bins/ac\": \"Watermelon\",\n",
    "                            \"WatermelonMini-cases/ac\": \"Watermelon\"}\n",
    "        bc_rename_commodity = {\"Broccoli-cases/ac\":\"Broccoli\", \"WatermelonLG-bins/ac\": \"Watermelon\", \"WatermelonMini-cases/ac\": \"Watermelon\", \"Pumpkins-Bins/ac\":\"Pumpkin\",\n",
    "                            \"Squash-lbs\": \"Squash\"}\n",
    "        outlook_budget[\"CommodityRaw\"] = outlook_budget[\"Commodity\"]\n",
    "        production_budget[\"Commodity\"] = production_budget[\"Commodity\"].replace(production_rename_commodity)\n",
    "        input_budget[\"Commodity\"] = input_budget[\"Commodity\"].replace(input_rename_commodity)\n",
    "        outlook_budget[\"Commodity\"] = outlook_budget[\"Commodity\"].replace(outlook_rename_commodity)\n",
    "        az_budget[\"Commodity\"] = az_budget[\"CommodityRaw\"].replace(az_rename_commodity)\n",
    "        bc_budget[\"Commodity\"] = bc_budget[\"CommodityRaw\"].replace(bc_rename_commodity)\n",
    "        ## standardizing location naming - merge calderbank grain with Swift Current\n",
    "        jdlease_rename_location = {\"Swift Current Total\":\"Swift Current\", \"Regina Farm\":\"Regina\", \"Calderbank\":\"Swift Current\",\n",
    "                                \"Airdrie\":\"Airdrie (grain)\", \"Eddystone\":\"Eddystone (grain)\"}\n",
    "        labour_rename_location = {\"NexGen (avg met. ton)\":\"NexGen\", \"Cache/Fisher/Look\":\"Aschroft\", \"MF AZ\":\"Arizona (produce)\", \"Box Elder\":\"Havre\", \n",
    "                                \"BC Veg\":\"BritishColumbia (produce)\",\"Monette Seeds CDN (avg met. ton)\":\"Monette Seeds\", \n",
    "                                \"BC Cattle (avg head)\":\"BritishColumbia (cattle)\", \"Eddystone Cattle (avg head)\":\"Eddystone (cattle)\",\n",
    "                                \"Swift Current Cattle (avg head)\":\"Waldeck\", \"Aridrie Cattle (avg head)\":\"Airdrie (cattle)\",\n",
    "                                \"Airdrie Farm\":\"Airdrie (grain)\", \"Eddystone Farm\":\"Eddystone (grain)\",\"Calderbank\":\"Calderbank (cattle)\"}\n",
    "        input_rename_location =  {\"Fly Creek/Camp 1\":\"Fly Creek\", \"Regina Farm\":\"Regina\",\"Swift Current Total\":\"Swift Current\", \"Box Elder\":\"Havre\", \"Regina Farm\":\"Regina\",\n",
    "                                \"Calderbank\":\"Calderbank (grain)\",\"Airdrie\":\"Airdrie (grain)\", \"Eddystone\":\"Eddystone (grain)\"}\n",
    "        production_rename_location = {\"Fly Creek/Camp 1\":\"Fly Creek\", \"Regina Farm\":\"Regina\",\"Swift Current Total\":\"Swift Current\", \"Box Elder\":\"Havre\", \"Regina Farm\":\"Regina\",\n",
    "                                    \"Colorado (Genoa)\":\"Colorado\", \"Calderbank\":\"Swift Current\",\"Airdrie\":\"Airdrie (grain)\", \"Eddystone\":\"Eddystone (grain)\"}\n",
    "        input_budget[\"Location\"] = input_budget[\"Location\"].replace(input_rename_location)\n",
    "        production_budget[\"Location\"] = production_budget[\"Location\"].replace(production_rename_location)\n",
    "        labour_budget[\"Location\"] = labour_budget[\"Location\"].replace(labour_rename_location)\n",
    "        jdlease[\"Location\"] = jdlease[\"Location\"].replace(jdlease_rename_location)\n",
    "        ## put input budget (chem/fert/seed) into aggregated totals\n",
    "        input_budget2 = input_budget.groupby([\"Location\",\"Type\"]).agg({\"Amount\":\"sum\"}).reset_index(drop=False)\n",
    "        input_budget2.loc[((input_budget2[\"Location\"].isin([\"Camp 4\",\"Fly Creek\", \"Havre\"]))&(input_budget2[\"Type\"]!=\"Acres\")), \"Amount\"] *= self.fx\n",
    "        ## aggregated totals for production budget and JD Lease\n",
    "        production_budget = pd.DataFrame(production_budget.groupby([\"Location\",\"Currency\",\"Commodity\",\"ForecastPrice\"]).agg({\"TotalYield\":\"sum\", \"ForecastProductionCAD\":\"sum\", \"ForecastProductionAdj\":\"sum\"}).reset_index(drop=False))\n",
    "        jdlease = pd.DataFrame(jdlease.groupby([\"Location\",\"Country\",\"Currency\",\"TotalCost25\"]).agg({\"Acres25\":\"sum\",\"AllocatedCost25\":\"sum\"}).reset_index(drop=False))\n",
    "        return input_budget2, production_budget, labour_budget, jdlease, az_budget, bc_budget, outlook_budget\n",
    "\n",
    "    def _budget_process_produce(self, budget_rules:pd.DataFrame,budget:pd.DataFrame,sheetname:str) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            this function provides a standardized way to process produce budgets\n",
    "        \"\"\"\n",
    "        budget_rules = budget_rules[budget_rules[\"SheetRef\"] == sheetname].copy(deep=True)\n",
    "        budget_rules[\"Commodity\"] = budget_rules.apply(lambda x: self._identify_product(x,for_budget=True), axis=1)\n",
    "        budget[\"Type\"] = budget[\"Type\"].str.strip()\n",
    "        # gross income - by commodity\n",
    "        reference = budget[budget[\"Type\"].isin([\"Acres\",\"Unit Price\",\"YieldPerAc\"])]\n",
    "        reference = reference.groupby([\"Commodity\",\"ProfitType\",\"CommodityRaw\"]).agg({\"AmountCAD\":\"prod\"}).reset_index(drop=False)\n",
    "        reference = reference.groupby([\"Commodity\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False)\n",
    "        reference = reference.rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        reference[\"Category\"] = \"Produce - production\"\n",
    "        if \"outlook\" in sheetname.lower():\n",
    "            for item in [\"Prairie Pathways\", \"Market Garden / CSA\"]:\n",
    "                reference.loc[reference[\"Commodity\"] == item, \"Category\"] = \"Produce - cash settlements\"\n",
    "        # seed expense - by commodity\n",
    "        expense = budget[budget[\"Type\"] == \"Seed\"].copy(deep=True)\n",
    "        expense = expense.drop(columns=\"CommodityRaw\")\n",
    "        expense = expense.groupby([\"Commodity\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False).rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        expense[\"Category\"] = \"Seed\"\n",
    "        # other expense - Fertilizer/Chemical - not by commodity\n",
    "        expense2 = budget[budget[\"Type\"].isin([\"Fertilizer\",\"Chemical\"])]\n",
    "        expense2 = expense2.groupby([\"Type\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False).rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        expense2[\"Commodity\"] = \"Others\"\n",
    "        expense2 = expense2.rename(columns={\"Type\":\"Category\"})\n",
    "        # combine\n",
    "        budget_produce = pd.merge(budget_rules, pd.concat([reference,expense, expense2]), on=[\"Commodity\",\"Category\"], how=\"left\")\n",
    "        budget_produce = budget_produce.fillna(value={\"TotalAmountCAD\":0})\n",
    "        budget_produce[\"AmountCAD\"] = budget_produce.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "        return budget_produce\n",
    "\n",
    "    def _budget_get_transactions(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            get actuals\n",
    "        \"\"\"\n",
    "        if self.is_dev:\n",
    "            transactions = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "            self.operation_acc = pd.read_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\")\n",
    "        else:\n",
    "            transactions = self.gold_pl.copy(deep=True)\n",
    "        transactions = transactions[transactions[\"FiscalYear\"] >= 2024]\n",
    "        transactions[\"AccName\"] = transactions[\"AccName\"].str.strip()\n",
    "        return transactions\n",
    "\n",
    "    def _create_budget(self, process_input:bool = False) -> None:\n",
    "        \"\"\" \n",
    "            In Progress: this function generates budgets\n",
    "        \"\"\"\n",
    "        if self.is_dev:\n",
    "            self.fx = 1.3988\n",
    "        location_adj = {\"Camp 4\": \"Billings\", \"Fly Creek\":\"Billings\"}\n",
    "        print(\"\\nCreating Budget\\n\")\n",
    "        if not self.is_dev:\n",
    "            if not self.pl_exist: self._finance_operational()\n",
    "        inputdata_path = self.gold_path[\"budget\"] / \"Outside Data\"\n",
    "        processed_path = self.gold_path[\"budget\"] / \"Processed Data\"\n",
    "        rule_path = self.gold_path[\"budget\"] / \"Budget Rules\"\n",
    "        copied_path = self.gold_path[\"budget\"]/\"Copied Data\"\n",
    "\n",
    "        # load actuals\n",
    "        transactions = self._budget_get_transactions()\n",
    "        \n",
    "        # process outside data\n",
    "        if process_input:\n",
    "            self._buget_process_input(inputdata_path=inputdata_path, processed_path=processed_path)\n",
    "        \n",
    "        # read outside data\n",
    "        input_budget2, production_budget, labour_budget, jdlease, az_budget, bc_budget, outlook_budget = self._budget_read_outsidedata(processed_path=processed_path)\n",
    "\n",
    "        # calculate Budgets\n",
    "        ## outside data\n",
    "        ### read rules\n",
    "        budget_rules = pd.read_csv(rule_path/\"OutsideData.csv\")\n",
    "        budget_rename_category = {\"Seed - farm\":\"Seed\"}\n",
    "        budget_rules[\"Category\"] = budget_rules[\"Category\"].replace(budget_rename_category)\n",
    "        ### separate locations into individual rows when they are separated with + in the rules df\n",
    "        budget_rules[\"Location\"] = budget_rules[\"Location\"].str.split(\"+\")\n",
    "        budget_rules = budget_rules.explode(\"Location\").reset_index(drop=True)\n",
    "        ### extract formula\n",
    "        budget_rules = budget_rules.melt(\n",
    "            id_vars=[\"Location\",\"Category\",\"AccFull\",\"SheetRef\"],\n",
    "            var_name=\"Month\",\n",
    "            value_name=\"Formula\"\n",
    "        )\n",
    "        budget_rules = budget_rules[~budget_rules[\"Location\"].isin([\"Outlook\", \"Arizona (produce)\", \"BritishColumbia (produce)\"])]  # produce copied data -> perfect alignment with Excel budget\n",
    "        budget_rules = budget_rules.fillna(value={\"Formula\":\"0\"})\n",
    "        budget_rules[\"Formula\"] = budget_rules[\"Formula\"].astype(str)\n",
    "        budget_rules[\"Formula\"] = budget_rules[\"Formula\"].replace({\"0\": \"*0\"})\n",
    "        ### calculating input budget for accounts per location\n",
    "        budget_rules_input = budget_rules[budget_rules[\"SheetRef\"] == \"Input Budget\"].copy(deep=True)\n",
    "        #### workaround input budget for Airdrie grain \n",
    "        input_budget2.loc[((input_budget2[\"Location\"]==\"Airdrie (grain)\")&(input_budget2[\"Type\"]==\"Acres\")),\"Type\"] = \"Custom work\"\n",
    "        ### merge budget rules with budget total per location\n",
    "        budget_input = pd.merge(budget_rules_input,input_budget2.rename(columns={\"Type\":\"Category\",\"Amount\":\"TotalAmountCAD\"}),on=[\"Location\",\"Category\"],how=\"left\")\n",
    "        #### revert back from workaround\n",
    "        input_budget2.loc[((input_budget2[\"Location\"]==\"Airdrie (grain)\")&(input_budget2[\"Type\"]==\"Custom work\")),\"Type\"] = \"Acres\"\n",
    "        ### apply the formula to compute per month\n",
    "        budget_input[\"AmountCAD\"] = budget_input.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"), axis=1)\n",
    "\n",
    "        ## production budget\n",
    "        ### combine Hay and Silage\n",
    "        production_budget[\"Commodity\"] = production_budget[\"Commodity\"].replace({\"Hay\":\"Hay/Silage\", \"Silage\":\"Hay/Silage\"})\n",
    "        ### add commodity column to budget rules\n",
    "        budget_rules_production = budget_rules[budget_rules[\"SheetRef\"] == \"Production Budget\"].copy(deep=True)\n",
    "        budget_rules_production[\"Commodity\"] = budget_rules_production.apply(lambda x: self._identify_product(x, for_budget=True), axis=1)\n",
    "        ### merge budget rules with budget totals\n",
    "        budget_production = pd.merge(budget_rules_production,production_budget.loc[:,[\"Location\",\"Commodity\",\"ForecastProductionCAD\"]].rename(columns={\"ForecastProductionCAD\":\"TotalAmountCAD\"}),\n",
    "                                         on = [\"Location\", \"Commodity\"], how=\"left\")\n",
    "        budget_production = budget_production.fillna(value={\"TotalAmountCAD\":0})\n",
    "        ### compute budget\n",
    "        budget_production[\"AmountCAD\"] = budget_production.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "\n",
    "        ## labour budget\n",
    "        budget_rules_labour = budget_rules[budget_rules[\"SheetRef\"] == \"Labour Budget\"].copy(deep=True)\n",
    "        budget_labour = pd.merge(budget_rules_labour, labour_budget.loc[:,[\"Location\",\"Month\",\"LabourBudgetCAD\"]].rename(columns={\"LabourBudgetCAD\":\"AmountCAD\"}),\n",
    "                                    on=[\"Location\",\"Month\"],how=\"left\")\n",
    "        \n",
    "        # ## produce budgets        !! using Copied Data for exact alignment\n",
    "        # ### BC\n",
    "        # budget_bc_produce = self._budget_process_produce(budget_rules=budget_rules,budget=bc_budget,sheetname=\"BC Produce Details\")\n",
    "        # ### AZ\n",
    "        # budget_az_produce = self._budget_process_produce(budget_rules=budget_rules,budget=az_budget,sheetname=\"AZ Details\")\n",
    "        # ### outlook\n",
    "        # budget_outlook = self._budget_process_produce(budget_rules=budget_rules,budget=outlook_budget.rename(columns={\"Amount\":\"AmountCAD\"}),sheetname=\"Outlook Details\")\n",
    "\n",
    "        ## JD lease\n",
    "        budget_rules_jd = budget_rules[budget_rules[\"SheetRef\"]==\"JD Lease\"].copy(deep=True)\n",
    "        budget_equipment = pd.merge(budget_rules_jd, jdlease.loc[:,[\"Location\",\"AllocatedCost25\"]].rename(columns={\"AllocatedCost25\":\"TotalAmountCAD\"}),\n",
    "                                        on = \"Location\", how = \"left\")\n",
    "        budget_equipment = budget_equipment.fillna(value={\"TotalAmountCAD\":0})\n",
    "        budget_equipment[\"AmountCAD\"] = budget_equipment.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "\n",
    "        ## adjustment for Swift Current\n",
    "        months = [\"April\", \"July\"]\n",
    "        for month in months:\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"].item()\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Fertilizer\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"].item()\n",
    "        months = [\"June\", \"September\"]\n",
    "        for month in months:\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"].item()\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Chemical\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"].item()\n",
    "        months = [\"May\", \"June\", \"September\"]\n",
    "        for month in months:\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"TotalAmountCAD\"].item()\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Swift Current\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"] += \\\n",
    "            budget_input.loc[((budget_input[\"Location\"]==\"Calderbank (grain)\")&(budget_input[\"Category\"]==\"Seed\")&(budget_input[\"Month\"]==month)),\"AmountCAD\"].item()\n",
    "        \n",
    "        # arithmetic rules\n",
    "        arithmetic = pd.read_csv(rule_path/\"Arithmetic.csv\")\n",
    "        ## faltten location\n",
    "        arithmetic[\"Location\"] = arithmetic[\"Location\"].str.split(\"+\")\n",
    "        arithmetic = arithmetic.explode(\"Location\").reset_index(drop=True)\n",
    "        arithmetic_rules = arithmetic.melt(\n",
    "            id_vars=[\"Location\",\"Category\",\"AccFull\", \"AccRef\", \"FixedRef\"],\n",
    "            var_name=\"Month\",\n",
    "            value_name=\"FormulaFull\"\n",
    "        )\n",
    "        arithmetic_rules = arithmetic_rules[~arithmetic_rules[\"Location\"].isin([\"Outlook\", \"Arizona (produce)\", \"BritishColumbia (produce)\"])]    # produce copied data -> perfect alignment with Excel budget\n",
    "        ## housekeeping\n",
    "        arithmetic_rules = arithmetic_rules.fillna(value={\"FormulaFull\":\"FY-1*0\"})\n",
    "        arithmetic_rules[\"FormulaFull\"] = arithmetic_rules[\"FormulaFull\"].astype(str)\n",
    "        arithmetic_rules[\"FormulaFull\"] = arithmetic_rules[\"FormulaFull\"].replace({\"0\":\"FY-1*0\"})\n",
    "        arithmetic_rules[\"ReferenceYear\"] = arithmetic_rules[\"FormulaFull\"].str.slice(0,4)\n",
    "        arithmetic_rules[\"Formula\"] = arithmetic_rules[\"FormulaFull\"].str.slice(4)\n",
    "        arithmetic_rules = arithmetic_rules.fillna(value={\"Formula\": \"0\"})\n",
    "        arithmetic_rules[\"Formula\"] = arithmetic_rules[\"Formula\"].astype(str)\n",
    "        arithmetic_rules[\"Formula\"] = arithmetic_rules[\"Formula\"].replace({\"0\":\"*0\"})\n",
    "        ## separating Fixed records\n",
    "        ### processing arithmetic_rules for billings - 1.ignore camp 4, 2. rename Fly Creek to Billings\n",
    "        ###     this is to avoid applying the arithmetic twice for billings, and all accounts except on are fly creek + camp 4 (identical calculation)\n",
    "        ###     the only exception is for amortization, camp 4 was fixed and fly creek was arithmetic - ignore the fixed cost for camp 4 for fixed as well\n",
    "        ### adjusted in excel sheet instead\n",
    "        # arithmetic_rules = arithmetic_rules[~((arithmetic_rules[\"Location\"]==\"Camp 4\") & (arithmetic_rules[\"Category\"]))]\n",
    "        # arithmetic_rules[\"Location\"] = arithmetic_rules[\"Location\"].replace(location_adj)\n",
    "        arithmetic_rules_fixed = arithmetic_rules[arithmetic_rules[\"AccRef\"] == \"Fixed\"].copy(deep=True)\n",
    "        arithmetic_rules = arithmetic_rules[arithmetic_rules[\"AccRef\"]!=\"Fixed\"].copy(deep=True)\n",
    "\n",
    "        ## process fixed records\n",
    "        arithmetic_rules_fixed = arithmetic_rules_fixed.drop(columns=[\"FormulaFull\",\"ReferenceYear\"]).rename(columns={\"FixedRef\":\"TotalAmountCAD\"})\n",
    "        arithmetic_rules_fixed[\"AmountCAD\"] = arithmetic_rules_fixed.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "\n",
    "        ## Extract Account Info\n",
    "        arithmetic_rules[\"AccNum\"] = arithmetic_rules[\"AccRef\"].apply(lambda x: \"\".join(x.split(\" \")[0:2]))\n",
    "        arithmetic_rules[\"AccName\"] = arithmetic_rules[\"AccRef\"].apply(lambda x: (\" \".join(x.split(\" \")[2:]).strip()))\n",
    "        assert \"Fixd\" not in arithmetic_rules.ReferenceYear.unique(), \"Fixd records incorrectly classified\"\n",
    "        ## separate FY-1 & FY+1\n",
    "        arithmetic_rules_prior = arithmetic_rules[arithmetic_rules[\"ReferenceYear\"] == \"FY-1\"].copy(deep=True)\n",
    "        arithmetic_rules = arithmetic_rules[arithmetic_rules[\"ReferenceYear\"] == \"FY+1\"].copy(deep=True)\n",
    "\n",
    "        ## process FY-1 with actuals - deleted AccName - assuming it's not being used for actuals\n",
    "        actuals = transactions.groupby([\"Location\", \"AccNum\", \"FiscalYear\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        arithmetic_rules_prior[\"FiscalYear\"] = self.currentFY - 1\n",
    "        duplicated = actuals[actuals.duplicated(subset=[\"AccNum\",\"FiscalYear\",\"Location\"],keep=False)]\n",
    "        assert len(duplicated) == 0, f\"Duplicated AccNum detected for FY-1 Actuals - {duplicated.AccNum.unique()}\"\n",
    "        budget_prior = pd.merge(arithmetic_rules_prior,actuals.rename(columns={\"AmountDisplay\":\"TotalAmountCAD\"}),\n",
    "                                on = [\"Location\",\"AccNum\",\"FiscalYear\"], how=\"left\")\n",
    "        budget_prior = budget_prior.fillna(value={\"TotalAmountCAD\": 0})\n",
    "        budget_prior[\"AmountCAD\"] = budget_prior.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"), axis=1)\n",
    "\n",
    "        ## processing FY+1 with current budget\n",
    "        ### budget sales that is based on production budget input sheet\n",
    "        arithmetic_rules_sales = arithmetic_rules[arithmetic_rules[\"Category\"].str.contains(\"cash settlements\")].copy(deep=True)\n",
    "        # production_reference = pd.concat([budget_production.copy(deep=True), budget_outlook.copy(deep=True), budget_az_produce.copy(deep=True),budget_bc_produce.copy(deep=True)])\n",
    "        production_reference = budget_production.copy(deep=True)   # excluding produce \n",
    "        production_reference = production_reference.groupby([\"Location\",\"AccFull\"]).agg({\"AmountCAD\":\"sum\"}).reset_index(drop=False)\n",
    "        budget_sales = pd.merge(arithmetic_rules_sales,production_reference.rename(columns={\"AccFull\":\"AccRef\"}), on=[\"Location\",\"AccRef\"], how=\"left\")\n",
    "        budget_sales = budget_sales.rename(columns={\"AmountCAD\":\"TotalAmountCAD\"})\n",
    "        budget_sales[\"AmountCAD\"] = budget_sales.apply(lambda x: eval(f\"{x[\"TotalAmountCAD\"]}{x[\"Formula\"]}\"),axis=1)\n",
    "        budget_prior = pd.concat([budget_prior, budget_sales],ignore_index=True)\n",
    "\n",
    "        ### budget inventory adjustment \n",
    "        arithmetic_rules_inventory = arithmetic_rules[arithmetic_rules[\"Category\"].str.contains(\"inventory adjustment\",case=False)].copy(deep=True)\n",
    "        budget_inventory = pd.merge(arithmetic_rules_inventory, budget_prior.loc[:,[\"Location\",\"AccFull\",\"Month\",\"AmountCAD\"]].rename(columns={\"AccFull\":\"AccRef\"}),\n",
    "                                on = [\"Location\",\"AccRef\", \"Month\"], how = \"left\")\n",
    "        budget_inventory[\"AmountCAD\"] = -budget_inventory[\"AmountCAD\"]\n",
    "        budget_prior = pd.concat([budget_prior, budget_inventory], ignore_index=True)\n",
    "\n",
    "        ## combine with fixed budgets\n",
    "        budget_prior = pd.concat([budget_prior,arithmetic_rules_fixed],ignore_index=True)\n",
    "\n",
    "        # copied data\n",
    "        budget_copy = pd.read_csv(copied_path/\"Copied Data.csv\")\n",
    "        copy_rename = {\"Outlook (grain)\": \"Outlook\", \"Outlook (produce)\":\"Outlook\"}\n",
    "        budget_copy[\"Location\"] = budget_copy[\"Location\"].replace(copy_rename)\n",
    "        budget_copy = budget_copy.melt(\n",
    "            id_vars=[\"Location\",\"Category\",\"AccFull\"],\n",
    "            var_name = \"Month\",\n",
    "            value_name = \"AmountCAD\"\n",
    "        )\n",
    "        budget_copy = budget_copy.fillna(value={\"AmountCAD\":0})\n",
    "        budget_copy[\"AmountCAD\"] = budget_copy[\"AmountCAD\"].astype(float)\n",
    "        budget_copy[\"FiscalYear\"] = self.currentFY\n",
    "        budget_copy[\"AccRef\"] = \"Copy\"\n",
    "        budget_copy[\"ReferenceYear\"] = \"NA\"\n",
    "        budget_copy[\"Formula\"] = \"NA\"\n",
    "        budget_copy[\"TotalAmountCAD\"] = budget_copy[\"AmountCAD\"]\n",
    "        budget_copy.loc[budget_copy[\"Location\"]==\"Seeds USA\", \"AmountCAD\"] *= self.fx\n",
    "        budget_copy.loc[budget_copy[\"Location\"]==\"Arizona (produce)\", \"AmountCAD\"] *= self.fx\n",
    "\n",
    "        # combining all budgets\n",
    "        # budget_outside = pd.concat([budget_input,budget_production,budget_labour,budget_equipment, budget_outlook, budget_az_produce, budget_bc_produce],ignore_index=True)\n",
    "        budget_outside = pd.concat([budget_input,budget_production,budget_labour,budget_equipment],ignore_index=True)   # produce budget inside copied data\n",
    "        budget_outside = budget_outside.drop(columns=[\"Commodity\"])\n",
    "        col_drop = [\"FormulaFull\",\"AccNum\",\"AccName_x\", \"AccName_y\", \"AccName\", \"FixedRef\"]\n",
    "        budget_prior = budget_prior.drop(columns=[x for x in col_drop if x in budget_prior.columns])\n",
    "        budget_all = pd.concat([budget_outside,budget_prior,budget_copy],ignore_index=True)\n",
    "        budget_all[\"AccNum\"] = budget_all[\"AccFull\"].apply(lambda x: \"\".join(x.split(\" \")[0:2]))\n",
    "        budget_all[\"AccName\"] = budget_all[\"AccFull\"].apply(lambda x: \" \".join(x.split(\" \")[2:]))\n",
    "        budget_all[\"FiscalYear\"] = self.currentFY \n",
    "        budget_all[\"DataType\"] = \"Budget\"\n",
    "        # budget_all.loc[budget_all[\"Category\"].str.contains(\"inventory adjustment\",case=False), \"AmountCAD\"] *= -1 # turn the sign positive for inventory adjustments (my classification only)\n",
    "\n",
    "        # enbale location to be modified, e.g., for Camp 4 + Fly Creek = Billings\n",
    "        budget_all[\"LocationRaw\"] = budget_all[\"Location\"]\n",
    "        budget_all[\"Location\"] = budget_all[\"Location\"].replace(location_adj)\n",
    "        # return budget_all\n",
    "\n",
    "        # save\n",
    "        self.check_file(self.gold_path[\"budget\"]/\"OutputFile\")\n",
    "        budget_all.to_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"budget_all.csv\", index=False)\n",
    "\n",
    "    def _budget_update(self, force_create:bool=True, force_process_input:bool=False) -> None:\n",
    "        \"\"\" \n",
    "            generate/update the actuals from the budget system\n",
    "        \"\"\"\n",
    "        print(\"\\nGenerating/Updating Actuals for budget system\\n\")\n",
    "        if self.is_dev:\n",
    "            self.operation_acc = pd.read_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\")\n",
    "            self.gold_pl = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "            self.fx = 1.3988\n",
    "        else:\n",
    "            if not self.pl_exist:\n",
    "                self._finance_operational()\n",
    "        budget_path = self.gold_path[\"budget\"]/\"OutputFile\"/\"budget_all.csv\"\n",
    "        if (not Path.exists(budget_path)) or force_create:\n",
    "            self._create_budget(process_input=force_process_input)\n",
    "        budget = pd.read_csv(budget_path)\n",
    "        budget = budget.loc[:,[\"Location\", \"SheetRef\", \"Month\", \"Formula\", \"TotalAmountCAD\", \"AmountCAD\", \"AccRef\", \"ReferenceYear\",\"FiscalYear\", \"AccNum\", \"DataType\", \"Category\"]]\n",
    "        budget_location_rename = {\"Airdrie (grain)\": \"Airdrie\", \"Airdrie (cattle)\": \"Airdrie\", \"Calderbank (cattle)\": \"Calderbank\",\n",
    "                                  \"Airdrie (corporate)\": \"Airdrie\", \"Seeds USA\":\"Arizona (produce)\"}\n",
    "        budget[\"Location\"] = budget[\"Location\"].replace(budget_location_rename)\n",
    "        # category_mapping = budget.loc[:,[\"AccNum\", \"Category\"]].drop_duplicates()     # problem with old changed AccNum mapped to incorrect Category\n",
    "        # organize Actuals\n",
    "        transactions = self._budget_get_transactions()\n",
    "        actuals_all = transactions.groupby([\"Location\",\"AccNum\", \"FiscalYear\", \"Month\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        actuals_all = actuals_all[actuals_all[\"FiscalYear\"] == self.currentFY]\n",
    "        actuals_all[\"DataType\"] = \"Actual\"\n",
    "        actuals_all = actuals_all.rename(columns={\"AmountDisplay\": \"AmountCAD\"})\n",
    "        # actuals_all = pd.merge(actuals_all,category_mapping,on=\"AccNum\",how=\"left\")   # problem with old changed AccNum mapped to incorrect Category\n",
    "        actuals_all.to_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"actuals_all.csv\", index=False)\n",
    "        print(f\"Location Unaccounted for in budget: {(set(budget.Location.unique()) - set(actuals_all.Location.unique()))}\")\n",
    "        # combine everything\n",
    "        all_all = pd.concat([budget,actuals_all],ignore_index=True)\n",
    "        all_all[\"FXRate\"] = self.fx\n",
    "        # reroute accounts for changing acc numbers\n",
    "        all_all[\"AccNum\"] = all_all[\"AccNum\"].replace(self.accnum_reroute)\n",
    "        # operational classification - AccNum to AccID mapping processed from _finance_operational() function\n",
    "        assert len(self.operation_acc[self.operation_acc.duplicated(subset=[\"AccNum\"],keep=False)]) == 0, \"Duplicated AccNum Detected - Operational Accounts Classification\"\n",
    "        self._extract_accnum_accid()\n",
    "        all_all[\"AccID\"] = all_all[\"AccNum\"].map(self.acc_map)\n",
    "        mismatch = all_all[all_all[\"AccID\"].isna()]\n",
    "        mismatch = mismatch[mismatch['AmountCAD']!=0]\n",
    "        print(f\"Total amount unaccounted for because of accnum mismatching - ${mismatch.AmountCAD.sum()}\")\n",
    "        print(f\"AccIDs with non-zero amount: {mismatch.AccNum.unique()}\")\n",
    "        # classify pillars\n",
    "        all_all[\"Pillar\"] = all_all.apply(lambda x: self._pillar_classification(x), axis=1)\n",
    "        # save\n",
    "        all_all.to_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"all_all.csv\", index=False)\n",
    "        self.check_file(self.gold_path[\"budget\"]/\"OutputPowerBI\")\n",
    "        # all_all.to_excel(self.gold_path[\"budget\"]/\"OutputPowerBI\"/\"BudgetActual.xlsx\", sheet_name=\"Budget\", index=False)\n",
    "        if not self.is_dev: \n",
    "            print(\"Saving ...\")\n",
    "            all_all.to_excel(self.gold_path[\"budget\"]/\"OutputPowerBI\"/\"BudgetActual.xlsx\", sheet_name=\"Budget\", index=False)\n",
    "            for pillar in [\"Grain\", \"Cattle\", \"Seed\", \"Produce\"]:\n",
    "                all_all[all_all[\"Pillar\"]==pillar].to_excel(self.gold_path[\"pillar_dashboard\"]/pillar/\"BudgetActual.xlsx\", sheet_name=\"Budget\", index=False)\n",
    "            \n",
    "    def _create_additional_financial(self, summary:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            this function create Gross Margin, Contribution Margin, EBITDA, Net Income financial terms in the summary table\n",
    "                summary table must be broken down to fiscal year, month, location\n",
    "        \"\"\"\n",
    "        complement_data = summary.head(0).copy(deep=True)\n",
    "        for y in summary.FiscalYear.unique():\n",
    "            subset_year = summary[summary[\"FiscalYear\"] == y]\n",
    "            for m in summary.Month.unique():\n",
    "                subset_month = subset_year[subset_year[\"Month\"] == m]\n",
    "                for l in summary.Location.unique():\n",
    "                    subset_location = subset_month[subset_month[\"Location\"] == l]\n",
    "                    if len(subset_location) == 0:\n",
    "                        continue\n",
    "                    items = [\"Sales Revenue\", \"Cost of Goods Sold\", \"Direct Operating Expenses\", \"Other Operating Revenue\", \"Operating Overheads\", \"Other Income\", \"Other Expense\"]\n",
    "                    values = dict.fromkeys(items, 0)\n",
    "                    for i in items:\n",
    "                        if i in subset_location.ProfitType.unique():\n",
    "                            values[i] = subset_location.loc[subset_location[\"ProfitType\"]==i, \"AmountDisplay\"].item()\n",
    "                    gross_margin = values[\"Sales Revenue\"] - values[\"Cost of Goods Sold\"]\n",
    "                    contribution_margin = gross_margin - values[\"Direct Operating Expenses\"] + values[\"Other Operating Revenue\"]\n",
    "                    ebitda = contribution_margin - values[\"Operating Overheads\"]\n",
    "                    net_income = ebitda + values[\"Other Income\"] - values[\"Other Expense\"]\n",
    "                    pillar = subset_location.Pillar.unique().item()\n",
    "                    row = {\"FiscalYear\": y, \"Month\": m, \"Location\": l, \"Pillar\":pillar}\n",
    "                    row_GM = row | {\"ProfitType\": \"Gross Margin\", \"AmountDisplay\": gross_margin}\n",
    "                    row_CM = row | {\"ProfitType\": \"Contribution Margin\", \"AmountDisplay\": contribution_margin}\n",
    "                    row_ebitda = row | {\"ProfitType\": \"EBITDA\", \"AmountDisplay\": ebitda}\n",
    "                    row_NI = row | {\"ProfitType\": \"Net Income\", \"AmountDisplay\": net_income}\n",
    "                    complement_data.loc[len(complement_data)] = row_GM\n",
    "                    complement_data.loc[len(complement_data)] = row_CM \n",
    "                    complement_data.loc[len(complement_data)] = row_ebitda \n",
    "                    complement_data.loc[len(complement_data)] = row_NI \n",
    "        return complement_data\n",
    "\n",
    "    def _finance_summary(self, create_allocation_reference = True) -> None:\n",
    "        \"\"\" \n",
    "            this function assemble summary tables for financial income statement style, including Gross Margin, EBITDA, Net Income, \n",
    "                compared to (with % change compared to last year and budget)\n",
    "                    1. Last Year\n",
    "                    2. Budget\n",
    "                    3. month-by-month\n",
    "                includes Units (e.g., Acres)\n",
    "        \"\"\"\n",
    "        if self.is_dev:\n",
    "            self.operation_acc = pd.read_csv(self.gold_path[\"finance_operational\"]/\"AccNumTOAccID.csv\")\n",
    "            self.gold_pl = pd.read_csv(self.gold_path[\"finance_operational\"]/\"PL.csv\")\n",
    "            self.fx = 1.3844\n",
    "        else:\n",
    "            if not self.pl_exist:\n",
    "                self._finance_operational()\n",
    "        # prepare dfs \n",
    "        data = self.gold_pl[self.gold_pl[\"FiscalYear\"] >=  2024].copy(deep=True)\n",
    "        account = self.gold_acc[self.gold_acc[\"AccountingType\"] == \"Income Statement\"]\n",
    "        # create AccID -> ProfitType mapping\n",
    "        id_prof_map = account.set_index(\"AccID\")[\"ProfitType\"]\n",
    "        data[\"ProfitType\"] = data[\"AccID\"].map(id_prof_map)\n",
    "        # summary by location, by pillar, by ProfitType, by Fiscal Year, by Month\n",
    "        summary = data.groupby([\"FiscalYear\", \"Month\", \"Location\", \"Pillar\", \"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summary) == len(data.groupby([\"FiscalYear\", \"Month\", \"Location\", \"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"})), \"duplicated location-pillar detected\"\n",
    "        # prepare df for additional financial lines, e.g., Gross Margin, Net Income, ...\n",
    "        complement_data = self._create_additional_financial(summary)\n",
    "        # concat two dfs\n",
    "        summary = pd.concat([summary, complement_data],ignore_index=True)\n",
    "        # assign datatype before budget summary\n",
    "        summary[\"DataType\"] = \"Actual\"\n",
    "        # read processed budget transactions\n",
    "        budget = pd.read_csv(self.gold_path[\"budget\"]/\"OutputFile\"/\"all_all.csv\")\n",
    "        budget = budget[budget[\"DataType\"] == \"Budget\"]\n",
    "        budget = budget.loc[:,[\"Location\", \"Month\", \"FiscalYear\", \"AmountCAD\", \"DataType\", \"AccID\", \"Pillar\",\"AccNum\"]]\n",
    "        budget = budget.rename(columns={\"AmountCAD\":\"AmountDisplay\"})\n",
    "        budget = budget[~((budget[\"AccID\"].isna())&(budget[\"AmountDisplay\"] == 0))].reset_index(drop=True)\n",
    "        assert len(budget[budget[\"AccID\"].isna()]) == 0, f\"Unaccounted accounts - {budget[budget[\"AccID\"].isna()].AccNum.unique()}\"\n",
    "        budget = budget.drop(columns=[\"AccNum\"])\n",
    "        # map ProfitType\n",
    "        budget[\"ProfitType\"] = budget[\"AccID\"].map(id_prof_map)\n",
    "        # budget summary\n",
    "        summary_budget = budget.groupby([\"FiscalYear\",\"Month\",\"Location\",\"Pillar\",\"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"}).reset_index(drop=False)\n",
    "        assert len(summary_budget) == len(budget.groupby([\"FiscalYear\",\"Month\",\"Location\",\"ProfitType\"]).agg({\"AmountDisplay\":\"sum\"})), \"repeat Pillar detected when summarizing budget\"\n",
    "        # additional financial terms\n",
    "        complement_data_budget = self._create_additional_financial(summary_budget)\n",
    "        # final processing\n",
    "        summary_budget = pd.concat([summary_budget,complement_data_budget],ignore_index=True)\n",
    "        summary_budget[\"DataType\"] = \"Budget\"\n",
    "        print(f\"Location missing from budget - {set(summary.Location.unique()) - set(summary_budget.Location.unique())}\")\n",
    "        print(f\"Location missing from actual - {set(summary_budget.Location.unique()) - set(summary.Location.unique())}\")\n",
    "        # save\n",
    "        summary_all = pd.concat([summary, summary_budget],ignore_index=True)\n",
    "        summary_all.to_csv(self.gold_path[\"finance_operational\"]/\"ProfitTypeSummary.csv\", index=False) # for reclassifying accounts\n",
    "        summary_all.to_excel(self.gold_path[\"finance_operational\"]/\"ProfitTypeSummary.xlsx\", sheet_name=\"ProfitTypeSummary\", index=False)\n",
    "        for pillar in [\"Grain\", \"Cattle\", \"Seed\", \"Produce\"]:\n",
    "            summary_all[summary_all[\"Pillar\"]==pillar].to_excel(self.gold_path[\"pillar_dashboard\"]/pillar/\"ProfitTypeSummary.xlsx\", sheet_name=\"ProfitTypeSummary\", index=False)\n",
    "\n",
    "    def _APAR_concat_memo(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "            This function concatenates all 'TransactionEntered' column per TransactionID_partial\n",
    "        \"\"\"\n",
    "        df_map = df.loc[:,[\"TransactionID_partial\", \"TransactionEntered\", \"Line_Id\", \"PrivateNote\", \"TransactionType\"]]\n",
    "        # fill missing TransactionEntered as Missing so it is string and can be concatenated\n",
    "        df_map = df_map.fillna(value={\"TransactionEntered\":\"Missing\", \"PrivateNote\": \"Missing\"})\n",
    "        # concatenate the line number into TransactionEntered so it's not too messy when concatenate TransactionEntered for multiple lines\n",
    "        df_map[\"TransactionEntered\"] = df_map[\"Line_Id\"].astype(str) + \". \" + df_map[\"TransactionEntered\"]\n",
    "        # concatenate -> one TransactionEntered per TransactionID_partial\n",
    "        df_map2 = df_map.sort_values(by=[\"TransactionID_partial\", \"Line_Id\"],ignore_index=True)\\\n",
    "                        .groupby([\"TransactionID_partial\",\"PrivateNote\",\"TransactionType\"])[\"TransactionEntered\"].agg(\" \".join).reset_index(drop=False)\n",
    "        assert len(df_map2[df_map2.duplicated(subset=\"TransactionID_partial\")]) == 0, \"duplicated transactionID spotted when creating TransactionEntered Concatenation\"\n",
    "        return df_map2\n",
    "    \n",
    "    def _APRporting_project(self, date:set[int] = None) -> None:\n",
    "        \"\"\" \n",
    "            This function connects APAgingDetails report from QBO API, and combined with raw tables such as Bill to form a comprehensive report that meets finance team's need\n",
    "                This function supports date input for processing the report at that exact date\n",
    "        \"\"\"\n",
    "        print(\"\\nProcessing AP Rerpot\\n\")\n",
    "        if date is not None:\n",
    "            assert len(date) == 3, f\"please pass the date as (YYYY, M, D), passed {date}\"\n",
    "            year, month, day = date\n",
    "        else:\n",
    "            year, month, day = self.today.year, self.today.month, self.today.day\n",
    "        # read APAging report from silver space\n",
    "        try:\n",
    "            report = pd.read_csv(self.silver_path[\"QBO\"][\"APAR\"] / \"AgedPayableDetail\" / str(year) / str(month) / f\"{day}.csv\")\n",
    "        except:\n",
    "            print(f'csv file not found at {self.silver_path[\"QBO\"][\"APAR\"] / \"AgedPayableDetail\" / str(year) / str(month) / f\"{day}.csv\"}')\n",
    "        report = report.rename(columns={\"TransactionTypeID\":\"TransactionID_partial\"})\n",
    "        report_transactiontype_rename = {\"Bill Payment (Cheque)\":\"BillPaymentCheck\", \"Bill Payment (Credit Card)\":\"BillPaymentCheck\", \"Bill Payment (Check)\":\"BillPaymentCheck\",\n",
    "                                        \"Cheque Expense\":\"Purchase\", \"Supplier Credit\": \"Vendor Credit\"}\n",
    "        # standardize the Transaction Types\n",
    "        report[\"TransactionType\"] = report[\"TransactionType\"].replace(report_transactiontype_rename)\n",
    "        # create mapping table from facts\n",
    "        cols = [\"TransactionDate\", \"TotalAmt\", \"PrivateNote\", \"APAccID\", \"DocNumber\", \"TransactionEntered\", \"Amount\", \"TransactionID\", \"VendorID\", \"FarmID\", \"TransactionID_partial\", \"Line_Id\",\n",
    "                \"AccID\"]\n",
    "        ## get bill table ready for mapping - only taking bill transactions that are relevant to AP report\n",
    "        bill_col = cols + [\"TermID\"]\n",
    "        bill = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"]/\"Bill.csv\",usecols=bill_col)\n",
    "        bill = bill[bill[\"TransactionID_partial\"].isin(report[report[\"TransactionType\"]==\"Bill\"][\"TransactionID_partial\"].unique())]\n",
    "        bill[\"TransactionType\"] = \"Bill\"\n",
    "        bill_map = self._APAR_concat_memo(bill) # results should only have 4 columns:  TransactionID_partial, PrivateNote, TransactionType, TransactionEntered (concatenated)\n",
    "        ## vendorcredit\n",
    "        vc = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"VendorCredit.csv\", usecols=cols)\n",
    "        vc = vc[vc[\"TransactionID_partial\"].isin(report[report[\"TransactionType\"]==\"Vendor Credit\"][\"TransactionID_partial\"].unique())]\n",
    "        vc[\"TransactionType\"] = \"Vendor Credit\"\n",
    "        vc_map = self._APAR_concat_memo(vc)\n",
    "        ## journal entry\n",
    "        journal_cols = cols + [\"JEType\"]\n",
    "        journal = pd.read_csv(self.silver_path[\"QBO\"][\"Raw\"] / \"JournalEntry.csv\", usecols=[x for x in journal_cols if x not in ['TotalAmt', 'APAccID']])\n",
    "        journal = journal[journal[\"TransactionID_partial\"].isin(report[report[\"TransactionType\"]==\"Journal Entry\"][\"TransactionID_partial\"].unique())]\n",
    "        journal[\"TransactionType\"] = \"Journal Entry\"\n",
    "        journal_map = self._APAR_concat_memo(journal)\n",
    "        # merging and save facts\n",
    "        fact = pd.concat([bill, vc, journal], ignore_index=True)\n",
    "        fact.to_excel(self.gold_path[\"APReporting\"]/\"Facts.xlsx\", index=False, sheet_name = \"Facts\")\n",
    "        # use map table to concate PrivateNote and TransactionEntered into AP Report\n",
    "        fact_map = pd.concat([bill_map, vc_map, journal_map],ignore_index=True)\n",
    "        report2 = pd.merge(report, fact_map, on=[\"TransactionID_partial\", \"TransactionType\"], how=\"left\")\n",
    "        assert len(report) == len(report2), \"After concatenating the TransactionEntered column, duplicated transactionID detected\"\n",
    "        report2.to_excel(self.gold_path[\"APReporting\"]/\"APReport.xlsx\", index=False, sheet_name=\"APReport\")\n",
    "        # save the Vendor and Location table\n",
    "        vendor = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension\"]/\"CSV\"/\"Vendor.csv\")\n",
    "        vendor.to_excel(self.gold_path[\"APReporting\"].parent / \"Vendor.xlsx\", sheet_name = \"Vendor\", index=False)\n",
    "        farm = pd.read_csv(self.silver_path[\"QBO\"][\"Dimension\"]/\"CSV\"/\"Farm.csv\")\n",
    "        farm.to_excel(self.gold_path[\"APReporting\"].parent / \"Location.xlsx\", sheet_name = \"Location\", index=False)\n",
    "        print(\"AP Report Done \\n\")\n",
    "\n",
    "    def _HP_transformation(self) -> None:\n",
    "        \"\"\" \n",
    "            This function transforms the raw data from Harvest Profit in the csv format from Silver space to curated data in Gold space for Power BI\n",
    "        \"\"\"\n",
    "        print(\"\\nTransforming Harvest Profit Data\\n\")\n",
    "        # file paths\n",
    "        silver_path_Delivery_HP = self.silver_path[\"Delivery\"][\"HP\"]\n",
    "        gold_path_inventory = self.gold_path[\"inventory\"]\n",
    "        gold_HP = gold_path_inventory/\"Grain\"\n",
    "        product_path = gold_path_inventory / \"Tables\" / \"commodities_acc.csv\"\n",
    "        folder_fields = silver_path_Delivery_HP / \"Fields\" / \"2025\"\n",
    "        customers_rename = {\"Viterra Kamsack\":\"Bunge Kamsack\"}\n",
    "        # read raw data\n",
    "        df = pd.read_csv(silver_path_Delivery_HP/f\"Loads_{self.today.year}_{self.today.month}.csv\")\n",
    "        # standardizing df\n",
    "        df_rename = {\"crop\": \"ProductRaw\", \"amount\":\"AmountRaw\", \"accepted_amount\":\"AcceptedAmountRaw\", \"amount_unit\":\"AmountRawUnit\", \"date\":\"TransactionDate\", \"entity_share\":\"LocationRaw\",\n",
    "                    \"contract\":\"ContractRaw\"}\n",
    "        df = df.rename(columns=df_rename)\n",
    "        df[\"TransactionDate\"] = pd.to_datetime(df[\"TransactionDate\"], format=\"%m/%d/%Y %I:%M %p\")\n",
    "        df[\"TransactionDate\"] = df[\"TransactionDate\"].dt.date\n",
    "        # standardize location\n",
    "        df_location_map = {\"MFB Billings - US\": \"Billings\", \"MFS Swift Current - CA\": \"SwiftCurrent\", \"MFH Hafford - CA\": \"Hafford\", \"MFK Kamsack - CA\":\"Kamsack\",\n",
    "                   \"MFR Regina - CA\": \"Regina\", \"MFPAS The Pas - CA\": \"ThePas\", \"MFPA  Prince Albert - CA\":\"PA\", \"MFRAY Raymore - CA\":\"Raymore\", \n",
    "                   \"MFE Eddystone - CA\": \"Eddystone\", \"MFO Outlook - CA\": \"Outlook\", \"MFAIR Airdrie - CA\": \"Airdrie\", \"Nexgen Seeds\": \"NexGen\"}\n",
    "        df[\"Location\"] = df[\"LocationRaw\"].replace(df_location_map)\n",
    "        # preliminary transformation\n",
    "        df = df[df[\"AmountRaw\"]!=0]\n",
    "        df[\"from\"] = df[\"from\"].str.strip()\n",
    "        df[\"to\"] = df[\"to\"].str.strip()\n",
    "        df.loc[((df[\"Location\"]==\"Kamsack\")&(df[\"to\"]==\"Viterra Kamsack\")), \"to\"] = \"Bunge Kamsack\"\n",
    "        df.loc[((df[\"Location\"]==\"Hafford\")&(df[\"to\"]==\"D.C.- P&H North Battleford\")), \"to\"] = \"P&H North Battleford\"\n",
    "        df.loc[((df[\"Location\"]==\"Hafford\")&(df[\"to\"]==\"D.C Bunge Wheat Truck Bin\")), \"to\"] = \"Bunge Hafford\"\n",
    "        df.loc[((df[\"Location\"]==\"Hafford\")&(df[\"to\"]==\"D.C.- Maymont GrainCorp Wheat - Truck Bin\")), \"to\"] = \"Maymont Hafford\"\n",
    "        # compute MT\n",
    "        ## convert Bu to MT for every location excpet Billings where they might have legit Bu measures\n",
    "        # df.loc[((df[\"Location\"]!=\"Billings\")&(df[\"AmountRawUnit\"]==\"Bu\")&(df[\"harvest_profit_id\"]!=1010153)), \"AmountRawUnit\"] = \"Mt\"\n",
    "        bushels = df[df[\"AmountRawUnit\"] == \"Bu\"].copy(deep=True)\n",
    "        ## when units are not Bu\n",
    "        df = df[df[\"AmountRawUnit\"] != \"Bu\"].copy(deep=True)\n",
    "        ## create unit mapping table\n",
    "        units = [\"Lbs\", \"Kg\", \"Mt\"]\n",
    "        divisor = [self.conversion_mt_to_lb, 1000, 1]\n",
    "        mapping_table = pd.DataFrame(data={\"AmountRawUnit\": units, \"divisor\":divisor})\n",
    "        mapping_table = mapping_table.set_index(\"AmountRawUnit\")[\"divisor\"]\n",
    "        ## map mapping table and perform arithmetics\n",
    "        df[\"divisor\"] = df[\"AmountRawUnit\"].map(mapping_table)\n",
    "        df[\"AcceptedAmount\"] = df[\"AcceptedAmountRaw\"] / df[\"divisor\"]\n",
    "        df[\"Amount\"] = df[\"AmountRaw\"] / df[\"divisor\"]\n",
    "        df = df.drop(columns=[\"divisor\"])\n",
    "        ## when units are bushels\n",
    "        if len(bushels) >= 1:\n",
    "            bushels[\"Amount\"] = bushels.apply(lambda x: x[\"AmountRaw\"] * self.conversion_bu_to_lb_canola / self.conversion_mt_to_lb if \"canola\" in x[\"ProductRaw\"].lower() \n",
    "                                            else x[\"AmountRaw\"] * self.conversion_bu_to_lb_others / self.conversion_mt_to_lb, axis=1)\n",
    "            bushels[\"AcceptedAmount\"] = bushels.apply(lambda x: x[\"AcceptedAmountRaw\"] * self.conversion_bu_to_lb_canola / self.conversion_mt_to_lb if \"canola\" in x[\"ProductRaw\"].lower() \n",
    "                                                    else x[\"AcceptedAmountRaw\"] * self.conversion_bu_to_lb_others / self.conversion_mt_to_lb, axis=1)\n",
    "            df = pd.concat([df, bushels], ignore_index=True)\n",
    "        # standardizing product - matching with QBO except Uncategorized Lentil\n",
    "        def _create_hp_product_mapping(products:list[str]) -> dict[str,str]:\n",
    "            \"\"\" \n",
    "                This function create mapping ProductRaw -> Product for Harvest Profit, only process strings, ignores missing values\n",
    "            \"\"\"\n",
    "            mapping = {}\n",
    "            for p in products:\n",
    "                if isinstance(p, str):\n",
    "                    p_lower = p.lower()\n",
    "                    if 'green lentil' in p_lower:\n",
    "                        mapping[p] = \"Green Lentil\"\n",
    "                    elif 'red lentil' in p_lower:\n",
    "                        mapping[p] = \"Red Lentil\"\n",
    "                    elif 'lentil' in p_lower:\n",
    "                        mapping[p] = \"Unclassified Lentil\"\n",
    "                    elif 'barley' in p_lower:\n",
    "                        mapping[p] = \"Barley\"\n",
    "                    elif 'canola' in p_lower:\n",
    "                        mapping[p] = \"Canola\"\n",
    "                    elif 'durum' in p_lower:\n",
    "                        mapping[p] = \"Durum\"\n",
    "                    elif 'winter' in p_lower and 'wheat' in p_lower:\n",
    "                        mapping[p] = \"Winter Wheat\"\n",
    "                    elif 'wheat' in p_lower:\n",
    "                        mapping[p] = \"Wheat\"\n",
    "                    elif 'chickpea' in p_lower:\n",
    "                        mapping[p] = \"Chickpea\"\n",
    "                    elif 'pea' in p_lower:\n",
    "                        mapping[p] = \"Field Pea\"\n",
    "                    elif 'carrots' in p_lower:\n",
    "                        mapping[p] = \"Carrots\"\n",
    "                    else:\n",
    "                        mapping[p] = \"Unrecognized\"\n",
    "            return mapping\n",
    "        product_mapping = _create_hp_product_mapping(df.ProductRaw.unique())\n",
    "        df[\"Product\"] = df[\"ProductRaw\"].map(product_mapping)\n",
    "        ## for FY 2025 - Regina - Green Lentil, fly creek & Raymore - Red Lentil, unspecified in HP - adjust\n",
    "        df.loc[((df[\"Product\"] == \"Unclassified Lentil\")&(df[\"Location\"] == \"Regina\")), \"Product\"] = \"Green Lentil\"\n",
    "        df.loc[((df[\"Product\"] == \"Unclassified Lentil\")&(df[\"Location\"] == \"Billings\")), \"Product\"] = \"Red Lentil\"\n",
    "        df.loc[((df[\"Product\"] == \"Unclassified Lentil\")&(df[\"Location\"] == \"Raymore\")), \"Product\"] = \"Red Lentil\"\n",
    "\n",
    "        \n",
    "        # determine transfer mode\n",
    "        \n",
    "        ## use Location + From/To name to match fields_df \n",
    "        df[\"FromExtended\"] = df[\"Location\"] + \"-\" + df[\"from\"].str.strip()\n",
    "        df[\"FromExtended\"] = df[\"FromExtended\"].str.split(\",\").str[0]   # only look at first portion of multiple inputs to determine whether it's harvest records\n",
    "        df[\"ToExtended\"] = df[\"Location\"] + \"-\" + df[\"to\"].str.strip()\n",
    "        \n",
    "        ## create fields df\n",
    "        files = os.listdir(folder_fields)\n",
    "        fields_df = pd.DataFrame()\n",
    "        for f in files:\n",
    "            location = f.split(\".\")[0].split(\"_\")[-1]   # extract the last part (location name) after _ separater before .xlsx suffix\n",
    "            temp = pd.read_excel(folder_fields/f,dtype={\"Field\": str, \"Acres\":float})\n",
    "            temp[\"Location\"] = location\n",
    "            fields_df = pd.concat([fields_df, temp],ignore_index=True)\n",
    "        fields_nexgen = fields_df[fields_df[\"Location\"]==\"SwiftCurrent\"].copy(deep=True)\n",
    "        fields_nexgen[\"Location\"] = \"NexGen\"\n",
    "        fields_df = pd.concat([fields_df, fields_nexgen], ignore_index=True)\n",
    "        ### Location + Field to match with HP data\n",
    "        fields_df[\"FieldRaw\"] = fields_df[\"Field\"]\n",
    "        fields_df[\"Field\"] = fields_df[\"Location\"] + \"-\" + fields_df[\"FieldRaw\"]\n",
    "        fields_df.to_csv(gold_HP / \"HPFields.csv\", index=False)\n",
    "        \n",
    "        ## determine whether a load is from field\n",
    "        ### determine if a load is harvest from a field\n",
    "        df[\"FromField\"] = df[\"FromExtended\"].isin(fields_df[\"Field\"].unique())\n",
    "        df[\"ToField\"] = df[\"ToExtended\"].isin(fields_df[\"Field\"].unique())\n",
    "        \n",
    "        ## determine whether a load is from/to bins and identify direct deliveries\n",
    "        ### read bins records\n",
    "        bins_df = pd.read_csv(gold_HP / \"HPBins.csv\", dtype={\"Bins\":str, \"Location\":str})\n",
    "        bins_nexgen = bins_df[bins_df[\"Location\"]==\"SwiftCurrent\"].copy(deep=True)\n",
    "        bins_nexgen[\"Location\"] = \"NexGen\"\n",
    "        bins_df = pd.concat([bins_df, bins_nexgen],ignore_index=True)\n",
    "        # bins_df[\"IsDirect\"] = bins_df[\"Bins\"].str.contains(r\"^(?!.*(?:Cart|Bin)).*(?:(?i:direct)|FTC|D\\.C\\.)\", regex=True)\n",
    "        bins_df[\"IsDirect\"] = False\n",
    "        bins_df[\"Bins\"] = bins_df[\"Location\"] + \"-\" + bins_df[\"Bins\"]\n",
    "        ### determine load from/to bins\n",
    "        df[\"FromBins\"] = df[\"FromExtended\"].isin(bins_df[~bins_df[\"IsDirect\"]][\"Bins\"].unique())\n",
    "        df[\"ToBins\"] = df[\"ToExtended\"].isin(bins_df[~bins_df[\"IsDirect\"]][\"Bins\"].unique())\n",
    "        ## transfer_mode - applicable for valid records only\n",
    "        ### transfer_mode = Harvest -> FromField==True & ToBins==True\n",
    "        df.loc[(df[\"FromField\"]&df[\"ToBins\"]), \"TransferMode\"] = \"Harvest\"\n",
    "        ### transfer_mode = Sales -> FromBins==True & ToBins==False & ToField==False\n",
    "        df.loc[((df[\"FromBins\"])&(~df[\"ToBins\"])&(~df[\"ToField\"])), \"TransferMode\"] = \"Sales\"\n",
    "        ### transfer_mode = BinTransfer -> FromBins==True & ToBins==True\n",
    "        df.loc[(df[\"FromBins\"]&df[\"ToBins\"]), \"TransferMode\"] = \"BinTransfer\"\n",
    "        ## identify direct deliveries\n",
    "        # mask = (\n",
    "        #     df[\"FromExtended\"].isin(bins_df[bins_df[\"IsDirect\"]].Bins.unique()) | \n",
    "        #     df[\"ToExtended\"].isin(bins_df[bins_df[\"IsDirect\"]].Bins.unique())\n",
    "\n",
    "        # )\n",
    "        # df[\"IsDirect\"] = mask \n",
    "        df[\"IsDirect\"] = False\n",
    "        ### in addition, if FromField==True & to is in customers list, this is also direct delivery\n",
    "        df.loc[((df[\"FromField\"])&(df[\"to\"].isin(self.customers_list))), \"IsDirect\"] = True\n",
    "        ### transfer_mode = Sales -> Direct Delivery\n",
    "        df.loc[df[\"IsDirect\"], \"TransferMode\"] = \"Sales\"\n",
    "        \n",
    "        ## invalid loads\n",
    "        df = df.fillna(value={\"TransferMode\": \"Invalid\"})\n",
    "        df.loc[df[\"TransferMode\"]!=\"Invalid\", \"Flag\"] = \"Valid Loads\"\n",
    "        ### flag invalid loads\n",
    "        #### when both from & to is missing, flag those records\n",
    "        df.loc[((df[\"from\"].isna())&(df[\"to\"].isna())), \"Flag\"] = \"[from] and [to] Unassigned\"\n",
    "        #### when from is missing and to is not a field or a bin, assume those records are to a customer, without stating which inventory it is from\n",
    "        df.loc[((df[\"TransferMode\"]==\"Invalid\")&(df[\"from\"].isna())&(~df[\"ToBins\"])&(df[\"to\"].notna())), \"Flag\"] = \"Deliveries with Unknown Origin (Bins)\"\n",
    "        #### when the transfer mode is sales, not direct delivery, is from bins, and to is missing, flag those loads as Delivery missing Customer\n",
    "        df.loc[((df[\"TransferMode\"]==\"Sales\")&(~df[\"IsDirect\"])&(df[\"FromBins\"])&(df[\"to\"].isna())), \"Flag\"] = \"Deliveries with Unknown Destination (Customer)\"\n",
    "        #### when the load is from field (harvest) and to is missing, flag those loads\n",
    "        df.loc[((df[\"FromField\"])&(df[\"to\"].isna())), \"Flag\"] = \"Harvest with Unknown Destination (Bins)\"\n",
    "        #### when to is a bin (most likely from harvest), and from is missing\n",
    "        df.loc[((df[\"TransferMode\"]==\"Invalid\")&(df[\"from\"].isna())&(df[\"ToBins\"])), \"Flag\"] = \"Harvest with Unknown Origin (Field)\"\n",
    "        exception = df[df[\"Flag\"].isna()]\n",
    "        if len(exception) != 0:\n",
    "            print(f\"\\nThere are {len(exception)} number of exceptions unaccounted for in \\n{exception.Location.value_counts()}\\n\")\n",
    "        df = df.fillna({\"Flag\": \"Unknown - Investigation Required\"})\n",
    "\n",
    "        \n",
    "        # create LoadType label to include invalid loads\n",
    "        df.loc[(df[\"IsDirect\"]), \"LoadType\"] = \"Direct Delivery\"\n",
    "        df.loc[(df[\"FromField\"] | df[\"ToBins\"]) & (~df[\"IsDirect\"]), \"LoadType\"] = \"Harvest\"\n",
    "        df.loc[((~(df[\"FromField\"] | df[\"ToBins\"])) & (~df[\"IsDirect\"])), \"LoadType\"] = \"Delivery from Inventory\"\n",
    "        df.loc[df[\"Flag\"]==\"[from] and [to] Unassigned\", \"LoadType\"] = \"Undefined Loads\"\n",
    "        # df.loc[((~(df[\"FromField\"] | df[\"ToBins\"])) & (~df[\"IsDirect\"])&(df[\"to\"].notna())), \"LoadType\"] = \"Delivery from Inventory\"\n",
    "        # df.loc[((~(df[\"FromField\"] | df[\"ToBins\"])) & (~df[\"IsDirect\"])&(df[\"to\"].isna())), \"LoadType\"] = \"Undefined Loads\"\n",
    "        df.loc[((~df[\"FromBins\"])&(~df[\"FromField\"])&(df[\"from\"].notna())&(~df[\"IsDirect\"])), \"LoadType\"] = \\\n",
    "            df.loc[((~df[\"FromBins\"])&(~df[\"FromField\"])&(df[\"from\"].notna())&(~df[\"IsDirect\"])), \"TransferMode\"] = \"Not Load\"\n",
    "        df.loc[df[\"TransferMode\"]==\"BinTransfer\", \"LoadType\"] = \"BinTransfer\"\n",
    "\n",
    "        # bin inventory - create a column to store bin no matter if it is from or to a bin\n",
    "        direct_others = df[df[\"LoadType\"].isin([\"Direct Delivery\",\"Undefined Loads\",\"Not Load\",\"BinTransfer\"])].copy(deep=True)\n",
    "        harvest = df[df[\"LoadType\"]==\"Harvest\"].copy(deep=True)\n",
    "        harvest[\"Bin\"] = harvest.loc[harvest[\"ToBins\"], \"to\"]\n",
    "        others = df[df[\"LoadType\"]==\"Delivery from Inventory\"].copy(deep=True)\n",
    "        others[\"Bin\"] = others[\"from\"]\n",
    "        df = pd.concat([harvest, others, direct_others], ignore_index=True)\n",
    "\n",
    "        # create MT monitoring mechanism\n",
    "        ranges = [-np.inf, 0, 10, 36, 48, 95, 10000, np.inf]\n",
    "        ranges_indexer = pd.IntervalIndex.from_arrays(left=ranges[:-1], right=ranges[1:], closed=\"left\")\n",
    "        range_names = np.array([\"Error:<0\",\"Invalid-Small:<10\", \"Unusual-Small:10-36\", \"Valid:36-48\", \"Invalid-Large:48-95\", \"Invalid-Large:>95\", \"Error:>10,000\"], dtype=str)\n",
    "        amounts_category = ranges_indexer.get_indexer(df[\"AcceptedAmount\"])\n",
    "        df[\"AmountCategory\"] = range_names[amounts_category]\n",
    "        ## for The Pas and Billings, 48 - 95 is valid\n",
    "        df.loc[(df[\"Location\"].isin([\"Billings\",\"ThePas\"])& (df[\"AmountCategory\"]==\"Invalid-Large:48-95\")), \"AmountCategory\"] = \"Valid:48-95\"\n",
    "\n",
    "        # handle bin transfer math\n",
    "        ## create a copy of bin transfer, one copy will be w.r.t 'to' bins, increase amount, another copy will be w.r.t 'from' bins, decrease amount\n",
    "        mask = df[\"LoadType\"].eq(\"BinTransfer\")\n",
    "        total_amount = df[~mask].AcceptedAmount.sum()\n",
    "        df3 = df[mask].copy(deep=True)\n",
    "        ## original df - bin transfer - is for capturing the change in bins for column 'to' - i.e., positive amount - went into this bin\n",
    "        df.loc[mask, \"Bin\"] = df.loc[mask,\"to\"]\n",
    "        ## df3 is for capturing the change in bins for column 'from' - i.e., negative amount - went out of this bin\n",
    "        df3[\"Bin\"] = df3[\"from\"]\n",
    "        for col in [\"AmountRaw\",\"AcceptedAmountRaw\",\"Amount\",\"AcceptedAmount\"]:\n",
    "            df3[col] = -df3[col]\n",
    "        df = pd.concat([df, df3], ignore_index=True)\n",
    "        total_amount2 = df.AcceptedAmount.sum()\n",
    "        assert np.abs(total_amount-total_amount2) < 0.001, f\"after handling bin transfer, the total amount changed from {total_amount} to {total_amount2} - with non-zero netted bin transfers amount\"\n",
    "\n",
    "        # use contract to fill in some missing products\n",
    "        ## create mapping\n",
    "        contract_mapping = (\n",
    "            df.dropna(subset=[\"ContractRaw\",\"Product\"], how=\"any\").drop_duplicates(subset=[\"ContractRaw\"],keep=\"first\")[[\"ContractRaw\",\"Product\"]].set_index(\"ContractRaw\")[\"Product\"]\n",
    "        )\n",
    "        ## create df equal-length series for all entries (Product column)\n",
    "        mapping = df[\"ContractRaw\"].map(contract_mapping)\n",
    "        ## fill na for product column only if product is missing \n",
    "        df[\"Product\"] = df[\"Product\"].fillna(mapping)\n",
    "\n",
    "        # saving\n",
    "        print(f\"Transformed {len(df)} loads, saving ...\")\n",
    "        df.to_csv(gold_HP / \"hp.csv\", index=False)\n",
    "        df.to_excel(gold_HP / \"hp.xlsx\", sheet_name=\"Loads\", index=False)\n",
    "\n",
    "    def run(self, force_run_time:bool=False, force_create_budget:bool=True, force_process_budget_input:bool=False, \n",
    "            PL_only:bool=False, AP_only:bool=False, HP_only:bool=False) -> None:\n",
    "        start = perf_counter()\n",
    "\n",
    "        \n",
    "        if not HP_only:\n",
    "            self._APRporting_project()  # always run AP report\n",
    "\n",
    "            if not AP_only:\n",
    "                # financial operational related projects - run if AP_only is False\n",
    "                self._process_units()\n",
    "                self._finance_operational()\n",
    "                # self._budget_update(force_create=force_create_budget, force_process_input=force_process_budget_input)\n",
    "                self._finance_summary()\n",
    "\n",
    "                if not PL_only:\n",
    "                    # run everything else\n",
    "                    self._weekly_banking()\n",
    "                    # payroll related\n",
    "                    self._payroll_project()\n",
    "                    if force_run_time or (self.today.weekday() in [0, 2, 6]): self._QBOTime_project()\n",
    "                    self._hr_summary()\n",
    "                    # inventory\n",
    "                    self._inventory_settlement()\n",
    "                    self._HP_transformation()\n",
    "        else:\n",
    "            # inventory\n",
    "            self._inventory_settlement()\n",
    "            self._HP_transformation()\n",
    "\n",
    "        end = perf_counter()\n",
    "        print(f\"\\nProjects Transformation Finished with {(end-start)/60:.3f} minutes\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
